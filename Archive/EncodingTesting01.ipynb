{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "impaired-surprise",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jewish-centre",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import glob\n",
    "import sys \n",
    "import yaml \n",
    "import glob\n",
    "import h5py \n",
    "import ray\n",
    "import logging \n",
    "import json\n",
    "import gc\n",
    "import cv2\n",
    "import time\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "# import io_dict_to_hdf5 as ioh5\n",
    "import xarray as xr\n",
    "import scipy.linalg as linalg\n",
    "import scipy.sparse as sparse\n",
    "\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from scipy import signal\n",
    "from pathlib import Path\n",
    "from scipy.optimize import minimize_scalar\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.ndimage import shift as imshift\n",
    "from sklearn.model_selection import train_test_split, GroupShuffleSplit\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import linear_model as lm \n",
    "from scipy.stats import binned_statistic\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import torch.nn.functional as F\n",
    "# from torchvision import transforms\n",
    "# from torch.utils.data import Dataset, DataLoader, TensorDataset, Subset\n",
    "# torch.backends.cudnn.benchmark = True\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "sys.path.append(str(Path('.').absolute().parent))\n",
    "from utils import *\n",
    "import io_dict_to_hdf5 as ioh5\n",
    "from format_data import load_ephys_data_aligned\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "FigPath = check_path(Path('~/Research/SensoryMotorPred_Data').expanduser(),'Figures/Decoding')\n",
    "\n",
    "ray.init(\n",
    "    ignore_reinit_error=True,\n",
    "    logging_level=logging.ERROR,\n",
    ")\n",
    "print(f'Dashboard URL: http://{ray.get_dashboard_url()}')\n",
    "print('Dashboard URL: http://localhost:{}'.format(ray.get_dashboard_url().split(':')[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desirable-break",
   "metadata": {},
   "source": [
    "To Do:  1. Make Possion GLM with L2 norm, solve equations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleared-screw",
   "metadata": {},
   "source": [
    "# Gather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broadband-peeing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_test(file_dict, save_dir, model_dt=.1, frac=.2, do_shuffle=False, do_norm=False):\n",
    "    ##### Load in preprocessed data #####\n",
    "    data = load_ephys_data_aligned(file_dict, save_dir, model_dt=model_dt)\n",
    "    ##### Find 'good' timepoints when mouse is active #####\n",
    "    nan_idxs = []\n",
    "    for key in data.keys():\n",
    "        nan_idxs.append(np.where(np.isnan(data[key]))[0])\n",
    "    good_idxs = np.ones(len(data['model_active']),dtype=bool)\n",
    "    good_idxs[data['model_active']<.5] = False\n",
    "    good_idxs[np.unique(np.hstack(nan_idxs))] = False\n",
    "    \n",
    "    data['raw_nsp'] = data['model_nsp'].copy()\n",
    "    ##### return only active data #####\n",
    "    for key in data.keys():\n",
    "        if (key != 'model_nsp') & (key != 'model_active'):\n",
    "            data[key] = data[key][good_idxs] # interp_nans(data[key]).astype(float)\n",
    "        elif (key == 'model_nsp'):\n",
    "            data[key] = data[key][good_idxs]\n",
    "\n",
    "    gss = GroupShuffleSplit(n_splits=1, train_size=.7, random_state=42)\n",
    "    nT = data['model_nsp'].shape[0]\n",
    "    groups = np.hstack([i*np.ones(int((frac*i)*nT) - int((.2*(i-1))*nT)) for i in range(1,int(1/frac)+1)])\n",
    "\n",
    "    for train_idx, test_idx in gss.split(np.arange(len(data['model_nsp'])), groups=groups):\n",
    "        print(\"TRAIN:\", train_idx, \"TEST:\", test_idx)\n",
    "    if do_shuffle:\n",
    "        train_idx = shuffle(train_idx, random_state=42)\n",
    "        test_idx = shuffle(test_idx, random_state=42)\n",
    "\n",
    "    data['model_dth'] = np.diff(data['model_th'],append=0)\n",
    "    data['model_dphi'] = np.diff(data['model_phi'],append=0)\n",
    "\n",
    "    data['model_vid_sm'] = (data['model_vid_sm'] - np.mean(data['model_vid_sm'],axis=0))/np.std(data['model_vid_sm'],axis=0) \n",
    "    if do_norm:\n",
    "        data['model_th'] = (data['model_th'] - np.mean(data['model_th'],axis=0))/np.std(model_th,axis=0) \n",
    "        data['model_phi'] = (data['model_phi'] - np.mean(data['model_phi'],axis=0))/np.std(model_phi,axis=0) \n",
    "        data['model_roll'] = (data['model_roll'] - np.mean(data['model_roll'],axis=0))/np.std(model_roll,axis=0) \n",
    "        data['model_pitch'] = (data['model_pitch'] - np.mean(data['model_pitch'],axis=0))/np.std(model_pitch,axis=0) \n",
    "\n",
    "    ##### Split Data by train/test #####\n",
    "    data_train_test = {\n",
    "    'train_vid': data['model_vid_sm'][train_idx],\n",
    "    'test_vid': data['model_vid_sm'][test_idx],\n",
    "    'train_nsp': data['model_nsp'][train_idx],\n",
    "    'test_nsp': data['model_nsp'][test_idx],\n",
    "    'train_th': data['model_th'][train_idx],\n",
    "    'test_th': data['model_th'][test_idx],\n",
    "    'train_phi': data['model_phi'][train_idx],\n",
    "    'test_phi': data['model_phi'][test_idx],\n",
    "    'train_roll': data['model_roll'][train_idx],\n",
    "    'test_roll': data['model_roll'][test_idx],\n",
    "    'train_pitch': data['model_pitch'][train_idx],\n",
    "    'test_pitch': data['model_pitch'][test_idx],\n",
    "    'train_t': data['model_t'][train_idx],\n",
    "    'test_t': data['model_t'][test_idx],\n",
    "    'train_dth': data['model_dth'][train_idx],\n",
    "    'test_dth': data['model_dth'][test_idx],\n",
    "    'train_dphi': data['model_dphi'][train_idx],\n",
    "    'test_dphi': data['model_dphi'][test_idx],\n",
    "    'train_gz': data['model_gz'][train_idx],\n",
    "    'test_gz': data['model_gz'][test_idx],\n",
    "    }\n",
    "    d1 = data\n",
    "    d1.update(data_train_test)\n",
    "    return d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "national-islam",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = Path('~/Research/SensoryMotorPred_Data/data/070921/J553RT/fm1').expanduser()\n",
    "with open(save_dir / 'file_dict.json','r') as fp:\n",
    "    file_dict = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "christian-navigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dict = {'cell': 0,\n",
    " 'drop_slow_frames': True,\n",
    " 'ephys': '/home/seuss/Goeppert/freely_moving_ephys/ephys_recordings/070921/J553RT/fm1/070921_J553RT_control_Rig2_fm1_ephys_merge.json',\n",
    " 'ephys_bin': '/home/seuss/Goeppert/freely_moving_ephys/ephys_recordings/070921/J553RT/fm1/070921_J553RT_control_Rig2_fm1_Ephys.bin',\n",
    " 'eye': '/home/seuss/Goeppert/freely_moving_ephys/ephys_recordings/070921/J553RT/fm1/070921_J553RT_control_Rig2_fm1_REYE.nc',\n",
    " 'imu': '/home/seuss/Goeppert/freely_moving_ephys/ephys_recordings/070921/J553RT/fm1/070921_J553RT_control_Rig2_fm1_imu.nc',\n",
    " 'mapping_json': '/home/seuss/Research/Github/FreelyMovingEphys/probes/channel_maps.json',\n",
    " 'mp4': True,\n",
    " 'name': '070921_J553RT_control_Rig2_fm1',\n",
    " 'probe_name': 'DB_P128-6',\n",
    " 'save': '/home/seuss/Goeppert/freely_moving_ephys/ephys_recordings/070921/J553RT/fm1',\n",
    " 'speed': None,\n",
    " 'stim_type': 'light',\n",
    " 'top': '/home/seuss/Goeppert/freely_moving_ephys/ephys_recordings/070921/J553RT/fm1/070921_J553RT_control_Rig2_fm1_TOP1.nc',\n",
    " 'world': '/home/seuss/Goeppert/freely_moving_ephys/ephys_recordings/070921/J553RT/fm1/070921_J553RT_control_Rig2_fm1_world.nc'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olympic-promise",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_train_test(file_dict, save_dir, do_shuffle=False, do_norm=False)\n",
    "locals().update(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "younger-dating",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_dt = .1\n",
    "# data = load_ephys_data_aligned(file_dict, save_dir, model_dt=model_dt)\n",
    "# nan_idxs = []\n",
    "# for key in data.keys():\n",
    "#     nan_idxs.append(np.where(np.isnan(data[key]))[0])\n",
    "# good_idxs = np.ones(len(data['model_active']),dtype=bool)\n",
    "# good_idxs[data['model_active']<.5] = False\n",
    "# good_idxs[np.unique(np.hstack(nan_idxs))] = False\n",
    "\n",
    "# raw_nsp = data['model_nsp'].copy()\n",
    "\n",
    "# for key in data.keys():\n",
    "#     if (key != 'model_nsp') & (key != 'model_active'):\n",
    "#         data[key] = data[key][good_idxs] # interp_nans(data[key]).astype(float)\n",
    "#     elif (key == 'model_nsp'):\n",
    "#         data[key] = data[key][good_idxs]\n",
    "# locals().update(data)\n",
    "\n",
    "# plt.hist(data['model_active'], bins=100)\n",
    "# plt.axvline(x=.5)\n",
    "# # movement_times = (data['model_active']>.5) & (~np.isnan(data['model_th']))\n",
    "\n",
    "# ##### Print memory of local variables #####\n",
    "# for name, size in sorted(((name, sys.getsizeof(value)) for name, value in locals().items()), key= lambda x: -x[1])[:10]:\n",
    "#     print(\"{:>30}: {:>8}\".format(name, sizeof_fmt(size)))\n",
    "\n",
    "# ##### Group shuffle #####\n",
    "# do_shuffle = False\n",
    "# do_norm = False\n",
    "\n",
    "# gss = GroupShuffleSplit(n_splits=1, train_size=.7, random_state=42)\n",
    "# nT = model_nsp.shape[0]\n",
    "# frac = .2\n",
    "# groups = np.hstack([i*np.ones(int((frac*i)*nT) - int((.2*(i-1))*nT)) for i in range(1,int(1/frac)+1)])\n",
    "\n",
    "# for train_idx, test_idx in gss.split(np.arange(len(model_nsp)), groups=groups):\n",
    "#     print(\"TRAIN:\", train_idx, \"TEST:\", test_idx)\n",
    "# if do_shuffle:\n",
    "#     train_idx, test_idx = shuffle(train_idx, test_idx, random_state=42)\n",
    "# model_dth = np.diff(model_th,append=0)\n",
    "# model_dphi = np.diff(model_phi,append=0)\n",
    "\n",
    "# model_vid_sm = (model_vid_sm - np.mean(model_vid_sm,axis=0))/np.std(model_vid_sm,axis=0) \n",
    "# if do_norm:\n",
    "#     model_th = (model_th - np.mean(model_th,axis=0))/np.std(model_th,axis=0) \n",
    "#     model_phi = (model_phi - np.mean(model_phi,axis=0))/np.std(model_phi,axis=0) \n",
    "#     model_roll = (model_roll - np.mean(model_roll,axis=0))/np.std(model_roll,axis=0) \n",
    "#     model_pitch = (model_pitch - np.mean(model_pitch,axis=0))/np.std(model_pitch,axis=0) \n",
    "\n",
    "# ##### Split Data by train/test #####\n",
    "# train_vid = model_vid_sm[train_idx]\n",
    "# test_vid = model_vid_sm[test_idx]\n",
    "# train_nsp = model_nsp[train_idx]\n",
    "# test_nsp = model_nsp[test_idx]\n",
    "# train_th = model_th[train_idx]\n",
    "# test_th = model_th[test_idx]\n",
    "# train_phi = model_phi[train_idx]\n",
    "# test_phi = model_phi[test_idx]\n",
    "# train_roll = model_roll[train_idx]\n",
    "# test_roll = model_roll[test_idx]\n",
    "# train_pitch = model_pitch[train_idx]\n",
    "# test_pitch = model_pitch[test_idx]\n",
    "# train_t = model_t[train_idx]\n",
    "# test_t = model_t[test_idx]\n",
    "# train_dth = model_dth[train_idx]\n",
    "# test_dth = model_dth[test_idx]\n",
    "# train_dphi = model_dphi[train_idx]\n",
    "# test_dphi = model_dphi[test_idx]\n",
    "# train_gz = model_gz[train_idx]\n",
    "# test_gz = model_gz[test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gross-shark",
   "metadata": {},
   "source": [
    "# Testing Tuning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intellectual-gates",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Tuning curve for theta\n",
    "def tuning_curve(model_nsp, var, model_dt = .025, N_bins=10):\n",
    "    var_range = np.linspace(np.nanmean(var)-2*np.nanstd(var), np.nanmean(var)+2*np.nanstd(var),N_bins)\n",
    "    tuning = np.zeros((model_nsp.shape[-1],len(var_range)-1))\n",
    "    tuning_std = np.zeros((model_nsp.shape[-1],len(var_range)-1))\n",
    "    for n in range(model_nsp.shape[-1]):\n",
    "        for j in range(len(var_range)-1):\n",
    "            usePts = (var>=var_range[j]) & (var<var_range[j+1])\n",
    "            tuning[n,j] = np.nanmean(model_nsp[usePts,n])/model_dt\n",
    "            tuning_std[n,j] = (np.nanstd(model_nsp[usePts,n])/model_dt)/ np.sqrt(np.count_nonzero(usePts))\n",
    "    return tuning, tuning_std, var_range[:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interstate-defeat",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning, tuning_std, var_range = tuning_curve(train_nsp, train_phi, N_bins=10, model_dt=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corporate-behalf",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 51\n",
    "fig, axs = plt.subplots(1,figsize=(7,5))\n",
    "axs.errorbar(var_range,tuning[n], yerr=tuning_std[n])\n",
    "axs.set_ylim(bottom=0)\n",
    "axs.set_xlabel('Eye Phi')\n",
    "axs.set_ylabel('Spikes/s')\n",
    "axs.set_title('Neuron: {}'.format(n))\n",
    "plt.tight_layout()\n",
    "fig.savefig(FigPath/'ExampleTuningCurve.png',bbox_inches='tight',transparent=False, facecolor='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protected-worship",
   "metadata": {},
   "source": [
    "## PCA on Vid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dependent-settle",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pcs = pca.fit_transform(model_vid_sm.reshape(-1,model_vid_sm.shape[1]*model_vid_sm.shape[2]))\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "comp_to_keep = np.where(np.cumsum(pca.explained_variance_ratio_)>.9)[0][0]\n",
    "plt.axvline(x=comp_to_keep)\n",
    "pca = PCA(n_components=comp_to_keep)\n",
    "pcs = pca.fit_transform(model_vid_sm.reshape(-1,model_vid_sm.shape[1]*model_vid_sm.shape[2]))\n",
    "print('keep {} PCs'.format(comp_to_keep))\n",
    "# recon = pca.inverse_transform(pcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limited-mother",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vid_sm.shape,model_th.shape,model_phi.shape,model_roll.shape,model_pitch.shape,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portable-standard",
   "metadata": {},
   "source": [
    "# GLM Movement Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bridal-excitement",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'poissonregressor'\n",
    "if model_type == 'elasticnetcv':\n",
    "    model = make_pipeline(StandardScaler(), lm.ElasticNetCV()) # lm.RidgeCV(alphas=np.arange(100,10000,1000))) #  #MultiOutputRegressor(lm.Ridge(),n_jobs=-1)) \n",
    "elif model_type == 'ridgecv':\n",
    "    model = make_pipeline(StandardScaler(), lm.RidgeCV(alphas=lambdas))\n",
    "elif model_type == 'poissonregressor':\n",
    "    model = make_pipeline(StandardScaler(), lm.PoissonRegressor())\n",
    "\n",
    "nks = np.shape(model_vid)[1:]; nk = nks[0]*nks[1]\n",
    "test_frac = 0.3\n",
    "ntest = int(nT*test_frac)\n",
    "titles = np.array(['th','phi','roll','pitch','dth','dphi'])\n",
    "move_train = np.hstack((model_th[ntest:,np.newaxis],model_phi[ntest:,np.newaxis],model_roll[ntest:,np.newaxis],model_pitch[ntest:,np.newaxis],model_dth[ntest:,np.newaxis],model_dphi[ntest:,np.newaxis]))\n",
    "move_test = np.hstack((model_th[:ntest,np.newaxis],model_phi[:ntest,np.newaxis],model_roll[:ntest,np.newaxis],model_pitch[:ntest,np.newaxis],model_dth[:ntest,np.newaxis],model_dphi[:ntest,np.newaxis]))\n",
    "sps_smooth_all = np.zeros((15,len(lag_list),move_test.shape[0]))\n",
    "pred_smooth_all = np.zeros((15,len(lag_list),move_test.shape[0]))\n",
    "cc_all = np.zeros((n_units,15,len(lag_list)))\n",
    "model_coef_all= [] # = np.zeros((15,len(lag_list)))\n",
    "titles_all = []\n",
    "\n",
    "celln = 51\n",
    "bin_length = 80\n",
    "model_ind = 0\n",
    "with PdfPages(FigPath/ 'ModelSelection_{}_MoveOnly.pdf'.format(model_type)) as pdf:\n",
    "\n",
    "    for n in range(1,5):\n",
    "        perms = np.array(list(itertools.combinations([0,1,2,3], n)))\n",
    "        for ind in range(perms.shape[0]):\n",
    "            fig, axs = plt.subplots(1,len(lag_list), figsize=(np.floor(7.5*len(lag_list)).astype(int),5))\n",
    "            move_train2 = move_train[:,perms[ind]]\n",
    "            move_test2 = move_test[:,perms[ind]]\n",
    "            # iterate through timing lags\n",
    "            for lag_ind, lag in enumerate(lag_list):\n",
    "                sps = np.roll(model_nsp.T[celln,:],-lag)\n",
    "                nT = len(sps)\n",
    "                sps_train = sps[ntest:]\n",
    "                sps_test = sps[:ntest]\n",
    "\n",
    "                model.fit(move_train2,sps_train)\n",
    "                sps_pred = model.predict(move_test2)\n",
    "\n",
    "                sps_smooth = (np.convolve(sps_test, np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "                pred_smooth = (np.convolve(sps_pred, np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "                cc_all[celln,model_ind,lag_ind] = np.corrcoef(sps_smooth, pred_smooth)[0,1]\n",
    "                sps_smooth_all[model_ind,lag_ind] = sps_smooth\n",
    "                pred_smooth_all[model_ind,lag_ind] = pred_smooth\n",
    "                model_coef_all.append(model[model_type].coef_)\n",
    "                \n",
    "                axs[lag_ind].plot(sps_smooth,'k',label='smoothed FR')\n",
    "                axs[lag_ind].plot(pred_smooth,'r', label='pred FR')\n",
    "                axs[lag_ind].set_title('cc={:.3f}'.format(cc_all[celln,model_ind,lag_ind]))\n",
    "            #     axs[1,lag_ind].imshow(sta_all[celln,lag_ind])\n",
    "            #     axs[1,lag_ind].set_title('lag={:d}'.format(lag_list[lag_ind]))\n",
    "            #     axs[1,lag_ind].axis('off')\n",
    "            #     plt.suptitle('No Smoothness splitdata pipeline')\n",
    "                \n",
    "            titles_all.append('_'.join([t for t in titles[perms[ind]]]))\n",
    "            plt.suptitle(titles_all[-1])\n",
    "            plt.tight_layout()\n",
    "            pdf.savefig()\n",
    "            \n",
    "            model_ind+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "million-communist",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(msetrain)\n",
    "plt.plot(msetest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handed-torture",
   "metadata": {},
   "source": [
    "# Parallel Processing GLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interpreted-charger",
   "metadata": {},
   "source": [
    "## Vis Only sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modified-original",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dgaze_p = train_dth + np.diff(train_gz,append=0)\n",
    "train_dgaze_n = train_dth - np.diff(train_gz,append=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seventh-jonathan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if model_type == 'elasticnetcv':\n",
    "#     model = make_pipeline(StandardScaler(), lm.ElasticNetCV()) # lm.RidgeCV(alphas=np.arange(100,10000,1000))) #  #MultiOutputRegressor(lm.Ridge(),n_jobs=-1)) \n",
    "# elif model_type == 'ridgecv':\n",
    "#     model = make_pipeline(StandardScaler(), lm.RidgeCV(alphas=lambdas))\n",
    "# elif model_type == 'poissonregressor':\n",
    "#     model = make_pipeline(StandardScaler(), lm.PoissonRegressor())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spoken-tracker",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def do_glm_fit_vis_skl(train_nsp, test_nsp, train_data, test_data, celln, model_type, nt_glm_lag, bin_length=40, model_dt=.1):\n",
    "    ##### Format data #####\n",
    "    # save shape of train_data for initialization\n",
    "    nks = np.shape(train_data)[1:]; nk = nks[0]*nks[1]\n",
    "\n",
    "    # Shift spikes by -lag for GLM fits\n",
    "    sps_train = train_nsp[:,celln] # np.roll(train_nsp[:,celln],-lag)\n",
    "    sps_test = test_nsp[:,celln] # np.roll(test_nsp[:,celln],-lag)\n",
    "\n",
    "    # Reshape data (video) into (T*n)xN array\n",
    "    x_train = train_data.reshape(train_data.shape[0],-1)\n",
    "    x_train = np.hstack([np.roll(x_train, nframes, axis=0) for nframes in reversed(range(nt_glm_lag))])\n",
    "    x_test = test_data.reshape(test_data.shape[0],-1) \n",
    "    x_test = np.hstack([np.roll(x_test,nframes, axis=0) for nframes in reversed(range(nt_glm_lag))])\n",
    "\n",
    "    if model_type == 'elasticnetcv':\n",
    "        model = lm.ElasticNetCV(l1_ratio=[.05, .01, .5, .7]) # lm.RidgeCV(alphas=np.arange(100,10000,1000))) #  #MultiOutputRegressor(lm.Ridge(),n_jobs=-1)) \n",
    "        model.fit(x_train,sps_train)\n",
    "        sta_all = np.reshape(model.coef_,(nt_glm_lag,)+nks)\n",
    "        sp_pred = model.predict(x_test)\n",
    "    elif model_type == 'ridgecv':\n",
    "        lambdas = 1024 * (2**np.arange(0,16))\n",
    "        model = lm.RidgeCV(alphas=lambdas)\n",
    "        model.fit(x_train,sps_train)\n",
    "        sta_all = np.reshape(model.coef_,(nt_glm_lag,)+nks)\n",
    "        sp_pred = model.predict(x_test)\n",
    "    else:\n",
    "        lambdas = (2**np.arange(0,16))\n",
    "        nlam = len(lambdas)\n",
    "        # Initialze mse traces for regularization cross validation\n",
    "        msetrain = np.zeros((nlam,1))\n",
    "        msetest = np.zeros((nlam,1))\n",
    "        pred_all =np.zeros((x_test.shape[0],nlam)) \n",
    "        w_ridge = np.zeros((x_train.shape[-1],nlam))\n",
    "        w_intercept = np.zeros((nlam,1))\n",
    "        # loop over regularization strength\n",
    "        for l in range(len(lambdas)):\n",
    "            if model_type == 'poissonregressor':\n",
    "                model = lm.PoissonRegressor(alpha=lambdas[l])\n",
    "            # calculate MAP estimate               \n",
    "            model.fit(x_train,sps_train)\n",
    "            w_ridge[:,l] = model.coef_\n",
    "            w_intercept[l] = model.intercept_\n",
    "            # calculate test and training rms error\n",
    "            msetrain[l] = np.mean((sps_train - model.predict(x_train))**2)\n",
    "            msetest[l] = np.mean((sps_test - model.predict(x_test))**2)\n",
    "            pred_all[:,l] = model.predict(x_test)\n",
    "        # select best cross-validated lambda for RF\n",
    "        best_lambda = np.argmin(msetest)\n",
    "        w = w_ridge[:,best_lambda]\n",
    "        intercept= w_intercept[best_lambda]\n",
    "        ridge_rf = w_ridge[:,best_lambda]\n",
    "        sta_all = np.reshape(w,(nt_glm_lag,)+nks)\n",
    "        sp_pred = pred_all[:,best_lambda]\n",
    "    #     model = make_pipeline(StandardScaler(), lm.PoissonRegressor(alpha=lambdas[best_lambda]))\n",
    "    #     model.fit(x_train,sps_train)\n",
    "\n",
    "    # predicted firing rate\n",
    "    # bin the firing rate to get smooth rate vs time\n",
    "    sp_smooth = (np.convolve(sps_test, np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "    pred_smooth = (np.convolve(sp_pred, np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "    # a few diagnostics\n",
    "    err = np.mean((sp_smooth-pred_smooth)**2)\n",
    "    cc = np.corrcoef(sp_smooth, pred_smooth)\n",
    "    cc_all = cc[0,1]\n",
    "\n",
    "    return cc_all, sta_all, sps_test, sp_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saving-recovery",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dt = .1\n",
    "model_type = 'elasticnetcv'\n",
    "nt_glm_lag=5\n",
    "# Load Data\n",
    "do_shuffle=False\n",
    "data = load_train_test(file_dict, save_dir, do_shuffle=do_shuffle, do_norm=True)\n",
    "locals().update(data)\n",
    "\n",
    "##### Start GLM Parallel Processing #####\n",
    "start = time.time()\n",
    "nks = np.shape(train_vid)[1:]; nk = nks[0]*nks[1]*nt_glm_lag\n",
    "\n",
    "# Put data into shared memory for parallization \n",
    "train_nsp_r = ray.put(train_nsp)\n",
    "test_nsp_r = ray.put(test_nsp)\n",
    "train_data_r = ray.put(train_vid)\n",
    "test_data_r = ray.put(test_vid)\n",
    "result_ids = []\n",
    "# Loop over parameters appending process ids\n",
    "for celln in range(train_nsp.shape[1]):\n",
    "    result_ids.append(do_glm_fit_vis_skl.remote(train_nsp_r, test_nsp_r, train_data_r, test_data_r, celln, model_type, nt_glm_lag, model_dt=model_dt))\n",
    "\n",
    "print('N_proc:', len(result_ids))\n",
    "results_p = ray.get(result_ids)\n",
    "print('GLM Add: ', time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operational-jersey",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Gather Data and Find Max CC Model #####\n",
    "mcc = np.stack([results_p[i][0] for i in range(len(results_p))])\n",
    "msta = np.stack([results_p[i][1] for i in range(len(results_p))])\n",
    "msp = np.stack([results_p[i][2] for i in range(len(results_p))])\n",
    "mpred = np.stack([results_p[i][3] for i in range(len(results_p))])\n",
    "\n",
    "# cc_all = cc_all.reshape((model_nsp.shape[1],nt_glm_lag,) + cc_all.shape[1:])\n",
    "# sta_all = sta_all.reshape((model_nsp.shape[1],nt_glm_lag,) + sta_all.shape[1:])\n",
    "# sp_raw = sp_raw.reshape((model_nsp.shape[1],nt_glm_lag,) + sp_raw.shape[1:])\n",
    "# pred_raw = pred_raw.reshape((model_nsp.shape[1],nt_glm_lag,) + pred_raw.shape[1:])\n",
    "\n",
    "# m_cells, m_lags = np.where(cc_all==np.max(cc_all,axis=(-1), keepdims=True))\n",
    "\n",
    "# mcc = cc_all[m_cells,m_lags]\n",
    "# msta = sta_all[m_cells,m_lags]\n",
    "# msp = sp_raw[m_cells,m_lags]\n",
    "# mpred = pred_raw[m_cells,m_lags]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alert-local",
   "metadata": {},
   "outputs": [],
   "source": [
    "GLM_Data = {'mcc': mcc,\n",
    "            'msta': msta,\n",
    "            'msp': msp,\n",
    "            'mpred': mpred,}\n",
    "if do_shuffle:\n",
    "    ioh5.save(save_dir/'GLM_{}_Data_VisOnly_notsmooth_dt{:03d}_T{:02d}_shuffled.h5'.format(model_type,int(model_dt*1000), nt_glm_lag), GLM_Data)\n",
    "else:\n",
    "    ioh5.save(save_dir/'GLM_{}_Data_VisOnly_notsmooth_dt{:03d}_T{:02d}.h5'.format(model_type,int(model_dt*1000), nt_glm_lag), GLM_Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominican-rebel",
   "metadata": {},
   "outputs": [],
   "source": [
    "celln=51\n",
    "fig, axs = plt.subplots(1,nt_glm_lag,figsize=(20,5))\n",
    "crange = np.max(np.abs(msta[celln]))\n",
    "for n in range(nt_glm_lag):\n",
    "    img = axs[n].imshow(msta[celln,n],cmap='seismic',vmin=-crange,vmax=crange)\n",
    "    add_colorbar(img)\n",
    "    axs[n].axis('off')\n",
    "    axs[n].set_title('Lag:{}'.format(n-nt_glm_lag+1))\n",
    "plt.suptitle('Celln:{}, cc={:.03f}'.format(celln,mcc[celln]),y=.75,fontsize=20)\n",
    "plt.tight_layout()\n",
    "\n",
    "# fig.savefig(FigPath/'TemporalRF_N{}.png'.format(celln), facecolor='white', transparent=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustained-protest",
   "metadata": {},
   "source": [
    "## VisOnly By Hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occasional-halloween",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def do_glm_fit_visonly(train_nsp, test_nsp, train_data, test_data, celln, perms, lag, lambdas, bin_length=40, model_dt=.1):\n",
    "    ##### Format data #####\n",
    "    # save shape of train_data for initialization\n",
    "    nks = np.shape(train_data)[1:]; nk = nks[0]*nks[1]\n",
    "    \n",
    "    # Shift spikes by -lag for GLM fits\n",
    "    sps_train = np.roll(train_nsp[:,celln],-lag)\n",
    "    sps_test = np.roll(test_nsp[:,celln],-lag)\n",
    "        \n",
    "    # Reshape data (video) into (T*n)xN array\n",
    "    x_train = train_data.reshape(train_data.shape[0],-1)\n",
    "    x_train = np.append(x_train, np.ones((x_train.shape[0],1)), axis = 1) # append column of ones for fitting intercept\n",
    "#     x_train = np.concatenate((x_train, move_train),axis=1)\n",
    "    \n",
    "    x_test = test_data.reshape(test_data.shape[0],-1) \n",
    "    x_test = np.append(x_test,np.ones((x_test.shape[0],1)), axis = 1) # append column of ones\n",
    "#     x_test = np.concatenate((x_test, move_test),axis=1)\n",
    "    \n",
    "    # Prepare Design Matrix\n",
    "    nlam = len(lambdas)\n",
    "    XXtr = x_train.T @ x_train\n",
    "    XYtr = x_train.T @ sps_train\n",
    "    \n",
    "    # Initialze mse traces for regularization cross validation\n",
    "    msetrain = np.zeros((nlam,1))\n",
    "    msetest = np.zeros((nlam,1))\n",
    "    w_ridge = np.zeros((x_train.shape[-1],nlam))\n",
    "    # Inverse matrix for regularization \n",
    "    Cinv = np.eye(nk)\n",
    "    Cinv = linalg.block_diag(Cinv,np.zeros((1, 1)))\n",
    "    # loop over regularization strength\n",
    "    for l in range(len(lambdas)):\n",
    "        # calculate MAP estimate               \n",
    "        w = np.linalg.solve(XXtr + lambdas[l]*Cinv, XYtr) # equivalent of \\ (left divide) in matlab\n",
    "        w_ridge[:,l] = w\n",
    "        # calculate test and training rms error\n",
    "        msetrain[l] = np.mean((sps_train - x_train@w)**2)\n",
    "        msetest[l] = np.mean((sps_test - x_test@w)**2)\n",
    "    \n",
    "    # select best cross-validated lambda for RF\n",
    "    best_lambda = np.argmin(msetest)\n",
    "    w = w_ridge[:,best_lambda]\n",
    "    ridge_rf = w_ridge[:,best_lambda]\n",
    "    sta_all = np.reshape(w[:-1],nks)\n",
    "    \n",
    "    # predicted firing rate\n",
    "    sp_pred = x_test@ridge_rf\n",
    "    # bin the firing rate to get smooth rate vs time\n",
    "    sp_smooth = (np.convolve(sps_test, np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "    pred_smooth = (np.convolve(sp_pred, np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "    # a few diagnostics\n",
    "    err = np.mean((sp_smooth-pred_smooth)**2)\n",
    "    cc = np.corrcoef(sp_smooth, pred_smooth)\n",
    "    cc_all = cc[0,1]\n",
    "    \n",
    "    return cc_all, sta_all, sps_test, sp_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opening-vault",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start = time.time()\n",
    "\n",
    "lag_list = [ -4, -2, 0 , 4, 2]\n",
    "lambdas = 1024 * (2**np.arange(0,16))\n",
    "nks = np.shape(model_vid_sm)[1:]; nk = nks[0]*nks[1]\n",
    "\n",
    "# Put data into shared memory for parallization \n",
    "train_nsp_r = ray.put(train_nsp)\n",
    "test_nsp_r = ray.put(test_nsp)\n",
    "train_data_r = ray.put(train_vid)\n",
    "test_data_r = ray.put(test_vid)\n",
    "result_ids = []\n",
    "# Loop over parameters appending process ids\n",
    "for celln in range(train_nsp.shape[1]):\n",
    "#     for n in range(1,len(titles)):\n",
    "    perms = [] #np.array(list(itertools.combinations(np.arange(len(titles), n)))\n",
    "#     for ind in range(perms.shape[0]):\n",
    "    for lag_ind, lag in enumerate(lag_list):    \n",
    "        result_ids.append(do_glm_fit_visonly.remote(train_nsp_r, test_nsp_r, train_data_r, test_data_r, celln, perms, lag, lambdas, model_dt=model_dt))\n",
    "\n",
    "print('N_proc:', len(result_ids))\n",
    "results_p = ray.get(result_ids)\n",
    "print('GLM Add: ', time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efficient-dominican",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Gather Data and Find Max CC Model #####\n",
    "cc_all = np.stack([results_p[i][0] for i in range(len(results_p))])\n",
    "sta_all = np.stack([results_p[i][1] for i in range(len(results_p))])\n",
    "sp_raw = np.stack([results_p[i][2] for i in range(len(results_p))])\n",
    "pred_raw = np.stack([results_p[i][3] for i in range(len(results_p))])\n",
    "\n",
    "cc_all = cc_all.reshape((model_nsp.shape[1],len(lag_list),) + cc_all.shape[1:])\n",
    "sta_all = sta_all.reshape((model_nsp.shape[1],len(lag_list),) + sta_all.shape[1:])\n",
    "sp_raw = sp_raw.reshape((model_nsp.shape[1],len(lag_list),) + sp_raw.shape[1:])\n",
    "pred_raw = pred_raw.reshape((model_nsp.shape[1],len(lag_list),) + pred_raw.shape[1:])\n",
    "\n",
    "m_cells, m_lags = np.where(cc_all==np.max(cc_all,axis=(-1), keepdims=True))\n",
    "\n",
    "mcc = cc_all[m_cells,m_lags]\n",
    "msta = sta_all[m_cells,m_lags]\n",
    "msp = sp_raw[m_cells,m_lags]\n",
    "mpred = pred_raw[m_cells,m_lags]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paperback-pendant",
   "metadata": {},
   "outputs": [],
   "source": [
    "GLM_add = {'cc_all': cc_all,\n",
    "            'sta_all': sta_all,\n",
    "            'sp_raw': sp_raw,\n",
    "            'pred_raw': pred_raw,}\n",
    "ioh5.save(save_dir/'Add_GLM_Data_VisOnly_notsmooth_dt{:03d}_Shuffled.h5'.format(int(model_dt*1000)), GLM_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correct-content",
   "metadata": {},
   "outputs": [],
   "source": [
    "GLM_data = ioh5.load(save_dir/'GLM_{}_Data_VisOnly_notsmooth_dt{:03d}_T{:02d}.h5'.format(model_type,int(model_dt*1000), nt_glm_lag))\n",
    "\n",
    "GLM_shuff = ioh5.load(save_dir/'GLM_{}_Data_VisOnly_notsmooth_dt{:03d}_T{:02d}_shuffled.h5'.format(model_type,int(model_dt*1000), nt_glm_lag))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vital-fortune",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposite-legend",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf = 2\n",
    "model_vid = np.zeros((model_vid_sm.shape[0],sf*model_vid_sm.shape[1],sf*model_vid_sm.shape[2]))\n",
    "for n in range(model_vid_sm.shape[0]):\n",
    "    model_vid[n] = cv2.resize(model_vid_sm[n],(sf*model_vid_sm.shape[2],sf*model_vid_sm.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "purple-robinson",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.animation import FuncAnimation, PillowWriter, FFMpegWriter\n",
    "def init():\n",
    "    axs[0].axis('off')\n",
    "    axs[1].set_xlabel('Frame #')\n",
    "    axs[1].set_ylabel('Smoothed FR (spks/s)')\n",
    "    axs[1].set_title('cc={:.2f}, \\n lag={:d}'.format(mcc[celln],lag_list[m_lags[celln]]))\n",
    "    plt.tight_layout()\n",
    "\n",
    "def update(t):\n",
    "    img.set_data(model_vid[t])\n",
    "    ln.set_data([t, t], [0, 1])\n",
    "    plt.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enabling-chair",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t = 3500# np.argmin((msp_smooth-pred_smooth)**2)\n",
    "dt = 1500\n",
    "celln = np.argmax(mcc)\n",
    "msp_smooth=(np.convolve(msp[celln], np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "pred_smooth=(np.convolve(mpred[celln], np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "\n",
    "fig, axs = plt.subplots(1,2,figsize=(12,5))\n",
    "img = axs[0].imshow(model_vid[t],cmap='gray')\n",
    "axs[0].axis('off')\n",
    "ln = axs[1].axvline(x=t,c='b')\n",
    "axs[1].plot(np.arange(t,t+dt),msp_smooth[t:t+dt],'k',label='test FR')\n",
    "axs[1].plot(np.arange(t,t+dt),pred_smooth[t:t+dt],'r', label='pred FR')\n",
    "axs[1].set_xlabel('Frame #')\n",
    "axs[1].set_ylabel('Smoothed FR (spks/s)')\n",
    "axs[1].set_title('cc={:.2f}, \\n lag={:d}'.format(mcc[celln],lag_list[m_lags[celln]]))\n",
    "plt.tight_layout()\n",
    "# fig.savefig(os.path.join(FigurePath,'testimg.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transsexual-sarah",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "ani = FuncAnimation(fig, update, tqdm(np.arange(t,t+dt)), init_func=init)  #range(tot_samps.shape[1])\n",
    "plt.show()\n",
    "vname =  'SampleVid.mp4'\n",
    "writervideo = FFMpegWriter(fps=60)\n",
    "ani.save(FigPath/vname, writer=writervideo)\n",
    "print('DONE!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arabic-anchor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.imshow(msta, animation_frame=0, binary_string=False,color_continuous_scale='RdBu_r')\n",
    "fig.update_layout(width=500,\n",
    "                  height=500,)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifty-casting",
   "metadata": {},
   "outputs": [],
   "source": [
    "GLM_data = ioh5.load(save_dir/'GLM_{}_Data_VisOnly_notsmooth_dt{:03d}_T{:02d}.h5'.format(model_type,int(model_dt*1000), nt_glm_lag))\n",
    "locals().update(GLM_data)\n",
    "mcc = cc_all\n",
    "msta = sta_all\n",
    "msp = sp_raw\n",
    "mpred = pred_raw\n",
    "# GLM_shuff = ioh5.load(save_dir/'GLM_{}_Data_VisOnly_notsmooth_dt{:03d}_T{:02d}_shuffled.h5'.format(model_type,int(model_dt*1000), nt_glm_lag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honest-malawi",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.hist(GLM_data['cc_all'],bins=20,color='k',alpha=.5,label='Test CC')\n",
    "plt.hist(GLM_shuff['cc_all'],bins=20,color='r', alpha=.5,label='Shuffled CC')\n",
    "plt.xlabel('Corr. Coeff.')\n",
    "plt.legend()\n",
    "plt.savefig(FigPath/'CC_comparison_{}.png'.format(model_type), facecolor='white', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "august-pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_add(alpha,stat_range,stat_all):\n",
    "    return np.mean((stat_range - stat_all+alpha)**2)\n",
    "\n",
    "def f_mult(alpha,stat_range,stat_all):\n",
    "    return np.mean((stat_range - stat_all*alpha)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "waiting-diamond",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_length=40\n",
    "data = load_train_test(file_dict, save_dir, do_shuffle=do_shuffle, do_norm=False)\n",
    "locals().update(data)\n",
    "##### Explore Neurons #####\n",
    "colors = plt.cm.winter(np.linspace(0,1,4))\n",
    "clrs = ['blue','orange','green','red']\n",
    "# Initialize movement combinations\n",
    "titles = np.array(['th','phi','roll','pitch']) # \n",
    "titles_all = []\n",
    "for n in range(1,len(titles)):\n",
    "    perms = np.array(list(itertools.combinations(np.arange(len(titles)), n)))\n",
    "    for ind in range(perms.shape[0]):\n",
    "        titles_all.append('_'.join([t for t in titles[perms[ind]]]))\n",
    "        \n",
    "# move_train = np.hstack((train_th[:,np.newaxis],train_phi[:,np.newaxis],train_roll[:,np.newaxis],train_pitch[:,np.newaxis], train_dth[:,np.newaxis],train_dphi[:,np.newaxis]))\n",
    "move_test = np.hstack((test_th[:,np.newaxis],test_phi[:,np.newaxis],test_roll[:,np.newaxis],test_pitch[:,np.newaxis], test_dth[:,np.newaxis],test_dphi[:,np.newaxis]))\n",
    "move_test = move_test - np.mean(move_test,axis=0)\n",
    "# Create all tuning curves for plotting\n",
    "N_bins=10\n",
    "ncells = model_nsp.shape[-1]\n",
    "ax_ylims = np.zeros((model_nsp.shape[-1],len(titles)))\n",
    "tuning_curves = np.zeros((model_nsp.shape[-1],len(titles),N_bins-1))\n",
    "tuning_stds = np.zeros((model_nsp.shape[-1],len(titles),N_bins-1))\n",
    "var_ranges = np.zeros((len(titles),N_bins-1))\n",
    "for modeln in range(len(titles)):\n",
    "    metric = move_test[:,modeln]\n",
    "    tuning, tuning_std, var_range = tuning_curve(test_nsp, metric, N_bins=N_bins, model_dt=.1)\n",
    "    tuning_curves[:,modeln] = tuning\n",
    "    tuning_stds[:,modeln] = tuning_std\n",
    "    ax_ylims[:,modeln] = np.max(tuning,axis=1)\n",
    "    var_ranges[modeln] = var_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turkish-updating",
   "metadata": {},
   "outputs": [],
   "source": [
    "with PdfPages(FigPath/ 'MaxCC_GLM_dt{:03d}_cellsummary_sig.pdf'.format(int(model_dt*1000))) as pdf:\n",
    "    for celln in tqdm(range(msp.shape[0])):\n",
    "        if mcc[celln]>.3:\n",
    "            fig, axs = plt.subplots(2,5, figsize=((35,10))) #np.floor(7.5*len(model_nsp)).astype(int)\n",
    "            predcell = mpred[celln]/model_dt\n",
    "            nspcell = msp[celln]/model_dt\n",
    "            msp_smooth=(np.convolve(msp[celln], np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "            pred_smooth=(np.convolve(mpred[celln], np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "            axs[0,0].plot(msp_smooth,'k',label='test FR')\n",
    "            axs[0,0].plot(pred_smooth,'r', label='pred FR')\n",
    "            axs[0,0].set_xlabel('Frame #')\n",
    "            axs[0,0].set_ylabel('Smoothed Firing Rate (spks/s)')\n",
    "            axs[0,0].legend()\n",
    "            axs[0,0].set_title('cc={:.2f}, \\n lag={:d}'.format(mcc[celln],lag_list[m_lags[celln]]))\n",
    "            crange = np.max(np.abs(msta[celln]))\n",
    "            img = axs[0,1].imshow(msta[celln],cmap='seismic',vmin=-crange,vmax=crange)\n",
    "            axs[0,1].set_title('STA,cell: {:d}'.format(celln))\n",
    "            axs[0,1].axis('off')\n",
    "            add_colorbar(img)\n",
    "\n",
    "\n",
    "            for modeln in range(len(titles)):\n",
    "                axs[0,2].errorbar(var_ranges[modeln],tuning_curves[celln,modeln], yerr=tuning_stds[celln,modeln],label=titles[modeln],c=clrs[modeln],lw=4,elinewidth=3)\n",
    "            axs[0,2].set_ylim(bottom=0,top=np.max(ax_ylims,axis=1)[celln]+2*np.max(tuning_stds,axis=(1,2))[celln])\n",
    "            axs[0,2].set_xlim(-50,50)\n",
    "            axs[0,2].set_xlabel('Angle ($ ^{\\degree}$)')\n",
    "            axs[0,2].set_ylabel('Spikes/s')\n",
    "            axs[0,2].set_title('Tuning Curves')\n",
    "            axs[0,2].legend(bbox_to_anchor=(1.01, 1), fontsize=12)\n",
    "\n",
    "\n",
    "            # Set up predicted spike range between 1-99th percentile\n",
    "            pred_range = np.quantile(predcell,[.1,.99])\n",
    "            msp_range = np.quantile(nspcell,[.1,.99])\n",
    "            pred_rangelin = np.linspace(pred_range[0],pred_range[1],5)\n",
    "\n",
    "            hist,xedges,yedges,img =axs[0,3].hist2d(mpred[celln]/model_dt,msp[celln]/model_dt,range=np.vstack((pred_range,msp_range)))#pred_smooth,msp_smooth)\n",
    "            # axs[0,3].scatter(mpred[celln]/model_dt,msp[celln]/model_dt,c='k',s=15)\n",
    "            axs[0,3].set_xlabel('Predicted Spike Rate')\n",
    "            axs[0,3].set_ylabel('Actual Spike Rate')\n",
    "            cbar = add_colorbar(img)\n",
    "            cbar.set_label('count')\n",
    "\n",
    "            mse_add = np.zeros((len(titles),len(nranges)-1,1))\n",
    "            mse_mult = np.zeros((len(titles),len(nranges)-1,1))\n",
    "            alpha_add = np.zeros((len(titles),len(nranges)-1,1))\n",
    "            alpha_mult = np.zeros((len(titles),len(nranges)-1,1))\n",
    "\n",
    "            for modeln in range(len(titles)):\n",
    "                metric = move_test[:,modeln]\n",
    "                nranges = np.quantile(metric,[0,.25,.5,.75,1])# np.linspace(np.nanmean(metric)-2*np.nanstd(metric), np.nanmean(metric)+2*np.nanstd(metric),N_bins)\n",
    "                nranges = np.quantile(metric,[0,.25,.5,.75,1])# np.linspace(np.nanmean(metric)-2*np.nanstd(metric), np.nanmean(metric)+2*np.nanstd(metric),N_bins)\n",
    "                stat_all, edges, _ = binned_statistic(predcell,nspcell, statistic='mean',bins=pred_rangelin)\n",
    "                edge_mids = np.array([(edges[i]+edges[i+1])/2 for i in range(len(edges)-1)])\n",
    "            #     axs[1,modeln].set_xlim(0,pred_range[1]+np.std(pred_range))\n",
    "            #     axs[1,modeln].set_ylim(0,np.max(stat)+np.std(stat))\n",
    "                for n in range(len(nranges)-1):\n",
    "                    ind = np.where(((metric<=nranges[n+1])&(metric>nranges[n])))[0]\n",
    "                    pred = predcell[ind]\n",
    "                    sp = nspcell[ind]\n",
    "\n",
    "                    stat_range, edges, _ = binned_statistic(pred, sp, statistic='mean',bins=pred_rangelin)\n",
    "                    edge_mids = np.array([(edges[i]+edges[i+1])/2 for i in range(len(edges)-1)])\n",
    "\n",
    "                    res_add = minimize_scalar(f_add,args=(stat_range,stat_all))\n",
    "                    res_mult = minimize_scalar(f_mult,args=(stat_range,stat_all))\n",
    "                    mse_add[modeln, n] = res_add.fun\n",
    "                    mse_mult[modeln, n] = res_mult.fun\n",
    "                    alpha_add[modeln, n] = res_add.x\n",
    "                    alpha_mult[modeln, n] = res_mult.x\n",
    "\n",
    "                    axs[1,modeln].plot(edge_mids, stat_range,'.-', c=colors[n],label='{:.02f} : {:.02f}'.format(nranges[n],nranges[n+1]),lw=4,ms=20)\n",
    "                    axs[1,modeln].plot(np.linspace(pred_range[0],pred_range[1]),np.linspace(pred_range[0],pred_range[1]),'k--',zorder=0)\n",
    "                    axs[1,modeln].set_title('Metric: {}'.format(titles[modeln]), color=clrs[modeln])\n",
    "                    axs[1,modeln].set_xlabel('Predicted Spike Rate')\n",
    "                    axs[1,modeln].set_ylabel('Actual Spike Rate')\n",
    "                axs[1,modeln].plot(edge_mids, stat_all,'.-', c='k', lw=5, ms=20, label='All_data')\n",
    "                axs[1,modeln].legend(loc='upper left', fontsize=12)\n",
    "                axs[1,modeln].axis('equal')\n",
    "\n",
    "            min_add = np.min(mse_add,axis=-1)\n",
    "            min_mult = np.min(mse_mult,axis=-1)\n",
    "\n",
    "            crange = np.max(np.abs(min_add-min_mult))\n",
    "            im = axs[1,-1].imshow(min_add-min_mult,cmap='seismic',vmin=-crange,vmax=crange)\n",
    "            axs[1,-1].set_yticks(np.arange(0,4))\n",
    "            axs[1,-1].set_yticklabels(titles)\n",
    "            axs[1,-1].set_ylabel('Movement Model')\n",
    "            axs[1,-1].set_xticks(np.arange(0,4))\n",
    "            axs[1,-1].set_xticklabels(['.25','.5','.75','1'])\n",
    "            axs[1,-1].set_xlabel('Quantile Range')\n",
    "            axs[1,-1].set_title('$MSE_{add}$ - $MSE_{mult}$')\n",
    "            cbar = add_colorbar(im)\n",
    "            plt.tight_layout()\n",
    "\n",
    "            pdf.savefig()\n",
    "            plt.close()\n",
    "        \n",
    "# fig.savefig(FigPath/'CellSummary_N{}.png'.format(celln), facecolor='white', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conservative-duplicate",
   "metadata": {},
   "outputs": [],
   "source": [
    "celln=50\n",
    "ncells=model_nsp.shape[-1]\n",
    "colors = plt.cm.winter(np.linspace(0,1,4))\n",
    "clrs = ['blue','orange','green','red']\n",
    "fig, axs = plt.subplots(2,5, figsize=((35,10))) #np.floor(7.5*len(model_nsp)).astype(int)\n",
    "predcell = mpred[celln]/model_dt\n",
    "nspcell = msp[celln]/model_dt\n",
    "msp_smooth=(np.convolve(msp[celln], np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "pred_smooth=(np.convolve(mpred[celln], np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "axs[0,0].plot(msp_smooth,'k',label='test FR')\n",
    "axs[0,0].plot(pred_smooth,'r', label='pred FR')\n",
    "axs[0,0].set_xlabel('Frame #')\n",
    "axs[0,0].set_ylabel('Smoothed Firing Rate (spks/s)')\n",
    "axs[0,0].legend()\n",
    "axs[0,0].set_title('cc={:.2f}, \\n lag={:d}'.format(mcc[celln],lag_list[m_lags[celln]]))\n",
    "crange = np.max(np.abs(msta[celln]))\n",
    "img = axs[0,1].imshow(msta[celln],cmap='seismic',vmin=-crange,vmax=crange)\n",
    "axs[0,1].set_title('STA,cell: {:d}'.format(celln))\n",
    "axs[0,1].axis('off')\n",
    "add_colorbar(img)\n",
    "\n",
    "\n",
    "for modeln in range(len(titles)):\n",
    "    axs[0,2].errorbar(var_ranges[modeln],tuning_curves[celln,modeln], yerr=tuning_stds[celln,modeln],label=titles[modeln],c=clrs[modeln],lw=4,elinewidth=3)\n",
    "axs[0,2].set_ylim(bottom=0,top=np.max(ax_ylims,axis=1)[celln]+2*np.max(tuning_stds,axis=(1,2))[celln])\n",
    "axs[0,2].set_xlim(-50,50)\n",
    "axs[0,2].set_xlabel('Angle ($ ^{\\degree}$)')\n",
    "axs[0,2].set_ylabel('Spikes/s')\n",
    "axs[0,2].set_title('Tuning Curves')\n",
    "axs[0,2].legend(bbox_to_anchor=(1.01, 1), fontsize=12)\n",
    "\n",
    "\n",
    "# Set up predicted spike range between 1-99th percentile\n",
    "stat_bins = 5\n",
    "pred_range = np.quantile(predcell,[.1,.99])\n",
    "msp_range = np.quantile(nspcell,[.1,.99])\n",
    "pred_rangelin = np.linspace(pred_range[0],pred_range[1],stat_bins)\n",
    "\n",
    "# hist,xedges,yedges,img =axs[0,3].hist2d(mpred[celln]/model_dt,msp[celln]/model_dt,range=np.vstack((pred_range,msp_range)))#pred_smooth,msp_smooth)\n",
    "axs[0,3].scatter(mpred[celln]/model_dt,msp[celln]/model_dt,c='k',s=15)\n",
    "axs[0,3].set_xlabel('Predicted Spike Rate')\n",
    "axs[0,3].set_ylabel('Actual Spike Rate')\n",
    "cbar = add_colorbar(img)\n",
    "cbar.set_label('count')\n",
    "\n",
    "mse_add = np.zeros((ncells,len(titles),len(nranges)-1))\n",
    "mse_mult = np.zeros((ncells,len(titles),len(nranges)-1))\n",
    "alpha_add = np.zeros((ncells,len(titles),len(nranges)-1))\n",
    "alpha_mult = np.zeros((ncells,len(titles),len(nranges)-1))\n",
    "\n",
    "traces = np.zeros((ncells,len(titles),len(nranges)-1,stat_bins-1)) # (model_type,quartile,FR)\n",
    "traces_mean = np.zeros((ncells,len(titles),stat_bins-1)) # (model_type,quartile,FR)\n",
    "edges_all = np.zeros((ncells,len(titles),len(nranges)-1,stat_bins-1)) # (model_type,quartile,FR)\n",
    "# df_traces = pd.DataFrame([],columns=['modeln','quartile','FR']) \n",
    "for modeln in range(len(titles)):\n",
    "    metric = move_test[:,modeln]\n",
    "    nranges = np.quantile(metric,[0,.25,.5,.75,1])# np.linspace(np.nanmean(metric)-2*np.nanstd(metric), np.nanmean(metric)+2*np.nanstd(metric),N_bins)\n",
    "    stat_all, edges, _ = binned_statistic(predcell,nspcell, statistic='mean',bins=pred_rangelin)\n",
    "    edge_mids = np.array([(edges[i]+edges[i+1])/2 for i in range(len(edges)-1)])\n",
    "    traces_mean[celln,modeln]=stat_all\n",
    "#     axs[1,modeln].set_xlim(0,pred_range[1]+np.std(pred_range))\n",
    "#     axs[1,modeln].set_ylim(0,np.max(stat)+np.std(stat))\n",
    "    for n in range(len(nranges)-1):\n",
    "        ind = np.where(((metric<=nranges[n+1])&(metric>nranges[n])))[0]\n",
    "        pred = predcell[ind]\n",
    "        sp = nspcell[ind]\n",
    "\n",
    "        stat_range, edges, _ = binned_statistic(pred, sp, statistic='mean',bins=pred_rangelin)\n",
    "        edge_mids = np.array([(edges[i]+edges[i+1])/2 for i in range(len(edges)-1)])\n",
    "        traces[celln,modeln,n]=stat_range\n",
    "        edges_all[celln,modeln,n]=edge_mids\n",
    "        res_add = minimize_scalar(f_add,args=(stat_range,stat_all))\n",
    "        res_mult = minimize_scalar(f_mult,args=(stat_range,stat_all))\n",
    "        mse_add[celln, modeln, n] = res_add.fun\n",
    "        mse_mult[celln, modeln, n] = res_mult.fun\n",
    "        alpha_add[celln, modeln, n] = res_add.x\n",
    "        alpha_mult[celln, modeln, n] = res_mult.x\n",
    "        \n",
    "        axs[1,modeln].plot(edge_mids, stat_range,'.-', c=colors[n],label='{:.02f} : {:.02f}'.format(nranges[n],nranges[n+1]),lw=4,ms=20)\n",
    "        axs[1,modeln].plot(np.linspace(pred_range[0],pred_range[1]),np.linspace(pred_range[0],pred_range[1]),'k--',zorder=0)\n",
    "        axs[1,modeln].set_title('Metric: {}'.format(titles[modeln]), color=clrs[modeln])\n",
    "        axs[1,modeln].set_xlabel('Predicted Spike Rate')\n",
    "        axs[1,modeln].set_ylabel('Actual Spike Rate')\n",
    "#     axs[1,modeln].plot(edge_mids, stat_all,'.-', c='k', lw=5, ms=20, label='All_data')\n",
    "    axs[1,modeln].legend(loc='upper left', fontsize=12)\n",
    "    axs[1,modeln].axis('equal')\n",
    "\n",
    "dmodel = mse_add[celln]-mse_mult[celln]\n",
    "crange = np.max(np.abs(dmodel))\n",
    "im = axs[1,-1].imshow(dmodel,cmap='seismic',vmin=-crange,vmax=crange)\n",
    "axs[1,-1].set_yticks(np.arange(0,4))\n",
    "axs[1,-1].set_yticklabels(titles)\n",
    "axs[1,-1].set_ylabel('Movement Model')\n",
    "axs[1,-1].set_xticks(np.arange(0,4))\n",
    "axs[1,-1].set_xticklabels(['.25','.5','.75','1'])\n",
    "axs[1,-1].set_xlabel('Quantile Range')\n",
    "axs[1,-1].set_title('$MSE_{add}$ - $MSE_{mult}$')\n",
    "cbar = add_colorbar(im)\n",
    "plt.tight_layout()\n",
    "\n",
    "fig.savefig(FigPath/'CellSummary_N{}.png'.format(celln), facecolor='white', transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rubber-leonard",
   "metadata": {},
   "source": [
    "### Plotting Temporal Fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bridal-michigan",
   "metadata": {},
   "outputs": [],
   "source": [
    "celln=51\n",
    "bin_length=40\n",
    "ncells=model_nsp.shape[-1]\n",
    "colors = plt.cm.winter(np.linspace(0,1,4))\n",
    "clrs = ['blue','orange','green','red']\n",
    "fig, axs = plt.subplots(3,5, figsize=((35,15))) #np.floor(7.5*len(model_nsp)).astype(int)\n",
    "predcell = mpred[celln]/model_dt\n",
    "nspcell = msp[celln]/model_dt\n",
    "msp_smooth=(np.convolve(msp[celln], np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "pred_smooth=(np.convolve(mpred[celln], np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "axs[1,0].plot(msp_smooth,'k',label='test FR')\n",
    "axs[1,0].plot(pred_smooth,'r', label='pred FR')\n",
    "axs[1,0].set_xlabel('Frame #')\n",
    "axs[1,0].set_ylabel('Firing Rate (spks/s)')\n",
    "axs[1,0].legend()\n",
    "axs[1,0].set_title('Smoothed FRs')\n",
    "\n",
    "crange = np.max(np.abs(msta[celln]))\n",
    "for n in range(nt_glm_lag):\n",
    "    img = axs[0,n].imshow(msta[celln,n],cmap='seismic',vmin=-crange,vmax=crange)\n",
    "    add_colorbar(img)\n",
    "    axs[0,n].axis('off')\n",
    "    axs[0,n].set_title('Lag:{:03d} ms'.format(int(1000*(n-nt_glm_lag+1)*model_dt)))\n",
    "    axs[0,n].axis('off')\n",
    "add_colorbar(img)\n",
    "\n",
    "# Eye Tuning Curve\n",
    "for modeln in range(len(titles)-2):\n",
    "    axs[1,1].errorbar(var_ranges[modeln],tuning_curves[celln,modeln], yerr=tuning_stds[celln,modeln],label=titles[modeln],c=clrs[modeln],lw=4,elinewidth=3)\n",
    "axs[1,1].set_ylim(bottom=0,top=np.max(ax_ylims,axis=1)[celln]+2*np.max(tuning_stds,axis=(1,2))[celln])\n",
    "axs[1,1].set_xlim(-50,50)\n",
    "axs[1,1].set_xlabel('Angle ($ ^{\\degree}$)')\n",
    "axs[1,1].set_ylabel('Spikes/s')\n",
    "axs[1,1].set_title('Eye Tuning Curves')\n",
    "axs[1,1].legend(bbox_to_anchor=(1.01, 1), fontsize=12)\n",
    "\n",
    "# Head Tuning Curves\n",
    "for modeln in range(2,len(titles)):\n",
    "    axs[1,2].errorbar(var_ranges[modeln],tuning_curves[celln,modeln], yerr=tuning_stds[celln,modeln],label=titles[modeln],c=clrs[modeln],lw=4,elinewidth=3)\n",
    "axs[1,2].set_ylim(bottom=0,top=np.max(ax_ylims,axis=1)[celln]+2*np.max(tuning_stds,axis=(1,2))[celln])\n",
    "axs[1,2].set_xlim(-50,50)\n",
    "axs[1,2].set_xlabel('Angle ($ ^{\\degree}$)')\n",
    "axs[1,2].set_ylabel('Spikes/s')\n",
    "axs[1,2].set_title('Head Tuning Curves')\n",
    "axs[1,2].legend(bbox_to_anchor=(1.01, 1), fontsize=12)\n",
    "\n",
    "# Set up predicted spike range between 1-99th percentile\n",
    "stat_bins = 5\n",
    "pred_range = np.quantile(predcell,[.1,.99])\n",
    "msp_range = np.quantile(nspcell,[.1,.99])\n",
    "pred_rangelin = np.linspace(pred_range[0],pred_range[1],stat_bins)\n",
    "quartiles = [0,.25,.5,.75,1]\n",
    "axs[1,3].scatter(mpred[celln]/model_dt,msp[celln]/model_dt,c='k',s=15)\n",
    "axs[1,3].set_xlabel('Predicted Spike Rate')\n",
    "axs[1,3].set_ylabel('Actual Spike Rate')\n",
    "cbar = add_colorbar(img)\n",
    "cbar.set_label('count')\n",
    "\n",
    "hist,xedges,yedges,img =axs[1,4].hist2d(mpred[celln]/model_dt,msp[celln]/model_dt,range=np.vstack((pred_range,msp_range)))#pred_smooth,msp_smooth)\n",
    "axs[1,4].set_xlabel('Predicted Spike Rate')\n",
    "axs[1,4].set_ylabel('Actual Spike Rate')\n",
    "cbar = add_colorbar(img)\n",
    "cbar.set_label('count')\n",
    "\n",
    "\n",
    "mse_add = np.zeros((ncells,len(titles),len(quartiles)-1))\n",
    "mse_mult = np.zeros((ncells,len(titles),len(quartiles)-1))\n",
    "alpha_add = np.zeros((ncells,len(titles),len(quartiles)-1))\n",
    "alpha_mult = np.zeros((ncells,len(titles),len(quartiles)-1))\n",
    "\n",
    "traces = np.zeros((ncells,len(titles),len(quartiles)-1,stat_bins-1)) # (model_type,quartile,FR)\n",
    "traces_mean = np.zeros((ncells,len(titles),stat_bins-1)) # (model_type,quartile,FR)\n",
    "edges_all = np.zeros((ncells,len(titles),len(quartiles)-1,stat_bins-1)) # (model_type,quartile,FR)\n",
    "# df_traces = pd.DataFrame([],columns=['modeln','quartile','FR']) \n",
    "for modeln in range(len(titles)):\n",
    "    metric = move_test[:,modeln]\n",
    "    nranges = np.quantile(metric,quartiles)# np.linspace(np.nanmean(metric)-2*np.nanstd(metric), np.nanmean(metric)+2*np.nanstd(metric),N_bins)\n",
    "    stat_all, edges, _ = binned_statistic(predcell,nspcell, statistic='mean',bins=pred_rangelin)\n",
    "    edge_mids = np.array([(edges[i]+edges[i+1])/2 for i in range(len(edges)-1)])\n",
    "    traces_mean[celln,modeln]=stat_all\n",
    "    max_fr = np.max(stat_all)\n",
    "#     axs[1,modeln].set_xlim(0,pred_range[1]+np.std(pred_range))\n",
    "#     axs[1,modeln].set_ylim(0,np.max(stat)+np.std(stat))\n",
    "\n",
    "    for n in range(len(nranges)-1):\n",
    "        ind = np.where(((metric<=nranges[n+1])&(metric>nranges[n])))[0]\n",
    "        pred = predcell[ind]\n",
    "        sp = nspcell[ind]\n",
    "\n",
    "        stat_range, edges, _ = binned_statistic(pred, sp, statistic='mean',bins=pred_rangelin)\n",
    "        edge_mids = np.array([(edges[i]+edges[i+1])/2 for i in range(len(edges)-1)])\n",
    "        traces[celln,modeln,n]=stat_range\n",
    "        edges_all[celln,modeln,n]=edge_mids\n",
    "        res_add = minimize_scalar(f_add,args=(stat_range/max_fr, stat_all/max_fr))\n",
    "        res_mult = minimize_scalar(f_mult,args=(stat_range/max_fr, stat_all/max_fr))\n",
    "        mse_add[celln, modeln, n] = res_add.fun\n",
    "        mse_mult[celln, modeln, n] = res_mult.fun\n",
    "        alpha_add[celln, modeln, n] = res_add.x\n",
    "        alpha_mult[celln, modeln, n] = res_mult.x\n",
    "\n",
    "        axs[2,modeln].plot(edge_mids, stat_range,'.-', c=colors[n],label='{:.02f} : {:.02f}'.format(nranges[n],nranges[n+1]),lw=4,ms=20)\n",
    "        axs[2,modeln].plot(np.linspace(pred_range[0],pred_range[1]),np.linspace(pred_range[0],pred_range[1]),'k--',zorder=0)\n",
    "        axs[2,modeln].set_title('Metric: {}'.format(titles[modeln]), color=clrs[modeln])\n",
    "        axs[2,modeln].set_xlabel('Predicted Spike Rate')\n",
    "        axs[2,modeln].set_ylabel('Actual Spike Rate')\n",
    "#     axs[1,modeln].plot(edge_mids, stat_all,'.-', c='k', lw=5, ms=20, label='All_data')\n",
    "    axs[2,modeln].legend(loc='upper left', fontsize=12)\n",
    "    axs[2,modeln].axis('equal')\n",
    "\n",
    "dmodel = mse_add[celln]-mse_mult[celln]\n",
    "crange = np.max(np.abs(dmodel))\n",
    "im = axs[2,-1].imshow(dmodel,cmap='seismic',vmin=-crange,vmax=crange)\n",
    "axs[2,-1].set_yticks(np.arange(0,4))\n",
    "axs[2,-1].set_yticklabels(titles)\n",
    "axs[2,-1].set_ylabel('Movement Model')\n",
    "axs[2,-1].set_xticks(np.arange(0,4))\n",
    "axs[2,-1].set_xticklabels(['.25','.5','.75','1'])\n",
    "axs[2,-1].set_xlabel('Quantile Range')\n",
    "axs[2,-1].set_title('$MSE_{add}$ - $MSE_{mult}$')\n",
    "cbar = add_colorbar(im)\n",
    "\n",
    "plt.suptitle('Celln:{}, cc={:.03f}'.format(celln,mcc[celln]),y=1,fontsize=30)\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "# fig.savefig(FigPath/'CellSummary_N{}_T{:02d}.png'.format(celln,nt_glm_lag), facecolor='white', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varied-swaziland",
   "metadata": {},
   "outputs": [],
   "source": [
    "with PdfPages(FigPath/ 'GLM_{}_dt{:03d}_T{:02d}_cellsummary_sig.pdf'.format(model_type,int(model_dt*1000),nt_glm_lag)) as pdf:\n",
    "    for celln in tqdm(range(msp.shape[0])):\n",
    "        if mcc[celln]>.3:\n",
    "            fig, axs = plt.subplots(3,5, figsize=((35,15))) #np.floor(7.5*len(model_nsp)).astype(int)\n",
    "            predcell = mpred[celln]/model_dt\n",
    "            nspcell = msp[celln]/model_dt\n",
    "            msp_smooth=(np.convolve(msp[celln], np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "            pred_smooth=(np.convolve(mpred[celln], np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "            axs[1,0].plot(msp_smooth,'k',label='test FR')\n",
    "            axs[1,0].plot(pred_smooth,'r', label='pred FR')\n",
    "            axs[1,0].set_xlabel('Frame #')\n",
    "            axs[1,0].set_ylabel('Firing Rate (spks/s)')\n",
    "            axs[1,0].legend()\n",
    "            axs[1,0].set_title('Smoothed FRs')\n",
    "\n",
    "            crange = np.max(np.abs(msta[celln]))\n",
    "            for n in range(nt_glm_lag):\n",
    "                img = axs[0,n].imshow(msta[celln,n],cmap='seismic',vmin=-crange,vmax=crange)\n",
    "                add_colorbar(img)\n",
    "                axs[0,n].axis('off')\n",
    "                axs[0,n].set_title('Lag:{:03d} ms'.format(int(1000*(n-nt_glm_lag+1)*model_dt)))\n",
    "                axs[0,n].axis('off')\n",
    "            add_colorbar(img)\n",
    "\n",
    "            # Eye Tuning Curve\n",
    "            for modeln in range(len(titles)-2):\n",
    "                axs[1,1].errorbar(var_ranges[modeln],tuning_curves[celln,modeln], yerr=tuning_stds[celln,modeln],label=titles[modeln],c=clrs[modeln],lw=4,elinewidth=3)\n",
    "            axs[1,1].set_ylim(bottom=0,top=np.max(ax_ylims,axis=1)[celln]+2*np.max(tuning_stds,axis=(1,2))[celln])\n",
    "            axs[1,1].set_xlim(-50,50)\n",
    "            axs[1,1].set_xlabel('Angle ($ ^{\\degree}$)')\n",
    "            axs[1,1].set_ylabel('Spikes/s')\n",
    "            axs[1,1].set_title('Eye Tuning Curves')\n",
    "            axs[1,1].legend(bbox_to_anchor=(1.01, 1), fontsize=12)\n",
    "            \n",
    "            # Head Tuning Curves\n",
    "            for modeln in range(2,len(titles)):\n",
    "                axs[1,2].errorbar(var_ranges[modeln],tuning_curves[celln,modeln], yerr=tuning_stds[celln,modeln],label=titles[modeln],c=clrs[modeln],lw=4,elinewidth=3)\n",
    "            axs[1,2].set_ylim(bottom=0,top=np.max(ax_ylims,axis=1)[celln]+2*np.max(tuning_stds,axis=(1,2))[celln])\n",
    "            axs[1,2].set_xlim(-50,50)\n",
    "            axs[1,2].set_xlabel('Angle ($ ^{\\degree}$)')\n",
    "            axs[1,2].set_ylabel('Spikes/s')\n",
    "            axs[1,2].set_title('Head Tuning Curves')\n",
    "            axs[1,2].legend(bbox_to_anchor=(1.01, 1), fontsize=12)\n",
    "            \n",
    "            \n",
    "            # Set up predicted spike range between 1-99th percentile\n",
    "            stat_bins = 5\n",
    "            pred_range = np.quantile(predcell,[.1,.99])\n",
    "            msp_range = np.quantile(nspcell,[.1,.99])\n",
    "            pred_rangelin = np.linspace(pred_range[0],pred_range[1],stat_bins)\n",
    "            quartiles = [0,.25,.5,.75,1]\n",
    "            axs[1,3].scatter(mpred[celln]/model_dt,msp[celln]/model_dt,c='k',s=15)\n",
    "            axs[1,3].set_xlabel('Predicted Spike Rate')\n",
    "            axs[1,3].set_ylabel('Actual Spike Rate')\n",
    "            cbar = add_colorbar(img)\n",
    "            cbar.set_label('count')\n",
    "\n",
    "            hist,xedges,yedges,img =axs[1,4].hist2d(mpred[celln]/model_dt,msp[celln]/model_dt,range=np.vstack((pred_range,msp_range)))#pred_smooth,msp_smooth)\n",
    "            axs[1,4].set_xlabel('Predicted Spike Rate')\n",
    "            axs[1,4].set_ylabel('Actual Spike Rate')\n",
    "            cbar = add_colorbar(img)\n",
    "            cbar.set_label('count')\n",
    "\n",
    "\n",
    "            mse_add = np.zeros((ncells,len(titles),len(quartiles)-1))\n",
    "            mse_mult = np.zeros((ncells,len(titles),len(quartiles)-1))\n",
    "            alpha_add = np.zeros((ncells,len(titles),len(quartiles)-1))\n",
    "            alpha_mult = np.zeros((ncells,len(titles),len(quartiles)-1))\n",
    "\n",
    "            traces = np.zeros((ncells,len(titles),len(quartiles)-1,stat_bins-1)) # (model_type,quartile,FR)\n",
    "            traces_mean = np.zeros((ncells,len(titles),stat_bins-1)) # (model_type,quartile,FR)\n",
    "            edges_all = np.zeros((ncells,len(titles),len(quartiles)-1,stat_bins-1)) # (model_type,quartile,FR)\n",
    "            # df_traces = pd.DataFrame([],columns=['modeln','quartile','FR']) \n",
    "            for modeln in range(len(titles)):\n",
    "                metric = move_test[:,modeln]\n",
    "                nranges = np.quantile(metric,quartiles)# np.linspace(np.nanmean(metric)-2*np.nanstd(metric), np.nanmean(metric)+2*np.nanstd(metric),N_bins)\n",
    "                stat_all, edges, _ = binned_statistic(predcell,nspcell, statistic='mean',bins=pred_rangelin)\n",
    "                edge_mids = np.array([(edges[i]+edges[i+1])/2 for i in range(len(edges)-1)])\n",
    "                traces_mean[celln,modeln]=stat_all\n",
    "                max_fr = np.max(stat_all)\n",
    "            #     axs[1,modeln].set_xlim(0,pred_range[1]+np.std(pred_range))\n",
    "            #     axs[1,modeln].set_ylim(0,np.max(stat)+np.std(stat))\n",
    "\n",
    "                for n in range(len(nranges)-1):\n",
    "                    ind = np.where(((metric<=nranges[n+1])&(metric>nranges[n])))[0]\n",
    "                    pred = predcell[ind]\n",
    "                    sp = nspcell[ind]\n",
    "\n",
    "                    stat_range, edges, _ = binned_statistic(pred, sp, statistic='mean',bins=pred_rangelin)\n",
    "                    edge_mids = np.array([(edges[i]+edges[i+1])/2 for i in range(len(edges)-1)])\n",
    "                    traces[celln,modeln,n]=stat_range\n",
    "                    edges_all[celln,modeln,n]=edge_mids\n",
    "                    res_add = minimize_scalar(f_add,args=(stat_range/max_fr, stat_all/max_fr))\n",
    "                    res_mult = minimize_scalar(f_mult,args=(stat_range/max_fr, stat_all/max_fr))\n",
    "                    mse_add[celln, modeln, n] = res_add.fun\n",
    "                    mse_mult[celln, modeln, n] = res_mult.fun\n",
    "                    alpha_add[celln, modeln, n] = res_add.x\n",
    "                    alpha_mult[celln, modeln, n] = res_mult.x\n",
    "\n",
    "                    axs[2,modeln].plot(edge_mids, stat_range,'.-', c=colors[n],label='{:.02f} : {:.02f}'.format(nranges[n],nranges[n+1]),lw=4,ms=20)\n",
    "                    axs[2,modeln].plot(np.linspace(pred_range[0],pred_range[1]),np.linspace(pred_range[0],pred_range[1]),'k--',zorder=0)\n",
    "                    axs[2,modeln].set_title('Metric: {}'.format(titles[modeln]), color=clrs[modeln])\n",
    "                    axs[2,modeln].set_xlabel('Predicted Spike Rate')\n",
    "                    axs[2,modeln].set_ylabel('Actual Spike Rate')\n",
    "            #     axs[1,modeln].plot(edge_mids, stat_all,'.-', c='k', lw=5, ms=20, label='All_data')\n",
    "                axs[2,modeln].legend(loc='upper left', fontsize=12)\n",
    "                axs[2,modeln].axis('equal')\n",
    "\n",
    "            dmodel = mse_add[celln]-mse_mult[celln]\n",
    "            crange = np.max(np.abs(dmodel))\n",
    "            im = axs[2,-1].imshow(dmodel,cmap='seismic',vmin=-crange,vmax=crange)\n",
    "            axs[2,-1].set_yticks(np.arange(0,4))\n",
    "            axs[2,-1].set_yticklabels(titles)\n",
    "            axs[2,-1].set_ylabel('Movement Model')\n",
    "            axs[2,-1].set_xticks(np.arange(0,4))\n",
    "            axs[2,-1].set_xticklabels(['.25','.5','.75','1'])\n",
    "            axs[2,-1].set_xlabel('Quantile Range')\n",
    "            axs[2,-1].set_title('$MSE_{add}$ - $MSE_{mult}$')\n",
    "            cbar = add_colorbar(im)\n",
    "\n",
    "            plt.suptitle('Celln:{}, cc={:.03f}'.format(celln,mcc[celln]),y=1,fontsize=30)\n",
    "            plt.tight_layout()\n",
    "\n",
    "            pdf.savefig()\n",
    "            plt.close()\n",
    "        \n",
    "# fig.savefig(FigPath/'CellSummary_N{}.png'.format(celln), facecolor='white', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "introductory-little",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1\n",
    "model_n=-1\n",
    "celln=51\n",
    "# for modeln in range(len(titles)):\n",
    "traces = np.zeros((ncells,len(titles),len(quartiles)-1,stat_bins-1)) # (model_type,quartile,FR)\n",
    "traces_mean = np.zeros((ncells,len(titles),stat_bins-1)) # (model_type,quartile,FR)\n",
    "edges_all = np.zeros((ncells,len(titles),len(quartiles)-1,stat_bins-1)) # (model_type,quartile,FR)\n",
    "metric = move_test[:,modeln]\n",
    "nranges = np.quantile(metric,[0,.25,.5,.75,1])# np.linspace(np.nanmean(metric)-2*np.nanstd(metric), np.nanmean(metric)+2*np.nanstd(metric),N_bins)\n",
    "stat_all, edges, _ = binned_statistic(predcell,nspcell, statistic='mean',bins=pred_rangelin)\n",
    "edge_mids = np.array([(edges[i]+edges[i+1])/2 for i in range(len(edges)-1)])\n",
    "#     axs[1,modeln].set_xlim(0,pred_range[1]+np.std(pred_range))\n",
    "#     axs[1,modeln].set_ylim(0,np.max(stat)+np.std(stat))\n",
    "#     for n in range(len(nranges)-1):\n",
    "ind = np.where(((metric<=nranges[n+1])&(metric>nranges[n])))[0]\n",
    "pred = predcell[ind]\n",
    "sp = nspcell[ind]\n",
    "\n",
    "stat_range, edges, _ = binned_statistic(pred, sp, statistic='mean',bins=pred_rangelin)\n",
    "edge_mids = np.array([(edges[i]+edges[i+1])/2 for i in range(len(edges)-1)])\n",
    "traces_mean[celln,modeln]=stat_all\n",
    "\n",
    "traces[celln,modeln,n]=stat_range\n",
    "edges_all[celln,modeln,n]=edge_mids\n",
    "res_add = minimize_scalar(f_add,args=(stat_range,stat_all))\n",
    "res_mult = minimize_scalar(f_mult,args=(stat_range,stat_all))\n",
    "mse_add[celln, modeln, n] = res_add.fun\n",
    "mse_mult[celln, modeln, n] = res_mult.fun\n",
    "alpha_add[celln, modeln, n] = res_add.x\n",
    "alpha_mult[celln, modeln, n] = res_mult.x\n",
    "        \n",
    "#         axs[1,modeln].plot(edge_mids, stat_range,'.-', c=colors[n],label='{:.02f} : {:.02f}'.format(nranges[n],nranges[n+1]),lw=4,ms=20)\n",
    "#         axs[1,modeln].plot(np.linspace(pred_range[0],pred_range[1]),np.linspace(pred_range[0],pred_range[1]),'k--',zorder=0)\n",
    "#         axs[1,modeln].set_title('Metric: {}'.format(titles[modeln]), color=clrs[modeln])\n",
    "#         axs[1,modeln].set_xlabel('Predicted Spike Rate')\n",
    "#         axs[1,modeln].set_ylabel('Actual Spike Rate')\n",
    "#     axs[1,modeln].plot(edge_mids, stat_all,'.-', c='k', lw=5, ms=20, label='All_data')\n",
    "#     axs[1,modeln].legend(loc='upper left', fontsize=12)\n",
    "#     axs[1,modeln].axis('equal')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southwest-calculation",
   "metadata": {},
   "source": [
    "SInce we are optimizing the visual do the elastic net with l1 norm look at RFS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outstanding-italic",
   "metadata": {},
   "source": [
    "Add temporal compoennt for spatiotemporal RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modern-dylan",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_add = minimize_scalar(f_add,args=(stat_range,stat_all),tol=1e-6)\n",
    "res_mult = minimize_scalar(f_mult,args=(stat_range,stat_all),tol=1e-6)\n",
    "mse_add[celln,modeln, n] = res_add.fun\n",
    "mse_mult[celln,modeln, n] = res_mult.fun\n",
    "alpha_add[celln,modeln, n] = res_add.x\n",
    "alpha_mult[celln,modeln, n] = res_mult.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driven-treasury",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_mult.x,alpha_mult[celln,modeln, n] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stretch-idaho",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_add,res_mult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anonymous-engagement",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_add[celln,modeln,n],alpha_mult[celln,modeln,n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norwegian-hospital",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_add[celln,modeln, n], mse_mult[celln,modeln, n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungarian-drilling",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.plot(edge_mids,traces_mean[celln,modeln],'.-', c='k', lw=5, ms=20, label='All_data')\n",
    "plt.plot(edge_mids,(traces_mean[celln,modeln]*alpha_mult[celln,modeln,n]).T,'--', label='MultFit',c=colors[n],lw=4,ms=20)\n",
    "plt.plot(edge_mids,(traces_mean[celln,modeln]+alpha_add[celln,modeln,n]).T,'-.', label='AddFit', c=colors[n],lw=4,ms=20)\n",
    "plt.plot(edge_mids, traces[celln,modeln,n],'.-', c=colors[n],label='{:.02f} : {:.02f}'.format(nranges[n],nranges[n+1]),lw=4,ms=20)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grave-appliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.arange(-10,10,.001)\n",
    "mse_add = np.zeros(alphas.shape[0])\n",
    "mse_mult = np.zeros(alphas.shape[0])\n",
    "for ind,alpha in enumerate(alphas):\n",
    "    mse_add[ind] = np.mean((stat_range - stat_all+alpha)**2)\n",
    "    mse_mult[ind] = np.mean((stat_range - stat_all*alpha)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modified-lyric",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(mse_add),np.min(mse_mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aware-housing",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mse_add)\n",
    "plt.plot(mse_mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acquired-accessory",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(stat_all+alphas[np.argmin(mse_add)])\n",
    "plt.plot(stat_all+alphas[np.argmin(mse_mult)])\n",
    "plt.plot(stat_all, 'k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "registered-bride",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(5,5))\n",
    "crange = np.max(np.abs(min_add-min_mult))\n",
    "im = axs.imshow(min_add-min_mult,cmap='seismic',vmin=-crange,vmax=crange,origin='lower')\n",
    "axs.set_yticks(np.arange(0,4))\n",
    "axs.set_yticklabels(titles)\n",
    "axs.set_ylabel('Movement Model')\n",
    "axs.set_xticks(np.arange(0,4))\n",
    "axs.set_xticklabels(['.25','.5','.75','1'])\n",
    "axs.set_xlabel('Quantile Range')\n",
    "axs.set_title('$MSE_{add}$ - $MSE_{mult}$')\n",
    "cbar = add_colorbar(im)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surgical-supply",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(5,5))\n",
    "crange = np.max(np.abs(alpha_mult))\n",
    "im = axs.imshow(alpha_mult,cmap='seismic',vmin=-crange,vmax=crange,origin='lower')\n",
    "axs.set_yticks(np.arange(0,4))\n",
    "axs.set_yticklabels(titles)\n",
    "axs.set_ylabel('Movement Model')\n",
    "axs.set_xticks(np.arange(0,4))\n",
    "axs.set_xticklabels(['.25','.5','.75','1'])\n",
    "axs.set_xlabel('Quantile Range')\n",
    "axs.set_title('$MSE_{add}$ - $MSE_{mult}$')\n",
    "cbar = add_colorbar(im)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handmade-weapon",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Explore Neurons Write to pdf #####\n",
    "with PdfPages(FigPath/ 'MaxCC_GLM_dt{:03d}.pdf'.format(int(model_dt*1000))) as pdf:\n",
    "    for ind in tqdm(range(model_nsp.shape[1])):\n",
    "        fig, axs = plt.subplots(1,3, figsize=((25,6))) #np.floor(7.5*len(model_nsp)).astype(int)\n",
    "        axs[0].plot((np.convolve(msp[ind], np.ones(bin_length), 'same')) / (bin_length * model_dt),'k',label='test FR')\n",
    "        axs[0].plot((np.convolve(mpred[ind], np.ones(bin_length), 'same')) / (bin_length * model_dt),'r', label='pred FR')\n",
    "        axs[0].set_xlabel('Frame #')\n",
    "        axs[0].set_ylabel('Firing Rate (spks/s)')\n",
    "        axs[0].legend()\n",
    "        axs[0].set_title('cc={:.2f}, {}, \\n lag={:d}'.format(mcc[ind],titles_all[m_models[ind]],lag_list[m_lags[ind]]))\n",
    "        img = axs[1].imshow(msta[ind],cmap='seismic')\n",
    "        axs[1].axis('off')\n",
    "        axs[1].set_title('STA,cell: {:d}'.format(ind))\n",
    "        add_colorbar(img)\n",
    "        axs[2].bar(np.arange(mw_move.shape[-1]),mw_move[ind],color='b')\n",
    "        axs[2].set_xticks(np.arange(mw_move.shape[-1]))\n",
    "        axs[2].set_xticklabels(titles)\n",
    "        axs[2].set_ylabel('Movement Weights')\n",
    "        axs[2].axhline(0, color='grey', linewidth=0.8)\n",
    "        plt.tight_layout()\n",
    "        pdf.savefig()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "balanced-quarterly",
   "metadata": {},
   "source": [
    "## Additive GLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passive-hands",
   "metadata": {},
   "source": [
    "### Testing Temporal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powerful-chart",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_data.reshape(train_data.shape[0],-1)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valid-harmony",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_vid; test_data=test_vid\n",
    "lag = 0\n",
    "celln = 51\n",
    "nt_glm_lag=5\n",
    "\n",
    "# Initialize movement combinations\n",
    "titles = np.array(['th','phi','dth','dphi']) # 'roll','pitch'\n",
    "titles_all = []\n",
    "for n in range(1,len(titles)):\n",
    "    perms = np.array(list(itertools.combinations(np.arange(len(titles)), n)))\n",
    "    for ind in range(perms.shape[0]):\n",
    "        titles_all.append('_'.join([t for t in titles[perms[ind]]]))\n",
    "        \n",
    "move_train = np.hstack((train_th[:,np.newaxis],train_phi[:,np.newaxis],train_roll[:,np.newaxis],train_pitch[:,np.newaxis], train_dth[:,np.newaxis],train_dphi[:,np.newaxis]))\n",
    "move_test = np.hstack((test_th[:,np.newaxis],test_phi[:,np.newaxis],test_roll[:,np.newaxis],test_pitch[:,np.newaxis], test_dth[:,np.newaxis],test_dphi[:,np.newaxis]))\n",
    "\n",
    "lag_list = [ -2, -1, 0 , 1, 2]\n",
    "lambdas = 1024 * (2**np.arange(0,16))\n",
    "nks = np.shape(model_vid_sm)[1:]; nk = nks[0]*nks[1]\n",
    "\n",
    "perms = []#np.array(list(itertools.combinations(np.arange(len(titles)), 1)))[1]\n",
    "\n",
    "##### Format data #####\n",
    "# save shape of train_data for initialization\n",
    "nks = np.shape(train_data)[1:]; nk = nks[0]*nks[1]*nt_glm_lag\n",
    "\n",
    "# Shift spikes by -lag for GLM fits\n",
    "sps_train = np.roll(train_nsp[:,celln],-lag)\n",
    "sps_test = np.roll(test_nsp[:,celln],-lag)\n",
    "\n",
    "# Initialize saving movement weights \n",
    "w_move = np.zeros(move_train.shape[1])\n",
    "\n",
    "# Take combination of movements\n",
    "move_train = move_train[:,perms]\n",
    "move_test = move_test[:,perms]\n",
    "\n",
    "# Reshape data (video) into (T*n)xN array\n",
    "x_train = train_data.reshape(train_data.shape[0],-1)\n",
    "x_train = np.hstack([np.roll(x_train, nframes, axis=0) for nframes in reversed(range(nt_glm_lag))])\n",
    "x_train = np.append(x_train, np.ones((x_train.shape[0],1)), axis=1) # append column of ones for fitting intercept\n",
    "# move_train = np.hstack([np.roll(move_train,nframes, axis=0) for nframes in reversed(range(nt_glm_lag))])\n",
    "# x_train = np.concatenate((x_train, move_train),axis=1)\n",
    "\n",
    "x_test = test_data.reshape(test_data.shape[0],-1) \n",
    "x_test = np.hstack([np.roll(x_test,nframes, axis=0) for nframes in reversed(range(nt_glm_lag))])\n",
    "x_test = np.append(x_test,np.ones((x_test.shape[0],1)), axis=1) # append column of ones\n",
    "# move_test = np.hstack([np.roll(move_test,nframes, axis=0) for nframes in reversed(range(nt_glm_lag))])\n",
    "# x_test = np.concatenate((x_test, move_test),axis=1)\n",
    "\n",
    "# Prepare Design Matrix\n",
    "nlam = len(lambdas)\n",
    "XXtr = x_train.T @ x_train\n",
    "XYtr = x_train.T @ sps_train\n",
    "\n",
    "# Initialze mse traces for regularization cross validation\n",
    "msetrain = np.zeros((nlam,1))\n",
    "msetest = np.zeros((nlam,1))\n",
    "w_ridge = np.zeros((x_train.shape[-1],nlam))\n",
    "# Inverse matrix for regularization \n",
    "Cinv = np.eye(nk)\n",
    "Cinv = linalg.block_diag(Cinv,np.zeros((1+move_test.shape[-1], 1+move_test.shape[-1])))\n",
    "# loop over regularization strength\n",
    "for l in range(len(lambdas)):  \n",
    "    # calculate MAP estimate               \n",
    "    w = np.linalg.solve(XXtr + lambdas[l]*Cinv, XYtr) # equivalent of \\ (left divide) in matlab\n",
    "    w_ridge[:,l] = w\n",
    "    # calculate test and training rms error\n",
    "    msetrain[l] = np.mean((sps_train - x_train@w)**2)\n",
    "    msetest[l] = np.mean((sps_test - x_test@w)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stretch-survival",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With temporal filter\n",
    "plt.plot(msetrain)\n",
    "plt.plot(msetest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "creative-technician",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lambda = np.argmin(msetest)\n",
    "w = w_ridge[:,best_lambda]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "backed-onion",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(w[:-(1+move_test.shape[-1])].reshape(20,30))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "falling-homeless",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,nt_glm_lag,figsize=(20,5))\n",
    "for n in range(nt_glm_lag):\n",
    "    img = axs[n].imshow(np.reshape(w[:-(1+move_test.shape[-1])],(nt_glm_lag,)+nks)[n],cmap='seismic')\n",
    "    add_colorbar(img)\n",
    "    axs[n].axis('off')\n",
    "    axs[n].set_title('Lag:{}'.format(n-nt_glm_lag+1))\n",
    "plt.suptitle('Celln:{}'.format(celln),y=.75,fontsize=20)\n",
    "plt.tight_layout()\n",
    "\n",
    "fig.savefig(FigPath/'TemporalRF_N{}.png'.format(celln), facecolor='white', transparent=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "miniature-immunology",
   "metadata": {},
   "outputs": [],
   "source": [
    "w[-move_test.shape[-1]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plastic-logic",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(w[-move_test.shape[-1]:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mineral-harrison",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geographic-citation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select best cross-validated lambda for RF\n",
    "best_lambda = np.argmin(msetest)\n",
    "w = w_ridge[:,best_lambda]\n",
    "ridge_rf = w_ridge[:,best_lambda]\n",
    "sta_all = np.reshape(w[:-(1+move_test.shape[-1])],(nt_glm_lag,)+nks)\n",
    "# w_move[perms] = w[-move_test.shape[-1]:]\n",
    "\n",
    "# predicted firing rate\n",
    "sp_pred = x_test@ridge_rf\n",
    "# bin the firing rate to get smooth rate vs time\n",
    "sp_smooth = (np.convolve(sps_test, np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "pred_smooth = (np.convolve(sp_pred, np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "# a few diagnostics\n",
    "err = np.mean((sp_smooth-pred_smooth)**2)\n",
    "cc = np.corrcoef(sp_smooth, pred_smooth)\n",
    "cc_all = cc[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exposed-strengthening",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sp_smooth)\n",
    "plt.plot(pred_smooth)\n",
    "cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extreme-dress",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def do_glm_temporal_fit(train_nsp, test_nsp, train_data, test_data, move_train, move_test, celln, perms, lag, lambdas, bin_length=40, model_dt=.1,nt_glm_lag=4):\n",
    "    ##### Format data #####\n",
    "    # save shape of train_data for initialization\n",
    "    nks = np.shape(train_data)[1:]; nk = nks[0]*nks[1]\n",
    "    \n",
    "    # Shift spikes by -lag for GLM fits\n",
    "    sps_train = np.roll(train_nsp[:,celln],-lag)\n",
    "    sps_test = np.roll(test_nsp[:,celln],-lag)\n",
    "    \n",
    "    # Initialize saving movement weights \n",
    "    w_move = np.zeros(move_train.shape[1])\n",
    "    \n",
    "    # Take combination of movements\n",
    "    move_train = move_train[:,perms]\n",
    "    move_test = move_test[:,perms]\n",
    "    \n",
    "    # Reshape data (video) into (T*n)xN array\n",
    "    x_train = train_data.reshape(train_data.shape[0],-1)\n",
    "    x_train = np.hstack([np.roll(x_train,nframes) for nframes in range(nt_glm_lag)])\n",
    "    x_train = np.append(x_train, np.ones((x_train.shape[0],1)), axis = 1) # append column of ones for fitting intercept\n",
    "    x_train = np.concatenate((x_train, move_train),axis=1)\n",
    "    \n",
    "    x_test = test_data.reshape(test_data.shape[0],-1) \n",
    "    x_test = np.hstack([np.roll(x_test,nframes) for nframes in range(nt_glm_lag)])\n",
    "    x_test = np.append(x_test,np.ones((x_test.shape[0],1)), axis = 1) # append column of ones\n",
    "    x_test = np.concatenate((x_test, move_test),axis=1)\n",
    "    \n",
    "    # Prepare Design Matrix\n",
    "    nlam = len(lambdas)\n",
    "    XXtr = x_train.T @ x_train\n",
    "    XYtr = x_train.T @ sps_train\n",
    "    \n",
    "    # Initialze mse traces for regularization cross validation\n",
    "    msetrain = np.zeros((nlam,1))\n",
    "    msetest = np.zeros((nlam,1))\n",
    "    w_ridge = np.zeros((x_train.shape[-1],nlam))\n",
    "    # Inverse matrix for regularization \n",
    "    Cinv = np.eye(nk)\n",
    "    Cinv = linalg.block_diag(Cinv,np.zeros((1+move_test.shape[-1], 1+move_test.shape[-1])))\n",
    "    # loop over regularization strength\n",
    "    for l in range(len(lambdas)):  \n",
    "        # calculate MAP estimate               \n",
    "        w = np.linalg.solve(XXtr + lambdas[l]*Cinv, XYtr) # equivalent of \\ (left divide) in matlab\n",
    "        w_ridge[:,l] = w\n",
    "        # calculate test and training rms error\n",
    "        msetrain[l] = np.mean((sps_train - x_train@w)**2)\n",
    "        msetest[l] = np.mean((sps_test - x_test@w)**2)\n",
    "    \n",
    "    # select best cross-validated lambda for RF\n",
    "    best_lambda = np.argmin(msetest)\n",
    "    w = w_ridge[:,best_lambda]\n",
    "    ridge_rf = w_ridge[:,best_lambda]\n",
    "    sta_all = np.reshape(w[:-(1+move_test.shape[-1])],nks)\n",
    "    w_move[perms] = w[-move_test.shape[-1]:]\n",
    "    \n",
    "    # predicted firing rate\n",
    "    sp_pred = x_test@ridge_rf\n",
    "    # bin the firing rate to get smooth rate vs time\n",
    "    sp_smooth = (np.convolve(sps_test, np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "    pred_smooth = (np.convolve(sp_pred, np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "    # a few diagnostics\n",
    "    err = np.mean((sp_smooth-pred_smooth)**2)\n",
    "    cc = np.corrcoef(sp_smooth, pred_smooth)\n",
    "    cc_all = cc[0,1]\n",
    "    \n",
    "    return cc_all, sta_all, sps_test, sp_pred, w_move"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriented-bermuda",
   "metadata": {},
   "source": [
    "### Parallel Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artistic-holmes",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def do_glm_fit(train_nsp, test_nsp, train_data, test_data, move_train, move_test, celln, perms, lag, lambdas, bin_length=40, model_dt=.1):\n",
    "    ##### Format data #####\n",
    "    # save shape of train_data for initialization\n",
    "    nks = np.shape(train_data)[1:]; nk = nks[0]*nks[1]\n",
    "    \n",
    "    # Shift spikes by -lag for GLM fits\n",
    "    sps_train = np.roll(train_nsp[:,celln],-lag)\n",
    "    sps_test = np.roll(test_nsp[:,celln],-lag)\n",
    "    \n",
    "    # Initialize saving movement weights \n",
    "    w_move = np.zeros(move_train.shape[1])\n",
    "    \n",
    "    # Take combination of movements\n",
    "    move_train = move_train[:,perms]\n",
    "    move_test = move_test[:,perms]\n",
    "    \n",
    "    # Reshape data (video) into (T*n)xN array\n",
    "    x_train = train_data.reshape(train_data.shape[0],-1)\n",
    "    x_train = np.append(x_train, np.ones((x_train.shape[0],1)), axis = 1) # append column of ones for fitting intercept\n",
    "    x_train = np.concatenate((x_train, move_train),axis=1)\n",
    "    \n",
    "    x_test = test_data.reshape(test_data.shape[0],-1) \n",
    "    x_test = np.append(x_test,np.ones((x_test.shape[0],1)), axis = 1) # append column of ones\n",
    "    x_test = np.concatenate((x_test, move_test),axis=1)\n",
    "    \n",
    "    # Prepare Design Matrix\n",
    "    nlam = len(lambdas)\n",
    "    XXtr = x_train.T @ x_train\n",
    "    XYtr = x_train.T @ sps_train\n",
    "    \n",
    "    # Initialze mse traces for regularization cross validation\n",
    "    msetrain = np.zeros((nlam,1))\n",
    "    msetest = np.zeros((nlam,1))\n",
    "    w_ridge = np.zeros((x_train.shape[-1],nlam))\n",
    "    # Inverse matrix for regularization \n",
    "    Cinv = np.eye(nk)\n",
    "    Cinv = linalg.block_diag(Cinv,np.zeros((1+move_test.shape[-1], 1+move_test.shape[-1])))\n",
    "    # loop over regularization strength\n",
    "    for l in range(len(lambdas)):  \n",
    "        # calculate MAP estimate               \n",
    "        w = np.linalg.solve(XXtr + lambdas[l]*Cinv, XYtr) # equivalent of \\ (left divide) in matlab\n",
    "        w_ridge[:,l] = w\n",
    "        # calculate test and training rms error\n",
    "        msetrain[l] = np.mean((sps_train - x_train@w)**2)\n",
    "        msetest[l] = np.mean((sps_test - x_test@w)**2)\n",
    "    \n",
    "    # select best cross-validated lambda for RF\n",
    "    best_lambda = np.argmin(msetest)\n",
    "    w = w_ridge[:,best_lambda]\n",
    "    ridge_rf = w_ridge[:,best_lambda]\n",
    "    sta_all = np.reshape(w[:-(1+move_test.shape[-1])],nks)\n",
    "    w_move[perms] = w[-move_test.shape[-1]:]\n",
    "    \n",
    "    # predicted firing rate\n",
    "    sp_pred = x_test@ridge_rf\n",
    "    # bin the firing rate to get smooth rate vs time\n",
    "    sp_smooth = (np.convolve(sps_test, np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "    pred_smooth = (np.convolve(sp_pred, np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "    # a few diagnostics\n",
    "    err = np.mean((sp_smooth-pred_smooth)**2)\n",
    "    cc = np.corrcoef(sp_smooth, pred_smooth)\n",
    "    cc_all = cc[0,1]\n",
    "    \n",
    "    return cc_all, sta_all, sps_test, sp_pred, w_move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cathedral-needle",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Initialize movement combinations\n",
    "titles = np.array(['th','phi','dth','dphi']) # 'roll','pitch'\n",
    "titles_all = []\n",
    "for n in range(1,len(titles)):\n",
    "    perms = np.array(list(itertools.combinations(np.arange(len(titles)), n)))\n",
    "    for ind in range(perms.shape[0]):\n",
    "        titles_all.append('_'.join([t for t in titles[perms[ind]]]))\n",
    "        \n",
    "move_train = np.hstack((train_th[:,np.newaxis],train_phi[:,np.newaxis],train_roll[:,np.newaxis],train_pitch[:,np.newaxis], train_dth[:,np.newaxis],train_dphi[:,np.newaxis]))\n",
    "move_test = np.hstack((test_th[:,np.newaxis],test_phi[:,np.newaxis],test_roll[:,np.newaxis],test_pitch[:,np.newaxis], test_dth[:,np.newaxis],test_dphi[:,np.newaxis]))\n",
    "\n",
    "lag_list = [ -2, -1, 0 , 2, 2]\n",
    "lambdas = 1024 * (2**np.arange(0,16))\n",
    "nks = np.shape(model_vid_sm)[1:]; nk = nks[0]*nks[1]\n",
    "\n",
    "# Put data into shared memory for parallization \n",
    "train_nsp_r = ray.put(train_nsp)\n",
    "test_nsp_r = ray.put(test_nsp)\n",
    "train_data_r = ray.put(train_vid)\n",
    "test_data_r = ray.put(test_vid)\n",
    "move_train_r = ray.put(move_train)\n",
    "move_test_r = ray.put(move_test)\n",
    "result_ids = []\n",
    "# Loop over parameters appending process ids\n",
    "for celln in range(train_nsp.shape[1]):\n",
    "    for n in range(1,len(titles)):\n",
    "        perms = np.array(list(itertools.combinations(np.arange(len(titles), n)))\n",
    "        for ind in range(perms.shape[0]):\n",
    "            for lag_ind, lag in enumerate(lag_list):    \n",
    "                result_ids.append(do_glm_fit.remote(train_nsp_r, test_nsp_r, train_data_r, test_data_r, move_train_r, move_test_r, celln, perms[ind], lag, lambdas, model_dt=model_dt))\n",
    "\n",
    "print('N_proc:', len(result_ids))\n",
    "results_p = ray.get(result_ids)\n",
    "print('GLM Add: ', time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disabled-editing",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Gather Data and Find Max CC Model #####\n",
    "cc_all = np.stack([results_p[i][0] for i in range(len(results_p))])\n",
    "sta_all = np.stack([results_p[i][1] for i in range(len(results_p))])\n",
    "sp_raw = np.stack([results_p[i][2] for i in range(len(results_p))])\n",
    "pred_raw = np.stack([results_p[i][3] for i in range(len(results_p))])\n",
    "w_move_all = np.stack([results_p[i][4] for i in range(len(results_p))])\n",
    "\n",
    "cc_all = cc_all.reshape((model_nsp.shape[1],len(titles_all),len(lag_list),) + cc_all.shape[1:])\n",
    "sta_all = sta_all.reshape((model_nsp.shape[1],len(titles_all),len(lag_list),) + sta_all.shape[1:])\n",
    "sp_raw = sp_raw.reshape((model_nsp.shape[1],len(titles_all),len(lag_list),) + sp_raw.shape[1:])\n",
    "pred_raw = pred_raw.reshape((model_nsp.shape[1],len(titles_all),len(lag_list),) + pred_raw.shape[1:])\n",
    "w_move_all = w_move_all.reshape((model_nsp.shape[1],len(titles_all),len(lag_list),) + w_move_all.shape[1:])\n",
    "\n",
    "m_cells, m_models, m_lags = np.where(cc_all==np.max(cc_all,axis=(-2,-1), keepdims=True))\n",
    "\n",
    "mcc = cc_all[m_cells,m_models,m_lags]\n",
    "msta = sta_all[m_cells,m_models,m_lags]\n",
    "msp = sp_raw[m_cells,m_models,m_lags]\n",
    "mpred = pred_raw[m_cells,m_models,m_lags]\n",
    "mw_move = w_move_all[m_cells,m_models,m_lags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "leading-jonathan",
   "metadata": {},
   "outputs": [],
   "source": [
    "GLM_add = {'cc_all': cc_all,\n",
    "            'sta_all': sta_all,\n",
    "            'sp_raw': sp_raw,\n",
    "            'pred_raw': pred_raw,\n",
    "            'w_move_all': w_move_all,}\n",
    "ioh5.save(save_dir/'Add_GLM_Data_notsmooth_dt{:03d}.h5'.format(int(model_dt*1000)), GLM_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "external-study",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.imshow(msta, animation_frame=0, binary_string=False,color_continuous_scale='RdBu_r')\n",
    "fig.update_layout(width=500,\n",
    "                  height=500,\n",
    "                 )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparative-mexico",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_length=40\n",
    "##### Explore Neurons #####\n",
    "ind = 50\n",
    "fig, axs = plt.subplots(1,3, figsize=((25,6))) #np.floor(7.5*len(model_nsp)).astype(int)\n",
    "axs[0].plot((np.convolve(msp[ind], np.ones(bin_length), 'same')) / (bin_length * model_dt),'k',label='test FR')\n",
    "axs[0].plot((np.convolve(mpred[ind], np.ones(bin_length), 'same')) / (bin_length * model_dt),'r', label='pred FR')\n",
    "axs[0].set_xlabel('Frame #')\n",
    "axs[0].set_ylabel('Firing Rate (spks/s)')\n",
    "axs[0].legend()\n",
    "axs[0].set_title('cc={:.2f}, {}, \\n lag={:d}'.format(mcc[ind],titles_all[m_models[ind]],lag_list[m_lags[ind]]))\n",
    "img = axs[1].imshow(msta[ind],cmap='seismic')\n",
    "axs[1].set_title('STA,cell: {:d}'.format(ind))\n",
    "axs[1].axis('off')\n",
    "add_colorbar(img)\n",
    "axs[2].bar(np.arange(mw_move.shape[-1]),mw_move[ind],color='b')\n",
    "axs[2].set_xticks(np.arange(mw_move.shape[-1]))\n",
    "axs[2].set_xticklabels(titles) # [m_models[ind]].split('_')\n",
    "axs[2].axhline(0, color='grey', linewidth=0.8)\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspected-coral",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Explore Neurons Write to pdf #####\n",
    "with PdfPages(FigPath/ 'MaxCC_GLM_dt{:03d}.pdf'.format(int(model_dt*1000))) as pdf:\n",
    "    for ind in tqdm(range(model_nsp.shape[1])):\n",
    "        fig, axs = plt.subplots(1,3, figsize=((25,6))) #np.floor(7.5*len(model_nsp)).astype(int)\n",
    "        axs[0].plot((np.convolve(msp[ind], np.ones(bin_length), 'same')) / (bin_length * model_dt),'k',label='test FR')\n",
    "        axs[0].plot((np.convolve(mpred[ind], np.ones(bin_length), 'same')) / (bin_length * model_dt),'r', label='pred FR')\n",
    "        axs[0].set_xlabel('Frame #')\n",
    "        axs[0].set_ylabel('Firing Rate (spks/s)')\n",
    "        axs[0].legend()\n",
    "        axs[0].set_title('cc={:.2f}, {}, \\n lag={:d}'.format(mcc[ind],titles_all[m_models[ind]],lag_list[m_lags[ind]]))\n",
    "        img = axs[1].imshow(msta[ind],cmap='seismic')\n",
    "        axs[1].axis('off')\n",
    "        axs[1].set_title('STA,cell: {:d}'.format(ind))\n",
    "        add_colorbar(img)\n",
    "        axs[2].bar(np.arange(mw_move.shape[-1]),mw_move[ind],color='b')\n",
    "        axs[2].set_xticks(np.arange(mw_move.shape[-1]))\n",
    "        axs[2].set_xticklabels(titles)\n",
    "        axs[2].set_ylabel('Movement Weights')\n",
    "        axs[2].axhline(0, color='grey', linewidth=0.8)\n",
    "        plt.tight_layout()\n",
    "        pdf.savefig()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romance-interest",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Plot cell model with lags #####\n",
    "celln=33\n",
    "model_ind=0\n",
    "fig, axs = plt.subplots(2,len(lag_list), figsize=(np.floor(7.5*len(lag_list)).astype(int),10))\n",
    "for lag_ind, lag in enumerate(lag_list):\n",
    "    axs[0,lag_ind].plot(sp_smooth[celln,model_ind,lag_ind],'k',label='smoothed FR')\n",
    "    axs[0,lag_ind].plot(pred_smooth[celln,model_ind,lag_ind],'r', label='pred FR')\n",
    "    axs[0,lag_ind].set_title('cc={:.2f}'.format(cc_all[celln,model_ind,lag_ind]))\n",
    "    axs[1,lag_ind].imshow(sta_all[celln,model_ind,lag_ind])\n",
    "    axs[1,lag_ind].set_title('lag={:d}'.format(lag_list[lag_ind]))\n",
    "    axs[1,lag_ind].axis('off')\n",
    "    plt.suptitle(titles_all[model_ind])\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "internal-blocking",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_all.reshape(n_units,len(lag_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quality-highland",
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure of receptive fields\n",
    "fig = plt.figure(figsize=(25,256),dpi=50)\n",
    "for celln in tqdm(range(n_units)):\n",
    "    for lag_ind, lag in enumerate(lag_list):\n",
    "        crange = np.max(np.abs(sta_all[celln,:,:,:]))\n",
    "        plt.subplot(n_units,6,(celln*6)+lag_ind + 1)  \n",
    "        plt.imshow(sta_all[celln, lag_ind, :, :], vmin=-crange, vmax=crange, cmap='jet')\n",
    "        plt.title('cc={:.2f}'.format (cc_all[celln,lag_ind]),fontsize=5)\n",
    "        plt.axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "successful-formula",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(save_dir/'STA1_5.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "further-moment",
   "metadata": {},
   "source": [
    "## Multiplicitive GLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collect-dressing",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def do_glm_fit_mult(train_nsp, test_nsp, train_data, test_data, move_train, move_test, celln, perms, lag, lambdas, alpha, bin_length=40, model_dt=.1):\n",
    "    sps_train = np.roll(train_nsp[:,celln],-lag)\n",
    "    sps_test = np.roll(test_nsp[:,celln],-lag)\n",
    "    move_train = move_train[:,perms]\n",
    "    move_test = move_test[:,perms]\n",
    "    nks = np.shape(train_data)[1:]; nk = nks[0]*nks[1]\n",
    "\n",
    "    x_train = train_data.reshape(train_data.shape[0],-1)*(1 + alpha*move_train)\n",
    "    x_train = np.append(x_train, np.ones((x_train.shape[0],1)), axis = 1) # append column of ones\n",
    "#     x_train = np.concatenate((x_train, move_train),axis=1) # x_train*(1+alpha*model_th)\n",
    "    \n",
    "    x_test = test_data.reshape(test_data.shape[0],-1)*(1 + alpha*move_test)\n",
    "    x_test = np.append(x_test,np.ones((x_test.shape[0],1)), axis = 1) # append column of ones\n",
    "#     x_test = np.concatenate((x_test, move_test),axis=1)\n",
    "\n",
    "    Cinv = np.eye(nk)\n",
    "    Cinv = linalg.block_diag(Cinv,np.zeros((1,1))) #move_test.shape[-1],move_test.shape[-1])\n",
    "    nlam = len(lambdas)\n",
    "    XXtr = x_train.T @ x_train\n",
    "    XYtr = x_train.T @ sps_train\n",
    "    msetrain = np.zeros((nlam,1))\n",
    "    msetest = np.zeros((nlam,1))\n",
    "    w_ridge = np.zeros((x_train.shape[-1],nlam))\n",
    "    \n",
    "    # loop over regularization strength\n",
    "    for l in range(len(lambdas)):  \n",
    "        # calculate MAP estimate               \n",
    "        w = np.linalg.solve(XXtr + lambdas[l]*Cinv, XYtr) # equivalent of \\ (left divide) in matlab\n",
    "        w_ridge[:,l] = w\n",
    "        # calculate test and training rms error\n",
    "        msetrain[l] = np.mean((sps_train - x_train@w)**2)\n",
    "        msetest[l] = np.mean((sps_test - x_test@w)**2)\n",
    "        \n",
    "    # select best cross-validated lambda for RF\n",
    "    best_lambda = np.argmin(msetest)\n",
    "    w = w_ridge[:,best_lambda]\n",
    "    ridge_rf = w_ridge[:,best_lambda]\n",
    "    sta_all = np.reshape(w[:-(move_test.shape[-1])],nks)\n",
    "    # plot predicted vs actual firing rate\n",
    "    # predicted firing rate\n",
    "    sp_pred = x_test@ridge_rf\n",
    "    # bin the firing rate to get smooth rate vs time\n",
    "    sp_smooth = (np.convolve(sps_test, np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "    pred_smooth = (np.convolve(sp_pred, np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "    # a few diagnostics\n",
    "    err = np.mean((sp_smooth-pred_smooth)**2)\n",
    "    cc = np.corrcoef(sp_smooth, pred_smooth)\n",
    "    cc_all = cc[0,1]\n",
    "    return cc_all, sta_all, sps_test, sp_pred, msetest[best_lambda]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alleged-front",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "titles = np.array(['th','phi','roll','pitch','dth','dphi'])\n",
    "titles_all = []\n",
    "for n in range(1,2):\n",
    "    perms = np.array(list(itertools.combinations([0,1,2,3], n)))\n",
    "    for ind in range(perms.shape[0]):\n",
    "        titles_all.append('_'.join([t for t in titles[perms[ind]]]))\n",
    "        \n",
    "move_train = np.hstack((train_th[:,np.newaxis],train_phi[:,np.newaxis],train_roll[:,np.newaxis],train_pitch[:,np.newaxis])) # ,train_dth[:,np.newaxis],train_dphi[:,np.newaxis]))\n",
    "move_test = np.hstack((test_th[:,np.newaxis],test_phi[:,np.newaxis],test_roll[:,np.newaxis],test_pitch[:,np.newaxis])) # ,test_dth[:,np.newaxis],test_dphi[:,np.newaxis]))\n",
    "\n",
    "lag_list = [ -4, -2, 0 , 2, 4]\n",
    "lambdas = 1024 * (2**np.arange(0,16))\n",
    "# alpha_list = np.linspace(-2,2,10)\n",
    "alpha_list = np.arange(-2,2+.5,.5)\n",
    "nks = np.shape(train_vid)[1:]; nk = nks[0]*nks[1]\n",
    "\n",
    "train_nsp_r = ray.put(train_nsp)\n",
    "test_nsp_r = ray.put(test_nsp)\n",
    "train_data_r = ray.put(train_vid)\n",
    "test_data_r = ray.put(test_vid)\n",
    "move_train_r = ray.put(move_train)\n",
    "move_test_r = ray.put(move_test)\n",
    "result_ids = []\n",
    "# celln = 51\n",
    "for celln in range(train_nsp.shape[1]):\n",
    "    for n in range(1,2):\n",
    "        perms = np.array(list(itertools.combinations([0,1,2,3,4,5], n)))\n",
    "        for ind in range(perms.shape[0]):\n",
    "            for lag_ind, lag in enumerate(lag_list):\n",
    "                for alpha in alpha_list:\n",
    "                    result_ids.append(do_glm_fit_mult.remote(train_nsp_r, test_nsp_r, train_data_r, test_data_r, move_train_r, move_test_r, celln, perms[ind], lag, lambdas, alpha))\n",
    "\n",
    "print('N_proc:', len(result_ids))\n",
    "results_p = ray.get(result_ids)                \n",
    "print('GLM Mult: ', time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proper-floor",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Gather Data and Find Max CC Model #####\n",
    "cc_all = np.stack([results_p[i][0] for i in range(len(results_p))])\n",
    "sta_all = np.stack([results_p[i][1] for i in range(len(results_p))])\n",
    "sp_raw = np.stack([results_p[i][2] for i in range(len(results_p))])\n",
    "pred_raw = np.stack([results_p[i][3] for i in range(len(results_p))])\n",
    "mse_test_all = np.stack([results_p[i][4] for i in range(len(results_p))])\n",
    "\n",
    "cc_all = cc_all.reshape((model_nsp.shape[1],len(titles_all),len(lag_list),len(alpha_list)) + cc_all.shape[1:])\n",
    "sta_all = sta_all.reshape((model_nsp.shape[1],len(titles_all),len(lag_list),len(alpha_list)) + sta_all.shape[1:])\n",
    "sp_raw = sp_raw.reshape((model_nsp.shape[1],len(titles_all),len(lag_list),len(alpha_list)) + sp_raw.shape[1:])\n",
    "pred_raw = pred_raw.reshape((model_nsp.shape[1],len(titles_all),len(lag_list),len(alpha_list)) + pred_raw.shape[1:])\n",
    "mse_test_all = mse_test_all.reshape((model_nsp.shape[1],len(titles_all),len(lag_list),len(alpha_list)) + mse_test_all.shape[1:])\n",
    "\n",
    "m_cells, m_models, m_lags, m_alphas = np.where(cc_all==np.max(cc_all,axis=(-3,-2,-1), keepdims=True))\n",
    "\n",
    "mcc = cc_all[m_cells,m_models,m_lags,m_alphas]\n",
    "msta = sta_all[m_cells,m_models,m_lags,m_alphas]\n",
    "msp = sp_raw[m_cells,m_models,m_lags,m_alphas]\n",
    "mpred = pred_raw[m_cells,m_models,m_lags,m_alphas]\n",
    "mmsetest = mse_test_all[m_cells,m_models,m_lags,m_alphas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reflected-philadelphia",
   "metadata": {},
   "outputs": [],
   "source": [
    "GLM_mult = {'cc_all': cc_all,\n",
    "            'sta_all': sta_all,\n",
    "            'sp_raw': sp_raw,\n",
    "            'pred_raw': pred_raw,\n",
    "            'mse_test_all': mse_test_all,}\n",
    "ioh5.save(save_dir/'Mult_GLM_Data_alpha_{:d}_{:d}_notsmooth_dt{:03d}.h5'.format(int(np.abs(np.min(alpha_list))),int(np.max(alpha_list)),int(model_dt*1000)), GLM_mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flexible-hydrogen",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Explore Neurons Write to pdf #####\n",
    "with PdfPages(FigPath/ 'MaxCC_GLM_mult_dt{:03d}.pdf'.format(int(model_dt*1000))) as pdf:\n",
    "    for ind in tqdm(range(model_nsp.shape[1])):\n",
    "        fig, axs = plt.subplots(1,2, figsize=((15,6))) #np.floor(7.5*len(model_nsp)).astype(int)\n",
    "        axs[0].plot((np.convolve(msp[ind], np.ones(bin_length), 'same')) / (bin_length * model_dt),'k',label='test FR')\n",
    "        axs[0].plot((np.convolve(mpred[ind], np.ones(bin_length), 'same')) / (bin_length * model_dt),'r', label='pred FR')\n",
    "        axs[0].legend()\n",
    "        axs[0].set_xlabel('Frame #')\n",
    "        axs[0].set_ylabel('Firing Rate (spks/s)')\n",
    "        axs[0].set_title('cc={:.2f}, {}, \\n lag={:d}, alpha={:.2f}'.format(mcc[ind],titles_all[m_models[ind]],lag_list[m_lags[ind]], alpha_list[m_alphas[ind]]))\n",
    "        img = axs[1].imshow(msta[ind],cmap='seismic')\n",
    "        axs[1].set_title('STA,cell: {:d}'.format(ind))\n",
    "        axs[1].axis('off')\n",
    "        add_colorbar(img)\n",
    "        plt.tight_layout()\n",
    "        pdf.savefig()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baking-reminder",
   "metadata": {},
   "source": [
    "Temporal component\n",
    "\n",
    "dth, dphi, at shorter timescales model_dt = .025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invisible-catch",
   "metadata": {},
   "source": [
    "Create movie with world cam and traces animation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alone-binary",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capable-rover",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_list = np.arange(-2,2+.5,.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premier-revision",
   "metadata": {},
   "source": [
    "## Comparison of Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vietnamese-confusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "GLM_add = ioh5.load(save_dir/'Add_GLM_Data_notsmooth.h5')\n",
    "GLM_mult = ioh5.load(save_dir/'Mult_GLM_Data_alpha_{:d}_{:d}_notsmooth.h5'.format(int(np.abs(np.min(alpha_list))),int(np.max(alpha_list))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polar-hypothesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### GLM Mult #####\n",
    "m_cells_mult, m_models_mult, m_lags_mult, m_alphas_mult = np.where(GLM_mult['cc_all']==np.max(GLM_mult['cc_all'], axis=(-3,-2,-1), keepdims=True))\n",
    "l=list(np.where((GLM_mult['cc_all']==np.max(GLM_mult['cc_all'], axis=(-3,-2,-1), keepdims=True)))[0])\n",
    "indexes = [l.index(x) for x in set(l)]\n",
    "m_cells_mult = m_cells_mult[indexes]\n",
    "m_models_mult = m_models_mult[indexes]\n",
    "m_lags_mult = m_lags_mult[indexes]\n",
    "m_alphas_mult = m_alphas_mult[indexes]\n",
    "\n",
    "mcc_mult = GLM_mult['cc_all'][m_cells_mult,m_models_mult,m_lags_mult,m_alphas_mult]\n",
    "msta_mult = GLM_mult['sta_all'][m_cells_mult,m_models_mult,m_lags_mult,m_alphas_mult]\n",
    "msp_mult = GLM_mult['sp_smooth'][m_cells_mult,m_models_mult,m_lags_mult,m_alphas_mult]\n",
    "mpred_mult = GLM_mult['pred_smooth'][m_cells_mult,m_models_mult,m_lags_mult,m_alphas_mult]\n",
    "mmsetest_mult = GLM_mult['mse_test_all'][m_cells_mult,m_models_mult,m_lags_mult,m_alphas_mult]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authorized-rugby",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### GLM Add #####\n",
    "m_cells_add, m_models_add, m_lags_add = np.where(GLM_add['cc_all']==np.max(GLM_add['cc_all'],axis=(-2,-1), keepdims=True))\n",
    "\n",
    "mcc_add = GLM_add['cc_all'][m_cells_add,m_models_add,m_lags_add]\n",
    "msta_add = GLM_add['sta_all'][m_cells_add,m_models_add,m_lags_add]\n",
    "msp_add = GLM_add['sp_smooth'][m_cells_add,m_models_add,m_lags_add]\n",
    "mpred_add = GLM_add['pred_smooth'][m_cells_add,m_models_add,m_lags_add]\n",
    "mw_move_add = GLM_add['w_move_all'][m_cells_add,m_models_add,m_lags_add]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executed-pittsburgh",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(m_lags_mult)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arranged-thompson",
   "metadata": {},
   "source": [
    "model the response of neurons as a product of the velocity and the spatial gradient\n",
    "\n",
    "gives an idea of what terms in regression model should carry weight \n",
    "\n",
    "\n",
    "9/14 potential meeting with cris, james and me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comprehensive-appreciation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "third-temperature",
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_list = [ -4, -2, 0 , 2, 4]\n",
    "plt.hist(m_lags_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coupled-serum",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmsetest_mult = GLM_mult['mse_test_all'][m_cells_mult,m_models_mult,m_lags_mult,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rotary-welcome",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "duplicate-photographer",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.argmin(mmsetest_mult,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "missing-netherlands",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(mmsetest_mult.shape[0]):\n",
    "    plt.plot(alpha_list, mmsetest_mult[n] - np.mean(mmsetest_mult[n]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patent-oxford",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,1, figsize=(5,5))\n",
    "axs.plot([0,1],[0,1], 'k')\n",
    "axs.scatter(mcc_add ,mcc_mult)\n",
    "axs.set_xlabel('CC Add')\n",
    "axs.set_ylabel('CC Mult.')\n",
    "axs.set_aspect('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innovative-seeking",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(mpred_add[21],msp_add[21], alpha=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "employed-caribbean",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import binned_statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surface-hawaii",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1)\n",
    "for n in range(mpred_add.shape[0]):\n",
    "    stat, edges, _ = binned_statistic(mpred_add[n], msp_add[n], statistic='mean')\n",
    "    edge_mids = [(edges[i]+edges[i+1])/2 for i in range(len(edges)-1)]\n",
    "    ax.scatter(edge_mids, stat)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "leading-guitar",
   "metadata": {},
   "source": [
    "restrict data of th and phi, pitch roll./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compatible-executive",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "precise-fountain",
   "metadata": {},
   "source": [
    "# GLM on PCA of population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "korean-carry",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pcs = pca.fit_transform(model_nsp)\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "comp_to_keep = np.where(np.cumsum(pca.explained_variance_ratio_)>.9)[0][0]\n",
    "plt.axvline(x=comp_to_keep)\n",
    "pca = PCA(n_components=comp_to_keep)\n",
    "pcs = pca.fit_transform(model_nsp)\n",
    "print('keep {} PCs'.format(comp_to_keep))\n",
    "# recon = pca.inverse_transform(pcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "involved-quantity",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pcs = pcs[train_idx]\n",
    "test_pcs = pcs[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viral-closer",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "titles = np.array(['th','phi','roll','pitch'])\n",
    "titles_all = []\n",
    "for n in range(1,5):\n",
    "    perms = np.array(list(itertools.combinations([0,1,2,3], n)))\n",
    "    for ind in range(perms.shape[0]):\n",
    "        titles_all.append('_'.join([t for t in titles[perms[ind]]]))\n",
    "        \n",
    "move_train = np.hstack((train_th[:,np.newaxis],train_phi[:,np.newaxis],train_roll[:,np.newaxis],train_pitch[:,np.newaxis])) # ,train_dth[:,np.newaxis],train_dphi[:,np.newaxis]))\n",
    "move_test = np.hstack((test_th[:,np.newaxis],test_phi[:,np.newaxis],test_roll[:,np.newaxis],test_pitch[:,np.newaxis])) # ,test_dth[:,np.newaxis],test_dphi[:,np.newaxis]))\n",
    "\n",
    "lag_list = [ -4, -2, 0 , 2, 4]\n",
    "lambdas = 1024 * (2**np.arange(0,16))\n",
    "\n",
    "train_nsp_r = ray.put(train_pcs)\n",
    "test_nsp_r = ray.put(test_pcs)\n",
    "train_data_r = ray.put(train_vid)\n",
    "test_data_r = ray.put(test_vid)\n",
    "move_train_r = ray.put(move_train)\n",
    "move_test_r = ray.put(move_test)\n",
    "result_ids = []\n",
    "\n",
    "for celln in range(train_pcs.shape[1]):\n",
    "    for n in range(1,5):\n",
    "        perms = np.array(list(itertools.combinations([0,1,2,3], n)))\n",
    "        for ind in range(perms.shape[0]):\n",
    "            for lag_ind, lag in enumerate(lag_list):    \n",
    "                result_ids.append(do_glm_fit.remote(train_nsp_r, test_nsp_r, train_data_r, test_data_r, move_train_r, move_test_r, celln, perms[ind], lag, lambdas))\n",
    "                      \n",
    "results_p = ray.get(result_ids)\n",
    "print('GLM: ', time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "social-possibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Gather Data and Find Max CC Model #####\n",
    "cc_all = np.stack([results_p[i][0] for i in range(len(results_p))])\n",
    "sta_all = np.stack([results_p[i][1] for i in range(len(results_p))])\n",
    "sp_smooth = np.stack([results_p[i][2] for i in range(len(results_p))])\n",
    "pred_smooth = np.stack([results_p[i][3] for i in range(len(results_p))])\n",
    "w_move_all = np.stack([results_p[i][4] for i in range(len(results_p))])\n",
    "\n",
    "cc_all = cc_all.reshape((pcs.shape[1],len(titles_all),len(lag_list),) + cc_all.shape[1:])\n",
    "sta_all = sta_all.reshape((pcs.shape[1],len(titles_all),len(lag_list),) + sta_all.shape[1:])\n",
    "sp_smooth = sp_smooth.reshape((pcs.shape[1],len(titles_all),len(lag_list),) + sp_smooth.shape[1:])\n",
    "pred_smooth = pred_smooth.reshape((pcs.shape[1],len(titles_all),len(lag_list),) + pred_smooth.shape[1:])\n",
    "w_move_all = w_move_all.reshape((pcs.shape[1],len(titles_all),len(lag_list),) + w_move_all.shape[1:])\n",
    "\n",
    "m_cells, m_models, m_lags = np.where(cc_all==np.max(cc_all,axis=(-2,-1), keepdims=True))\n",
    "\n",
    "mcc = cc_all[m_cells,m_models,m_lags]\n",
    "msta = sta_all[m_cells,m_models,m_lags]\n",
    "msp = sp_smooth[m_cells,m_models,m_lags]\n",
    "mpred = pred_smooth[m_cells,m_models,m_lags]\n",
    "mw_move = w_move_all[m_cells,m_models,m_lags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaptive-magic",
   "metadata": {},
   "outputs": [],
   "source": [
    "GLM_pcs = {'cc_all': cc_all,\n",
    "            'sta_all': sta_all,\n",
    "            'sp_smooth': sp_smooth,\n",
    "            'pred_smooth': pred_smooth,\n",
    "            'w_move_all': w_move_all,}\n",
    "ioh5.save(save_dir/'Add_GLM_PCs_Data.h5', GLM_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constitutional-equity",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.imshow(msta, animation_frame=0, binary_string=False,color_continuous_scale='RdBu_r')\n",
    "fig.update_layout(width=500,\n",
    "                  height=500,\n",
    "                 )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "growing-luther",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Explore Neurons Write to pdf #####\n",
    "with PdfPages(FigPath/ 'MaxCC_pcaSpikes.pdf') as pdf:\n",
    "    for ind in tqdm(range(pcs.shape[1])):\n",
    "        fig, axs = plt.subplots(1,3, figsize=((25,6))) #np.floor(7.5*len(model_nsp)).astype(int)\n",
    "        axs[0].plot(msp[ind],'k',label='test pc{:d}'.format(ind))\n",
    "        axs[0].plot(mpred[ind],'r', label='pred pc{:d}'.format(ind))\n",
    "        axs[0].legend()\n",
    "        axs[0].set_title('cc={:.2f}, {}, \\n lag={:d}'.format(mcc[ind],titles_all[m_models[ind]],lag_list[m_lags[ind]]))\n",
    "        img = axs[1].imshow(msta[ind],cmap='seismic')\n",
    "        axs[1].set_title('STA, PC: {:d}'.format(ind))\n",
    "        axs[1].axis('off')\n",
    "        add_colorbar(img)\n",
    "        axs[2].bar(np.arange(mw_move.shape[-1]),mw_move[ind],color='b')\n",
    "        axs[2].set_xticks(np.arange(mw_move.shape[-1]))\n",
    "        axs[2].set_xticklabels(titles)\n",
    "        axs[2].axhline(0, color='grey', linewidth=0.8)\n",
    "        plt.tight_layout()\n",
    "        pdf.savefig()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considered-artist",
   "metadata": {},
   "source": [
    "spatial gradient as input for GLM mult. by dth/dphi/dgaze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unique-halifax",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 2004\n",
    "plt.figure(figsize=(2,2))\n",
    "plt.imshow(model_vid_sm[t],cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.savefig(FigPath/'ExampleFrame.png',facecolor='white', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animated-cambridge",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaging-maria",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "least-intent",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "composite-closer",
   "metadata": {},
   "source": [
    "# Sequential GLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sporting-marker",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##### Train_Test Split with sklearn #####\n",
    "# model_vid = model_vid_sm\n",
    "# model_dt = .1\n",
    "# nks = np.shape(model_vid)[1:]; nk = nks[0]*nks[1]\n",
    "# nT = np.shape(pcs)[0]\n",
    "# x = model_vid.reshape(pcs.shape[0], -1).copy()\n",
    "# # image dimensions\n",
    "# n_units = np.shape(pcs)[1]\n",
    "\n",
    "# titles = np.array(['th','phi','roll','pitch'])\n",
    "# move_train = np.hstack((train_th[:,np.newaxis],train_phi[:,np.newaxis],train_roll[:,np.newaxis],train_pitch[:,np.newaxis],train_dth[:,np.newaxis],train_dphi[:,np.newaxis]))\n",
    "# move_test = np.hstack((test_th[:,np.newaxis],test_phi[:,np.newaxis],test_roll[:,np.newaxis],test_pitch[:,np.newaxis],test_dth[:,np.newaxis],test_dphi[:,np.newaxis]))\n",
    "\n",
    "# perms = np.array([0,1,2,3]) #np.array(list(itertools.combinations([0,1,2,3], n)))\n",
    "# move_train = move_train[:,perms]\n",
    "# move_test = move_test[:,perms]\n",
    "\n",
    "# # set up prior matrix (regularizer)\n",
    "# # L2 prior\n",
    "# Imat = np.eye(nk)\n",
    "# Imat = linalg.block_diag(Imat,np.zeros((1+move_test.shape[-1],1+move_test.shape[-1])))\n",
    "# # smoothness prior\n",
    "# consecutive = np.ones((nk, 1))\n",
    "# consecutive[nks[1]-1::nks[1]] = 0\n",
    "# diff = np.zeros((1,2))\n",
    "# diff[0,0] = -1\n",
    "# diff[0,1]= 1\n",
    "# Dxx = sparse.diags((consecutive @ diff).T, np.array([0, 1]), (nk-1,nk))\n",
    "# Dxy = sparse.diags((np.ones((nk,1))@ diff).T, np.array([0, nks[1]]), (nk-nks[1], nk))\n",
    "# Dx = Dxx.T @ Dxx + Dxy.T @ Dxy\n",
    "# D  = linalg.block_diag(Dx.toarray(),np.zeros((1+move_test.shape[-1],1+move_test.shape[-1])))   \n",
    "# # summed prior matrix\n",
    "# # Cinv = D + Imat\n",
    "# Cinv = Imat\n",
    "\n",
    "# lag_list = [ -4, -2, 0 , 2, 4]\n",
    "# lambdas = 1024 * (2**np.arange(0,16))\n",
    "# nlam = len(lambdas)\n",
    "# # set up empty arrays for receptive field and cross correlation\n",
    "# sta_all = np.zeros((n_units, len(lag_list), nks[0], nks[1]))\n",
    "# cc_all = np.zeros((n_units,len(lag_list)))\n",
    "\n",
    "# celln = 1\n",
    "# fig, axs = plt.subplots(2,len(lag_list), figsize=(np.floor(7.5*len(lag_list)).astype(int),10))\n",
    "# for lag_ind, lag in enumerate(lag_list):\n",
    "    \n",
    "#     sps_train = np.roll(train_pcs[:,celln],-lag)\n",
    "#     sps_test = np.roll(test_pcs[:,celln],-lag)\n",
    "    \n",
    "   \n",
    "#     #calculate a few terms\n",
    "#     x_train = train_vid.reshape(train_vid.shape[0],-1)\n",
    "#     x_train = np.append(x_train, np.ones((x_train.shape[0],1)), axis = 1) # append column of ones\n",
    "#     x_train = np.concatenate((x_train,move_train),axis=1)\n",
    "\n",
    "#     x_test = test_vid.reshape(test_vid.shape[0],-1)\n",
    "#     x_test = np.append(x_test,np.ones((x_test.shape[0],1)), axis = 1) # append column of ones\n",
    "#     x_test = np.concatenate((x_test,move_test),axis=1)\n",
    "    \n",
    "#     XXtr = x_train.T @ x_train\n",
    "#     XYtr = x_train.T @ sps_train\n",
    "    \n",
    "#     msetrain = np.zeros((nlam,1))\n",
    "#     msetest = np.zeros((nlam,1))\n",
    "#     w_ridge = np.zeros((nk+1+move_test.shape[1],nlam))\n",
    "#     # initial guess\n",
    "#     # loop over regularization strength\n",
    "#     for l in range(len(lambdas)):  \n",
    "#         # calculate MAP estimate               \n",
    "#         w = np.linalg.solve(XXtr + lambdas[l]*Cinv, XYtr) # equivalent of \\ (left divide) in matlab\n",
    "#         w_ridge[:,l] = w\n",
    "#         # calculate test and training rms error\n",
    "#         msetrain[l] = np.mean((sps_train - x_train@w)**2)\n",
    "#         msetest[l] = np.mean((sps_test - x_test@w)**2)\n",
    "#     # select best cross-validated lambda for RF\n",
    "#     best_lambda = np.argmin(msetest)\n",
    "#     w = w_ridge[:,best_lambda]\n",
    "#     ridge_rf = w_ridge[:,best_lambda]\n",
    "#     sta_all[celln,lag_ind,:,:] = np.reshape(w[:-(1+move_test.shape[-1])],nks)\n",
    "#     # plot predicted vs actual firing rate\n",
    "#     # predicted firing rate\n",
    "#     sp_pred = x_test@ridge_rf\n",
    "#     # bin the firing rate to get smooth rate vs time\n",
    "#     bin_length = 40\n",
    "#     sp_smooth = (np.convolve(sps_test, np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "#     pred_smooth = (np.convolve(sp_pred, np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "#     # a few diagnostics\n",
    "#     err = np.mean((sp_smooth-pred_smooth)**2)\n",
    "#     cc = np.corrcoef(sp_smooth, pred_smooth)\n",
    "#     cc_all[celln,lag_ind] = cc[0,1]\n",
    "\n",
    "#     axs[0,lag_ind].plot(sp_smooth,'k',label='smoothed FR')\n",
    "#     axs[0,lag_ind].plot(pred_smooth,'r', label='pred FR')\n",
    "#     axs[0,lag_ind].set_title('cc={:.2f}'.format(cc_all[celln,lag_ind]))\n",
    "#     axs[1,lag_ind].imshow(sta_all[celln,lag_ind])\n",
    "#     axs[1,lag_ind].set_title('lag={:d}'.format(lag_list[lag_ind]))\n",
    "#     axs[1,lag_ind].axis('off')\n",
    "#     plt.suptitle('GLM on PCA spikes')\n",
    "#     plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sixth-baker",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "discrete-banking",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classical-extent",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protected-large",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

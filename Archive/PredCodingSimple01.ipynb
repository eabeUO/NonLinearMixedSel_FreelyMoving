{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "simplified-drain",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anonymous-rapid",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import glob\n",
    "import sys \n",
    "import yaml \n",
    "import glob\n",
    "import h5py \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import io_dict_to_hdf5 as ioh5\n",
    "\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from scipy import interpolate \n",
    "from scipy import signal\n",
    "from pathlib import Path\n",
    "from scipy.linalg import block_diag\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, Subset\n",
    "torch.backends.cudnn.benchmark = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "sys.path.append(str(Path('.').absolute().parent))\n",
    "from utils import check_path, add_colorbar\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "FigPath = check_path(Path('~/Research/SensoryMotorPred_Data').expanduser(),'Figures/PredCodingModel')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fresh-craps",
   "metadata": {},
   "source": [
    "# Set up Input Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rubber-sessions",
   "metadata": {},
   "source": [
    "## Natural Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "typical-instruction",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from tqdm.notebook import tqdm\n",
    "import scipy.io as sio\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjustable-public",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DoG filter as a model of LGN\n",
    "def DoG(img, ksize=(5,5), sigma=1.3, k=1.6):\n",
    "    g1 = cv2.GaussianBlur(img, ksize, sigma)\n",
    "    g2 = cv2.GaussianBlur(img, ksize, k*sigma)\n",
    "    dog = g1 - g2\n",
    "    return (dog - dog.min())/(dog.max()-dog.min())\n",
    "\n",
    "# Gaussian mask for inputs\n",
    "def GaussianMask(sizex=16, sizey=16, sigma=5):\n",
    "    x = np.arange(0, sizex, 1, float)\n",
    "    y = np.arange(0, sizey, 1, float)\n",
    "    x, y = np.meshgrid(x,y)\n",
    "    \n",
    "    x0 = sizex // 2\n",
    "    y0 = sizey // 2\n",
    "    mask = np.exp(-((x-x0)**2 + (y-y0)**2) / (2*(sigma**2)))\n",
    "    return mask / np.sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "renewable-device",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess of inputs\n",
    "num_images = 10\n",
    "num_iter = 5000\n",
    "\n",
    "# datasets from http://www.rctn.org/bruno/sparsenet/\n",
    "mat_images = sio.loadmat(os.path.expanduser('~/Research/Github/PredictiveCoding-RaoBallard-Model/datasets/IMAGES.mat'))\n",
    "imgs = mat_images['IMAGES']\n",
    "mat_images_raw = sio.loadmat(os.path.expanduser('~/Research/Github/PredictiveCoding-RaoBallard-Model/datasets/IMAGES_RAW.mat'))\n",
    "imgs_raw = mat_images_raw['IMAGESr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continued-scientist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot datasets\n",
    "fig = plt.figure(figsize=(8, 4))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.imshow(imgs_raw[:,:,i], cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "fig.suptitle(\"Natural Images\", fontsize=20)\n",
    "plt.subplots_adjust(top=0.9)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "super-nowhere",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocal-supplier",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "coral-choice",
   "metadata": {},
   "source": [
    "## Sim Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "covered-madagascar",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dim = 256\n",
    "hidden_dims = [32,128]\n",
    "n_layers = len(hidden_dims)\n",
    "N = in_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increasing-blair",
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = 3\n",
    "mu = 0\n",
    "x1 = np.arange(-2*N,2*N+1)\n",
    "f1 = (1/(sd*np.sqrt(2*np.pi))*np.exp(-1/2*((x1-mu)/sd)**2))\n",
    "f1 = f1/np.max(f1)\n",
    "\n",
    "L = x1.shape[0]\n",
    "I_mat = np.array([f1[L//2-k:L//2+N-k] for k in np.arange(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perfect-microwave",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2,figsize=(10,5))\n",
    "ax[0].plot(x1,f1)\n",
    "im2 = ax[1].pcolormesh(I_mat)\n",
    "cbar = add_colorbar(im2)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finnish-specialist",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samps=3\n",
    "data_in = np.random.multivariate_normal(mean=np.zeros(I_mat.shape[0]),cov=I_mat,size=(num_samps,))\n",
    "im = plt.pcolormesh(data_in.T)#,aspect='auto')\n",
    "cbar = add_colorbar(im,linewidth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "round-survival",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(data_in[0].reshape(16,16))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stone-april",
   "metadata": {},
   "source": [
    "# Creating Time dependent input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genetic-content",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(image, stepSize, windowSize):\n",
    "    # slide a window across the image\n",
    "    for y in range(0, image.shape[0], stepSize):\n",
    "        for x in range(0, image.shape[1], stepSize):\n",
    "            # yield the current window\n",
    "            yield (x, y, image[y:y + windowSize[1], x:x + windowSize[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "utility-brisbane",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = imgs[:,:,n]\n",
    "winW, winH = (16,16)\n",
    "img2 = []\n",
    "for (x, y, window) in sliding_window(img, stepSize=8, windowSize=(winW, winH)):\n",
    "    # if the window does not meet our desired window size, ignore it\n",
    "    if window.shape[0] != winH or window.shape[1] != winW:\n",
    "        continue\n",
    "    else:\n",
    "        img2.append(window)\n",
    "img2 = np.stack(img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removable-semiconductor",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "m = 2\n",
    "x = 8*(n+1)\n",
    "y = 8*(m+1)\n",
    "# Create figure and axes\n",
    "fig, ax = plt.subplots(1,1, figsize=(10,10))\n",
    "ax.imshow(img, aspect='auto',cmap='gray')\n",
    "# Create a Rectangle patch\n",
    "rect = patches.Rectangle((x, y), 16, 16, linewidth=2, edgecolor='r', facecolor='none')\n",
    "# Add the patch to the Axes\n",
    "ax.add_patch(rect)\n",
    "fig.savefig(FigPath/'NaturalImageEx.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latest-supervision",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clop three inputs\n",
    "inputs = np.array([(gmask*img2[i]).flatten() for i in range(3)])\n",
    "\n",
    "inputs = (inputs - np.mean(inputs)) * input_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diagnostic-parcel",
   "metadata": {},
   "source": [
    "# Direct Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concrete-duration",
   "metadata": {},
   "source": [
    "## Non-Recurrent Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minimal-texture",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RaoBallard1999Model:\n",
    "    def __init__(self, dt=1, sigma2=1, sigma2_td=10):\n",
    "        self.dt = dt\n",
    "        self.inv_sigma2 = 1/sigma2 # 1 / sigma^2        \n",
    "        self.inv_sigma2_td = 1/sigma2_td # 1 / sigma_td^2\n",
    "        \n",
    "        self.k1 = 0.3 # k_1: update rate\n",
    "        self.k2 = 0.2 # k_2: learning rate\n",
    "        \n",
    "        self.lam = 0.02 # sparsity rate\n",
    "        self.alpha = 1\n",
    "        self.alphah = 0.05\n",
    "        \n",
    "        self.num_units_level0 = 256\n",
    "        self.num_units_level1 = 32\n",
    "        self.num_units_level2 = 128\n",
    "        self.num_level1 = 3\n",
    "        \n",
    "        U = np.random.randn(self.num_units_level0, \n",
    "                            self.num_units_level1)\n",
    "        Uh = np.random.randn(int(self.num_level1*self.num_units_level1),\n",
    "                             self.num_units_level2)\n",
    "        self.U = U.astype(np.float32) * np.sqrt(2/(self.num_units_level0+self.num_units_level1))\n",
    "        self.Uh = Uh.astype(np.float32) * np.sqrt(2/(int(self.num_level1*self.num_units_level1)+self.num_units_level2)) \n",
    "                \n",
    "        self.r = np.zeros((self.num_level1, self.num_units_level1))\n",
    "        self.rh = np.zeros((self.num_units_level2))\n",
    "    \n",
    "    def initialize_states(self, inputs):\n",
    "        self.r = inputs @ self.U \n",
    "        self.rh = self.Uh.T @ np.reshape(self.r, (int(self.num_level1*self.num_units_level1)))\n",
    "    \n",
    "    def calculate_total_error(self, error, errorh):\n",
    "        recon_error = self.inv_sigma2*np.sum(error**2) + self.inv_sigma2_td*np.sum(errorh**2)\n",
    "        sparsity_r = self.alpha*np.sum(self.r**2) + self.alphah*np.sum(self.rh**2)\n",
    "        sparsity_U = self.lam*(np.sum(self.U**2) + np.sum(self.Uh**2))\n",
    "        return recon_error + sparsity_r + sparsity_U\n",
    "        \n",
    "    def __call__(self, inputs, training=False):\n",
    "        # inputs : (3, 256)\n",
    "        r_reshaped = np.reshape(self.r, (int(self.num_level1*self.num_units_level1))) # (96)\n",
    "\n",
    "        fx = self.r @ self.U.T\n",
    "        fxh = self.Uh @ self.rh # (96, )\n",
    "        \n",
    "        # Calculate errors\n",
    "        error = inputs - fx # (3, 256)\n",
    "        errorh = r_reshaped - fxh # (96, ) \n",
    "        errorh_reshaped = np.reshape(errorh, (self.num_level1, self.num_units_level1)) # (3, 32)\n",
    "        \n",
    "        g_r = self.alpha * self.r / (1 + self.r**2) # (3, 32)\n",
    "        g_rh = self.alphah * self.rh / (1 + self.rh**2) # (64, )\n",
    "        \n",
    "        # Update r and rh\n",
    "        dr = self.inv_sigma2 * error @ self.U - self.inv_sigma2_td * errorh_reshaped - g_r\n",
    "        drh = self.inv_sigma2_td * self.Uh.T @ errorh - g_rh\n",
    "        \n",
    "        dr = self.k1 * dr\n",
    "        drh = self.k1 * drh\n",
    "        \n",
    "        rr = self.r.copy()\n",
    "        # Updates                \n",
    "        self.r = self.r + dr\n",
    "        self.rh = self.rh + drh\n",
    "        \n",
    "        if training:  \n",
    "            dU = self.inv_sigma2 * error.T @ self.r - 3*self.lam * self.U\n",
    "            dUh = self.inv_sigma2_td * np.outer(errorh, self.rh) - self.lam * self.Uh\n",
    "            \n",
    "            self.U = self.U + self.k2 * dU\n",
    "            self.Uh = self.Uh + self.k2 * dUh\n",
    "            \n",
    "        return error, errorh, dr, drh, self.r, self.rh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooperative-plenty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "model = RaoBallard1999Model()\n",
    "# model = Recurrent_PredCoding()\n",
    "\n",
    "# Simulation constants\n",
    "H, W, num_images = imgs.shape\n",
    "nt_max = 1000 # Maximum number of simulation time\n",
    "eps = 1e-3 # small value which determines convergence\n",
    "input_scale = 40 # scale factor of inputs .05 for sim data, 40 for nat images\n",
    "gmask = GaussianMask() # Gaussian mask\n",
    "error_list = [] # List to save errors\n",
    "r_t = [] # List to save r\n",
    "rh_t = []  # list to save rh\n",
    "U_t = []\n",
    "Uh_t = []\n",
    "J_t = []\n",
    "J_t = []\n",
    "e1, e2 = [], []\n",
    "dr_t, drh_t = [], []\n",
    "\n",
    "winW, winH = (16,16)\n",
    "\n",
    "FigPath = check_path(Path('~/Research/SensoryMotorPred_Data').expanduser(),'Figures/PredCodingModel')\n",
    "data_path = check_path(Path('~/Research/SensoryMotorPred_Data/data/').expanduser(),'PredCodingData/flat')\n",
    "deltaJ_type = 'flat'\n",
    "FigPath = check_path(FigPath, deltaJ_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trained-genome",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for n in range(2): #imgs.shape[-1]\n",
    "    img2 = []\n",
    "    idx = np.random.randint(0, num_images)\n",
    "    img = imgs[:, :, idx]\n",
    "#     img = imgs[:,:,n]\n",
    "    for (x, y, window) in sliding_window(img, stepSize=winW//2, windowSize=(winW, winH)):\n",
    "        # if the window does not meet our desired window size, ignore it\n",
    "        if window.shape[0] != winH or window.shape[1] != winW:\n",
    "            continue\n",
    "        else:\n",
    "            img2.append(window)\n",
    "            \n",
    "    img2 = np.stack(img2)\n",
    "    num_iter = img2.shape[0]-3\n",
    "    pbar = tqdm(range(num_iter))\n",
    "    for iter_ in pbar:\n",
    "    #     # Get images randomly\n",
    "    #     idx = np.random.randint(0, num_images)\n",
    "    #     img = imgs[:, :, idx]\n",
    "\n",
    "    #     # Get the coordinates of the upper left corner of clopping image randomly.\n",
    "    #     beginx = np.random.randint(0, W-27)\n",
    "    #     beginy = np.random.randint(0, H-17)\n",
    "    #     img_clopped = img[beginy:beginy+16, beginx:beginx+26]\n",
    "\n",
    "        # Clop three inputs\n",
    "        inputs = np.array([(gmask*img2[iter_+i]).flatten() for i in range(3)])\n",
    "    #     inputs = np.random.multivariate_normal(mean=np.zeros(I_mat.shape[0]),cov=I_mat,size=(num_samps,))\n",
    "\n",
    "        inputs = (inputs - np.mean(inputs)) * input_scale\n",
    "\n",
    "        # Reset states\n",
    "        model.initialize_states(inputs)\n",
    "\n",
    "        # Input an image patch until latent variables are converged \n",
    "        for i in range(nt_max):\n",
    "            # Update r and rh without update weights \n",
    "            error, errorh, dr, drh, r, rh = model(inputs, training=False)\n",
    "            r_t.append(r)\n",
    "            rh_t.append(rh)\n",
    "            e1.append(error)\n",
    "            e2.append(errorh)\n",
    "            # Compute norm of r and rh\n",
    "            dr_norm = np.linalg.norm(dr, ord=2) \n",
    "            drh_norm = np.linalg.norm(drh, ord=2)\n",
    "            dr_t.append(dr)\n",
    "            drh_t.append(drh)\n",
    "            # Check convergence of r and rh, then update weights\n",
    "            if dr_norm < eps and drh_norm < eps:\n",
    "                error, errorh, dr, drh, r, rh = model(inputs, training=True)\n",
    "                U_t.append(model.U)\n",
    "                Uh_t.append(model.Uh)\n",
    "    #             print('converged after {:d}'.format(i))\n",
    "                break\n",
    "\n",
    "            # If failure to convergence, break and print error\n",
    "            if i >= nt_max-2: \n",
    "                print(\"Error at patch:\", iter_)\n",
    "                print(dr_norm, drh_norm)\n",
    "                break\n",
    "\n",
    "        error_list.append(model.calculate_total_error(error, errorh)) # Append errors\n",
    "\n",
    "        # Decay learning rate         \n",
    "        if iter_ % 40 == 39:\n",
    "            model.k2 /= 1.015\n",
    "\n",
    "        # Print moving average error\n",
    "        if iter_ % 1000 == 999:  \n",
    "            print(\"iter: \"+str(iter_+1)+\"/\"+str(num_iter)+\", Moving error:\", np.mean(error_list[iter_-999:iter_]))\n",
    "        pbar.set_description('total_error: {:.02f}'.format(error_list[-1]), refresh=True)\n",
    "    \n",
    "r_t = np.array(r_t)\n",
    "rh_t = np.array(rh_t)\n",
    "U_t = np.array(U_t)\n",
    "Uh_t = np.array(Uh_t)\n",
    "e1 = np.array(e1)\n",
    "e2 = np.array(e2)\n",
    "dr_t = np.array(dr_t)\n",
    "drh_t = np.array(drh_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deadly-incident",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(x, n=100) :\n",
    "    ret = np.cumsum(x, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n\n",
    "\n",
    "with PdfPages(FigPath/ 'PredCoding_flat.pdf') as pdf:\n",
    "    moving_average_error = moving_average(np.array(error_list))\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    plt.ylabel(\"Error\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.yticks(np.arange(0,5))\n",
    "    plt.ylim([0,5])\n",
    "    plt.plot(np.arange(len(moving_average_error)), moving_average_error)\n",
    "    plt.tight_layout()\n",
    "    pdf.savefig()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot Receptive fields of level 1\n",
    "    fig = plt.figure(figsize=(8, 4))\n",
    "    plt.subplots_adjust(hspace=0.1, wspace=0.1)\n",
    "    for i in range(32):\n",
    "        plt.subplot(4, 8, i+1)\n",
    "        plt.imshow(np.reshape(model.U[:, i], (16, 16)), cmap=\"gray\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    fig.suptitle(\"Receptive fields of level 1\", fontsize=20)\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "    plt.tight_layout()\n",
    "    pdf.savefig()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot Receptive fields of level 2\n",
    "    zero_padding = np.zeros((80, 32))\n",
    "    U0 = np.concatenate((model.U, zero_padding, zero_padding))\n",
    "    U1 = np.concatenate((zero_padding, model.U, zero_padding))\n",
    "    U2 = np.concatenate((zero_padding, zero_padding, model.U))\n",
    "    U_ = np.concatenate((U0, U1, U2), axis = 1)\n",
    "    Uh_ = U_ @ model.Uh  \n",
    "\n",
    "    fig = plt.figure(figsize=(8, 5))\n",
    "    plt.subplots_adjust(hspace=0.1, wspace=0.1)\n",
    "    for i in range(36):\n",
    "        plt.subplot(6, 6, i+1)\n",
    "        plt.imshow(np.reshape(Uh_[:, i], (16, 26), order='F'), cmap=\"gray\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    fig.suptitle(\"Receptive fields of level 2\", fontsize=20)\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "    plt.tight_layout()\n",
    "    pdf.savefig()\n",
    "    plt.show()\n",
    "\n",
    "    fig, axs = plt.subplots(1,2, figsize=(10,5))\n",
    "    axs[0].imshow(np.cov(U_))\n",
    "    axs[0].set_title('Cov(U)')\n",
    "    axs[1].imshow(np.cov(Uh_))\n",
    "    axs[1].set_title('Cov(Uh)')\n",
    "    plt.tight_layout()\n",
    "    pdf.savefig()\n",
    "\n",
    "#     fig, axs = plt.subplots(1,2, figsize=(10,5))\n",
    "#     axs[0].imshow(model.J)\n",
    "#     axs[0].set_title('J Matrix')\n",
    "#     axs[1].imshow(np.cov(model.J))\n",
    "#     axs[1].set_title('Cov(J)')\n",
    "#     plt.tight_layout()\n",
    "#     pdf.savefig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funded-format",
   "metadata": {},
   "source": [
    "## Recurrent Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saving-reputation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recurrent_PredCoding:\n",
    "    def __init__(self, dt=1, sigma2=1, sigma2_td=1.1, deltaJ=True):\n",
    "        self.dt = dt\n",
    "        self.inv_sigma2 = 1/sigma2 # 1 / sigma^2        \n",
    "        self.inv_sigma2_td = 1/sigma2_td # 1 / sigma_td^2\n",
    "        \n",
    "        self.deltaJ = deltaJ\n",
    "        \n",
    "        self.k1 = 0.03 # k_1: update rate\n",
    "        self.k2 = 0.02 # k_2: learning rate\n",
    "        \n",
    "        self.lam = 0.002 # sparsity rate\n",
    "        self.alpha = 1\n",
    "        self.alphah = 0.05\n",
    "        \n",
    "        self.num_units_level0 = 256\n",
    "        self.num_units_level1 = 32\n",
    "        self.num_units_level2 = 128\n",
    "        self.num_level1 = 3\n",
    "        \n",
    "        U = np.random.randn(self.num_units_level0, \n",
    "                            self.num_units_level1)\n",
    "        Uh = np.random.randn(int(self.num_level1*self.num_units_level1),\n",
    "                             self.num_units_level2)\n",
    "        self.U = U.astype(np.float32) * np.sqrt(.5/(self.num_units_level0+self.num_units_level1))\n",
    "        self.Uh = Uh.astype(np.float32) * np.sqrt(.5/(int(self.num_level1*self.num_units_level1)+self.num_units_level2)) \n",
    "        \n",
    "        self.J = np.random.randn(self.num_units_level2,\n",
    "                                 self.num_units_level2).astype(np.float32) * np.sqrt(.5/(int(self.num_units_level2*self.num_units_level1)+self.num_units_level2)) \n",
    "        self.r = np.zeros((self.num_level1, self.num_units_level1))\n",
    "        self.rh = np.zeros((self.num_units_level2))\n",
    "        \n",
    "        self.h = np.zeros((self.num_units_level2,\n",
    "                             self.num_units_level2))\n",
    "        \n",
    "    def initialize_states(self, inputs):\n",
    "        self.r = inputs @ self.U \n",
    "        self.rh = self.Uh.T @ np.reshape(self.r, (int(self.num_level1*self.num_units_level1))) + self.J @ self.rh # np.reshape(self.rh, (int(self.num_level1*self.num_units_level1)))\n",
    "        \n",
    "        \n",
    "    def calculate_total_error(self, error, errorh):\n",
    "        recon_error = self.inv_sigma2*np.sum(error**2) + self.inv_sigma2_td*np.sum(errorh**2)\n",
    "        sparsity_r = self.alpha*np.sum(self.r**2) + self.alphah*np.sum(self.rh**2)\n",
    "        sparsity_U = self.lam*(np.sum(self.U**2) + np.sum(self.Uh**2))\n",
    "        return recon_error + sparsity_r + sparsity_U\n",
    "        \n",
    "    def __call__(self, inputs, training=False):\n",
    "        # inputs : (3, 256)\n",
    "        r_reshaped = np.reshape(self.r, (int(self.num_level1*self.num_units_level1))) # (96)\n",
    "\n",
    "        fx = np.tanh(self.r @ self.U.T)\n",
    "        fxh = np.tanh(self.Uh @ self.rh) # (96, )\n",
    "\n",
    "        # Calculate errors\n",
    "        error = (inputs - fx)*(1-fx**2)#@ # (3, 256)\n",
    "        errorh = (r_reshaped - fxh)*(1-fxh**2)#@ # (96, ) \n",
    "        errorh_reshaped = np.reshape(errorh, (self.num_level1, self.num_units_level1)) # (3, 32)\n",
    "        \n",
    "        g_r = self.alpha * self.r / (1 + self.r**2) # (3, 32)\n",
    "        g_rh = self.alphah * self.rh / (1 + self.rh**2) # (64, )\n",
    "        \n",
    "        temp = np.zeros((len(self.rh),len(self.rh)))\n",
    "        np.fill_diagonal(temp, self.rh)\n",
    "        dh = (self.h + (1-np.tanh(self.J@self.rh)**2)*temp) # self.J@self.h + \n",
    "\n",
    "        # Update r and rh\n",
    "        dr = self.inv_sigma2 * (error) @ self.U - self.inv_sigma2_td * errorh_reshaped - g_r\n",
    "        drh = self.inv_sigma2_td * self.Uh.T @ (errorh) - g_rh + self.inv_sigma2_td * np.tanh(self.J@self.rh)\n",
    "        \n",
    "        dr = self.k1 * dr\n",
    "        drh = self.k1 * drh\n",
    "        \n",
    "        # Updates                \n",
    "        self.r = self.r + dr\n",
    "        self.rh = self.rh + drh\n",
    "        self.h = self.h + dh\n",
    "        \n",
    "        if training:  \n",
    "            dU = self.inv_sigma2 * (error).T  @ self.r - 3*self.lam * self.U\n",
    "            dUh = self.inv_sigma2_td * (np.outer(errorh, self.rh)) - self.lam * self.Uh\n",
    "            \n",
    "            if self.deltaJ == True:\n",
    "                dJ = self.inv_sigma2_td * (errorh ) @ self.Uh * self.h\n",
    "                self.J = self.J + self.k2*.1 * dJ # self.k2 \n",
    "\n",
    "            self.U = self.U + self.k2 * dU\n",
    "            self.Uh = self.Uh + self.k2 * dUh\n",
    "        return error, errorh, dr, drh, self.r, self.rh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solved-length",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "# model = RaoBallard1999Model()\n",
    "deltaJ = True\n",
    "model = Recurrent_PredCoding(deltaJ=deltaJ)\n",
    "\n",
    "# Simulation constants\n",
    "H, W, num_images = imgs.shape\n",
    "nt_max = 1000 # Maximum number of simulation time\n",
    "eps = 1e-3 # small value which determines convergence\n",
    "input_scale = 40 # scale factor of inputs .05 for sim data, 40 for nat images\n",
    "gmask = GaussianMask() # Gaussian mask\n",
    "error_list = [] # List to save errors\n",
    "r_t = [] # List to save r\n",
    "rh_t = []  # list to save rh\n",
    "U_t = []\n",
    "Uh_t = []\n",
    "J_t = []\n",
    "J_t = []\n",
    "e1, e2 = [], []\n",
    "dr_t, drh_t = [], []\n",
    "\n",
    "winW, winH = (16,16)\n",
    "\n",
    "FigPath = check_path(Path('~/Research/SensoryMotorPred_Data').expanduser(),'Figures/PredCodingModel')\n",
    "# deltaJ = False\n",
    "if deltaJ == True:\n",
    "    data_path = check_path(Path('~/Research/SensoryMotorPred_Data/data/').expanduser(),'PredCodingData/deltaJ')\n",
    "    deltaJ_type = 'deltaJ'\n",
    "    FigPath = check_path(FigPath, deltaJ_type)\n",
    "else:\n",
    "    data_path = check_path(Path('~/Research/SensoryMotorPred_Data/data/').expanduser(),'PredCodingData/randJ')\n",
    "    deltaJ_type = 'randJ'\n",
    "    FigPath = check_path(FigPath, deltaJ_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "senior-daughter",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for n in range(2): #imgs.shape[-1]\n",
    "    img2 = []\n",
    "    idx = np.random.randint(0, num_images)\n",
    "    img = imgs[:, :, idx]\n",
    "#     img = imgs[:,:,n]\n",
    "    for (x, y, window) in sliding_window(img, stepSize=winW//2, windowSize=(winW, winH)):\n",
    "        # if the window does not meet our desired window size, ignore it\n",
    "        if window.shape[0] != winH or window.shape[1] != winW:\n",
    "            continue\n",
    "        else:\n",
    "            img2.append(window)\n",
    "            \n",
    "    img2 = np.stack(img2)\n",
    "    num_iter = img2.shape[0]-3\n",
    "#     num_iter = 10\n",
    "    pbar = tqdm(range(num_iter))\n",
    "    for iter_ in pbar:\n",
    "        # Clop three inputs\n",
    "        inputs = np.array([(gmask*img2[iter_+i]).flatten() for i in range(3)])\n",
    "\n",
    "        inputs = (inputs - np.mean(inputs)) * input_scale\n",
    "\n",
    "        # Reset states\n",
    "        model.initialize_states(inputs)\n",
    "    \n",
    "        # Input an image patch until latent variables are converged \n",
    "        for i in range(nt_max):\n",
    "            # Update r and rh without update weights \n",
    "            error, errorh, dr, drh, r, rh = model(inputs, training=False)\n",
    "            r_t.append(r)\n",
    "            rh_t.append(rh)\n",
    "            \n",
    "            e1.append(error)\n",
    "            e2.append(errorh)\n",
    "            \n",
    "            # Compute norm of r and rh\n",
    "            dr_norm = np.linalg.norm(dr, ord=2) \n",
    "            drh_norm = np.linalg.norm(drh, ord=2)\n",
    "            dr_t.append(dr)\n",
    "            drh_t.append(drh)\n",
    "            # Check convergence of r and rh, then update weights\n",
    "            if dr_norm < eps and drh_norm < eps:\n",
    "                error, errorh, dr, drh, r, rh = model(inputs, training=True)\n",
    "                U_t.append(model.U)\n",
    "                Uh_t.append(model.Uh)\n",
    "                J_t.append(model.J)\n",
    "    #             print('converged after {:d}'.format(i))\n",
    "                break\n",
    "\n",
    "            # If failure to convergence, break and print error\n",
    "            if i >= nt_max-2: \n",
    "                print(\"Error at patch:\", iter_)\n",
    "                print(dr_norm, drh_norm)\n",
    "                break\n",
    "\n",
    "        error_list.append(model.calculate_total_error(error, errorh)) # Append errors\n",
    "\n",
    "        # Decay learning rate         \n",
    "        if iter_ % 40 == 39:\n",
    "            model.k2 /= 1.015\n",
    "\n",
    "        # Print moving average error\n",
    "        if iter_ % 1000 == 999:  \n",
    "            print(\"iter: \"+str(iter_+1)+\"/\"+str(num_iter)+\", Moving error:\", np.mean(error_list[iter_-999:iter_]))\n",
    "        pbar.set_description('total_error: {:.02f}'.format(error_list[-1]), refresh=True)\n",
    "    \n",
    "r_t = np.array(r_t)\n",
    "rh_t = np.array(rh_t)\n",
    "U_t = np.array(U_t)\n",
    "Uh_t = np.array(Uh_t)\n",
    "e1 = np.array(e1)\n",
    "e2 = np.array(e2)\n",
    "dr_t = np.array(dr_t)\n",
    "drh_t = np.array(drh_t)\n",
    "J_t = np.array(J_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guilty-worry",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "r_t = np.array(r_t)\n",
    "rh_t = np.array(rh_t)\n",
    "U_t = np.array(U_t)\n",
    "Uh_t = np.array(Uh_t)\n",
    "e1 = np.array(e1)\n",
    "e2 = np.array(e2)\n",
    "dr_t = np.array(dr_t)\n",
    "drh_t = np.array(drh_t)\n",
    "J_t = np.array(J_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authorized-batman",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(drh_t[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrapped-hybrid",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effective-point",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(x, n=100) :\n",
    "    ret = np.cumsum(x, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n\n",
    "\n",
    "with PdfPages(FigPath/ 'PredCoding_Rec_{}.pdf'.format(deltaJ_type)) as pdf:\n",
    "    moving_average_error = moving_average(np.array(error_list))\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    plt.ylabel(\"Error\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.yticks(np.arange(0,5))\n",
    "    plt.ylim([0,5])\n",
    "    plt.plot(np.arange(len(moving_average_error)), moving_average_error)\n",
    "    plt.tight_layout()\n",
    "    pdf.savefig()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot Receptive fields of level 1\n",
    "    fig = plt.figure(figsize=(8, 4))\n",
    "    plt.subplots_adjust(hspace=0.1, wspace=0.1)\n",
    "    for i in range(32):\n",
    "        plt.subplot(4, 8, i+1)\n",
    "        plt.imshow(np.reshape(model.U[:, i], (16, 16)), cmap=\"gray\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    fig.suptitle(\"Receptive fields of level 1\", fontsize=20)\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "    plt.tight_layout()\n",
    "    pdf.savefig()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot Receptive fields of level 2\n",
    "    zero_padding = np.zeros((80, 32))\n",
    "    U0 = np.concatenate((model.U, zero_padding, zero_padding))\n",
    "    U1 = np.concatenate((zero_padding, model.U, zero_padding))\n",
    "    U2 = np.concatenate((zero_padding, zero_padding, model.U))\n",
    "    U_ = np.concatenate((U0, U1, U2), axis = 1)\n",
    "    Uh_ = U_ @ model.Uh  \n",
    "\n",
    "    fig = plt.figure(figsize=(8, 5))\n",
    "    plt.subplots_adjust(hspace=0.1, wspace=0.1)\n",
    "    for i in range(36):\n",
    "        plt.subplot(6, 6, i+1)\n",
    "        plt.imshow(np.reshape(Uh_[:, i], (16, 26), order='F'), cmap=\"gray\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    fig.suptitle(\"Receptive fields of level 2\", fontsize=20)\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "    plt.tight_layout()\n",
    "    pdf.savefig()\n",
    "    plt.show()\n",
    "\n",
    "    fig, axs = plt.subplots(1,2, figsize=(10,5))\n",
    "    axs[0].imshow(np.cov(U_))\n",
    "    axs[0].set_title('Cov(U)')\n",
    "    axs[1].imshow(np.cov(Uh_))\n",
    "    axs[1].set_title('Cov(Uh)')\n",
    "    plt.tight_layout()\n",
    "    pdf.savefig()\n",
    "\n",
    "    fig, axs = plt.subplots(1,2, figsize=(10,5))\n",
    "    axs[0].imshow(model.J)\n",
    "    axs[0].set_title('J Matrix')\n",
    "    axs[1].imshow(np.cov(model.J))\n",
    "    axs[1].set_title('Cov(J)')\n",
    "    plt.tight_layout()\n",
    "    pdf.savefig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closed-feelings",
   "metadata": {},
   "outputs": [],
   "source": [
    "J_t[0]==J_t[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "creative-birmingham",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "##### Print memory of local variables #####\n",
    "for name, size in sorted(((name, sys.getsizeof(value)) for name, value in locals().items()), key= lambda x: -x[1])[:10]:\n",
    "    print(\"{:>30}: {:>8}\".format(name, sizeof_fmt(size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatty-phase",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(r_t)[:,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vanilla-allah",
   "metadata": {},
   "outputs": [],
   "source": [
    "# r_t = np.array(r_t)\n",
    "# rh_t = np.array(rh_t)\n",
    "U_t = np.array(U_t)\n",
    "Uh_t = np.array(Uh_t)\n",
    "e1 = np.array(e1)\n",
    "e2 = np.array(e2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grave-cliff",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_data = {\n",
    "    'r_t':r_t, \n",
    "    'rh_t':rh_t, \n",
    "    'U_t':U_t, \n",
    "    'Uh_t':Uh_t, \n",
    "    'e1':e1, \n",
    "    'e2':e2, \n",
    "    'J_t':J_t,\n",
    "}\n",
    "\n",
    "ioh5.save((data_path / 'PredCodingRec_{}.h5'.format(deltaJ_type)),pred_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consolidated-visit",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(e1[:20,0,0])\n",
    "plt.plot(e2[:20,0])\n",
    "# plt.plot(error_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manufactured-columbus",
   "metadata": {},
   "outputs": [],
   "source": [
    "dr_t.shape,drh_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "posted-qualification",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(dr_t[:100,0,0])\n",
    "plt.plot(drh_t[:100,0])\n",
    "# plt.plot(r_t[:,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adverse-schema",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(r_t[:100,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marked-asset",
   "metadata": {},
   "outputs": [],
   "source": [
    "fxh = r_t.reshape(-1,96) - (model.Uh @ rh_t.T).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compound-packing",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fxh[0:100,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solar-transmission",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.rh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "military-bones",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(np.cov(model.r.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "missing-mechanism",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.shape, model.U.shape, model.r.shape, model.Uh.shape, model.rh.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legendary-playing",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appointed-drive",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt=1\n",
    "sigma2=1 \n",
    "sigma2_td=10\n",
    "\n",
    "inv_sigma2 = 1/sigma2 # 1 / sigma^2        \n",
    "inv_sigma2_td = 1/sigma2_td # 1 / sigma_td^2\n",
    "\n",
    "k1 = 0.3 # k_1: update rate\n",
    "k2 = 0.2 # k_2: learning rate\n",
    "\n",
    "lam = 0.002 # sparsity rate\n",
    "alpha = 1\n",
    "alphah = 0.05\n",
    "\n",
    "num_units_level0 = 256\n",
    "num_units_level1 = 32\n",
    "num_units_level2 = 128\n",
    "num_level1 = 3\n",
    "\n",
    "U = np.random.randn(num_units_level0, \n",
    "                    num_units_level1)\n",
    "Uh = np.random.randn(int(num_level1*num_units_level1),\n",
    "                     num_units_level2)\n",
    "U = U.astype(np.float32) * np.sqrt(2/(num_units_level0+num_units_level1))\n",
    "Uh = Uh.astype(np.float32) * np.sqrt(2/(int(num_level1*num_units_level1)+num_units_level2)) \n",
    "\n",
    "J = np.random.randn(num_units_level2,\n",
    "                         num_units_level2).astype(np.float32) * np.sqrt(2/(int(num_units_level2*num_units_level1)+num_units_level2)) \n",
    "r = np.zeros((num_level1, num_units_level1))\n",
    "rh = np.zeros((num_units_level2))\n",
    "\n",
    "h = np.zeros((num_units_level2,\n",
    "                     num_units_level2))\n",
    "\n",
    "# def initialize_states(self, inputs):\n",
    "# r = inputs @ U \n",
    "# rh = Uh.T @ np.reshape(r, (int(num_level1*num_units_level1))) + J @ rh# np.reshape(rh, (int(num_level1*num_units_level1)))\n",
    "\n",
    "\n",
    "# def calculate_total_error(self, error, errorh):\n",
    "# recon_error = inv_sigma2*np.sum(error**2) + inv_sigma2_td*np.sum(errorh**2)\n",
    "# sparsity_r = alpha*np.sum(r**2) + alphah*np.sum(rh**2)\n",
    "# sparsity_U = lam*(np.sum(U**2) + np.sum(Uh**2))\n",
    "#     return recon_error + sparsity_r + sparsity_U\n",
    "\n",
    "# def __call__(self, inputs, training=False):\n",
    "#     # inputs : (3, 256)\n",
    "#     r_reshaped = np.reshape(r, (int(num_level1*num_units_level1))) # (96)\n",
    "\n",
    "#     fx = r @ U.T\n",
    "#     fxh = Uh @ rh # (96, )\n",
    "\n",
    "#     # Calculate errors\n",
    "#     error = inputs - fx # (3, 256)\n",
    "#     errorh = r_reshaped - fxh # (96, ) \n",
    "#     errorh_reshaped = np.reshape(errorh, (num_level1, num_units_level1)) # (3, 32)\n",
    "\n",
    "#     g_r = alpha * r / (1 + r**2) # (3, 32)\n",
    "#     g_rh = alphah * rh / (1 + rh**2) # (64, )\n",
    "\n",
    "#     temp = np.zeros((len(rh),len(rh)))\n",
    "#     np.fill_diagonal(temp,rh)\n",
    "#     dh = h + temp\n",
    "\n",
    "#     # Update r and rh\n",
    "#     dr = inv_sigma2 * error @ U - inv_sigma2_td * errorh_reshaped - g_r\n",
    "#     drh = inv_sigma2_td * Uh.T @ errorh - g_rh + inv_sigma2_td * J@rh\n",
    "\n",
    "#     dr = k1 * dr\n",
    "#     drh = k1 * drh\n",
    "\n",
    "#     # Updates                \n",
    "#     r = r + dr\n",
    "#     rh = rh + drh\n",
    "\n",
    "#     if training:  \n",
    "#         dU = inv_sigma2 * error.T @ r - 3*lam * U\n",
    "#         dUh = inv_sigma2_td * np.outer(errorh, rh) - lam * Uh\n",
    "\n",
    "#         dJ = inv_sigma2_td * np.outer(errorh, rh).T @ Uh *h\n",
    "\n",
    "#         U += k2 * dU\n",
    "#         Uh += k2 * dUh\n",
    "#         J += k2 * dJ\n",
    "#     return error, errorh, dr, drh, r, rh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranging-chemical",
   "metadata": {},
   "source": [
    "# NN implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "figured-entertainment",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RaoBallard1999Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, dt=1, sigma2=1, sigma2_td=10):\n",
    "        super(RaoBallard1999Model, self).__init__()\n",
    "        self.dt = dt\n",
    "        self.inv_sigma2 = 1/sigma2 # 1 / sigma^2        \n",
    "        self.inv_sigma2_td = 1/sigma2_td # 1 / sigma_td^2\n",
    "        \n",
    "        self.k1 = 0.3 # k_1: update rate\n",
    "        self.k2 = 0.2 # k_2: learning rate\n",
    "        \n",
    "        self.lam = 0.002 # sparsity rate\n",
    "        self.alpha = 1\n",
    "        self.alphah = 0.05\n",
    "        \n",
    "        self.num_units_level0 = 256\n",
    "        self.num_units_level1 = 32\n",
    "        self.num_units_level2 = 128\n",
    "        self.num_level1 = 3\n",
    "        \n",
    "        self.U = torch.randn(self.num_units_level0, \n",
    "                            self.num_units_level1, requires_grad=True)\n",
    "        self.Uh = torch.randn(int(self.num_level1*self.num_units_level1),\n",
    "                             self.num_units_level2, requires_grad=True)\n",
    "#         self.U = torch.Tensor(U*np.sqrt(2/(self.num_units_level0+self.num_units_level1))).requires_grad_()\n",
    "#         self.Uh = torch.Tensor(Uh*np.sqrt(2/(int(self.num_level1*self.num_units_level1)+self.num_units_level2))).requires_grad_()\n",
    "        self.r = torch.zeros((self.num_level1, self.num_units_level1), requires_grad=True)\n",
    "        self.rh = torch.zeros((self.num_units_level2), requires_grad=True)\n",
    "        self.o_r = torch.optim.Adam([self.r], lr =self.k1)\n",
    "        self.o_rh = torch.optim.Adam([self.rh], lr =self.k1)\n",
    "        self.o_U = torch.optim.Adam([self.U], lr =self.k2)\n",
    "        self.o_Uh = torch.optim.Adam([self.Uh], lr =self.k2)\n",
    "    def initialize_states(self, inputs):\n",
    "        print(self.r.is_leaf)\n",
    "        self.r = (inputs @ self.U).requires_grad_()\n",
    "        self.rh = (self.Uh.T @ torch.reshape(self.r, (int(self.num_level1*self.num_units_level1),))).requires_grad_()\n",
    "        print(self.r.is_leaf)\n",
    "\n",
    "    def calculate_total_error(self, error, errorh):\n",
    "        recon_error = self.inv_sigma2*torch.sum(error**2) + self.inv_sigma2_td*torch.sum(errorh**2)\n",
    "        sparsity_r = self.alpha*torch.sum(self.r**2) + self.alphah*torch.sum(self.rh**2)\n",
    "        sparsity_U = self.lam*(torch.sum(self.U**2) + torch.sum(self.Uh**2))\n",
    "        return recon_error + sparsity_r + sparsity_U\n",
    "        \n",
    "    def forward(self, inputs, training=False):\n",
    "        # inputs : (3, 256)\n",
    "        r_reshaped = torch.reshape(self.r, (int(self.num_level1*self.num_units_level1),)) # (96)\n",
    "\n",
    "        fx = self.r @ self.U.T\n",
    "        fxh = self.Uh @ self.rh # (96, )\n",
    "        \n",
    "        # Calculate errors\n",
    "        error = inputs - fx # (3, 256)\n",
    "        errorh = r_reshaped - fxh # (96, ) \n",
    "        errorh_reshaped = torch.reshape(errorh, (self.num_level1, self.num_units_level1)) # (3, 32)\n",
    "        self.o_r.zero_grad()\n",
    "        self.o_rh.zero_grad()\n",
    "        if training:\n",
    "            self.o_U.zero_grad()\n",
    "            self.o_Uh.zero_grad()\n",
    "        loss = self.calculate_total_error(error,errorh)\n",
    "        loss.backward(retain_graph=True)\n",
    "        self.o_r.step()\n",
    "        self.o_rh.step()\n",
    "        if training:\n",
    "            self.o_U.step()\n",
    "            self.o_Uh.step()\n",
    "\n",
    "        return error, errorh, self.r.grad, self.rh.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structural-madison",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RaoBallard1999Model()\n",
    "inputs = torch.Tensor(data_in)#.to(device)\n",
    "model.initialize_states(inputs.cpu())\n",
    "# print(model.r)\n",
    "# error, errorh, dr, drh = model(inputs.cpu())\n",
    "# print(model.r)\n",
    "\n",
    "# model(inputs,training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expensive-attention",
   "metadata": {},
   "outputs": [],
   "source": [
    "for iter_ in tqdm(range(num_iter)):\n",
    "    inputs = np.random.multivariate_normal(mean=np.zeros(I_mat.shape[0]),cov=I_mat,size=(num_samps,))\n",
    "    inputs = (inputs - np.mean(inputs)) * input_scale\n",
    "    inputs = torch.Tensor(inputs)\n",
    "    # Reset states\n",
    "    model.initialize_states(inputs)\n",
    "    \n",
    "    # Input an image patch until latent variables are converged \n",
    "    for i in range(nt_max):\n",
    "        # Update r and rh without update weights \n",
    "        error, errorh = model(inputs, training=False)\n",
    "        \n",
    "        # Compute norm of r and rh\n",
    "        dr_norm = np.linalg.norm(dr, ord=2) \n",
    "        drh_norm = np.linalg.norm(drh, ord=2)\n",
    "        \n",
    "        # Check convergence of r and rh, then update weights\n",
    "        if dr_norm < eps and drh_norm < eps:\n",
    "            error, errorh, dr, drh = model(inputs, training=True)\n",
    "            break\n",
    "        \n",
    "        # If failure to convergence, break and print error\n",
    "        if i >= nt_max-2: \n",
    "            print(\"Error at patch:\", iter_)\n",
    "            print(dr_norm, drh_norm)\n",
    "            break\n",
    "   \n",
    "    error_list.append(model.calculate_total_error(error, errorh)) # Append errors\n",
    "\n",
    "    # Decay learning rate         \n",
    "    if iter_ % 40 == 39:\n",
    "        model.k2 /= 1.015\n",
    "    \n",
    "    # Print moving average error\n",
    "    if iter_ % 1000 == 999:  \n",
    "        print(\"iter: \"+str(iter_+1)+\"/\"+str(num_iter)+\", Moving error:\", np.mean(error_list[iter_-999:iter_]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developed-actor",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_units_level0 = 256\n",
    "num_units_level1 = 32\n",
    "num_units_level2 = 128\n",
    "num_level1 = 3\n",
    "feedforward = nn.ModuleDict()\n",
    "feedback = nn.ModuleDict()\n",
    "# for layer in range(len(hidden_dims)):\n",
    "#     if layer == len(hidden_dims) - 1:\n",
    "#         out_dim = hidden_dims[layer]\n",
    "#     else: \n",
    "#         out_dim = hidden_dims[layer+1]\n",
    "layer=0\n",
    "feedforward.add_module('U{:d}'.format(layer),nn.Linear(num_units_level0, num_units_level1, bias=False))\n",
    "feedforward.add_module('NonLinearity{:d}'.format(layer), nn.ReLU())\n",
    "# feedforward.add_module('U{:d}_T'.format(layer),nn.Linear(num_units_level1, num_units_level0, bias=False))\n",
    "#     in_dim = out_dim\n",
    "#     if layer < len(hidden_dims)-1:\n",
    "feedback.add_module('Uh{:d}'.format(layer),nn.Linear(num_level1*num_units_level1,num_units_level2, bias=False))\n",
    "feedback.add_module('NonLinearity{:d}'.format(layer), nn.ReLU())\n",
    "# with torch.no_grad():\n",
    "#     feedback['Uh0'].weight = nn.Parameter(feedforward['U0'].weight.T)\n",
    "optimizer = torch.optim.Adam(list(feedforward.parameters()) + list(feedback.parameters()), lr =.001)\n",
    "feedforward.to(device)\n",
    "feedback.to(device)\n",
    "print(feedforward)\n",
    "print(feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reflected-tractor",
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback['Uh0'].weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eight-seeking",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.Tensor(data_in).to(device)\n",
    "r = feedforward['U0'](inputs)\n",
    "rh = feedback['Uh0'](r.view(-1))\n",
    "\n",
    "\n",
    "fx = torch.matmul(r,feedforward['U0'].weight.T)\n",
    "fxh = feedback['Uh0'](rh)\n",
    "\n",
    "error = inputs - fx\n",
    "errorh = r.view(-1) - fxh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fluid-theater",
   "metadata": {},
   "outputs": [],
   "source": [
    "feedforward['U0'].weight.T.shape, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upset-plymouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "fxh.shape,r.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entitled-maryland",
   "metadata": {},
   "outputs": [],
   "source": [
    "Nepochs = 100\n",
    "# log = pd.DataFrame([],columns=['Loss'])\n",
    "log = {'loss': []}\n",
    "for epoch in tqdm(range(Nepochs)):\n",
    "    result = torch.Tensor(data_in).to(device)\n",
    "    for layer in range(n_layers-1):\n",
    "        fback = feedback['Layer{:d}'.format(layer)](result)\n",
    "        result = torch.abs(result - fback)\n",
    "    loss = result.mean()\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    log['loss'].append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swiss-mason",
   "metadata": {},
   "outputs": [],
   "source": [
    "log['activations'] = np.zeros((n_layers+1,) + tuple(result.shape))\n",
    "with torch.no_grad():\n",
    "    result = torch.Tensor(data_in).to(device)\n",
    "    for layer in range(n_layers):\n",
    "        l_activations = encoder['Layer{:d}'.format(layer)](result)\n",
    "        result = torch.abs(result - l_activations)\n",
    "        log['activations'][layer] = l_activations.cpu()\n",
    "    log['activations'][layer+1] = result.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acute-turner",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,3,figsize=(15,5))\n",
    "for n in range(log['activations'].shape[0]):\n",
    "    axs[n].pcolormesh(np.cov(log['activations'][n].T))\n",
    "    axs[n].set_title('Layer {:d} Cov Matrix'.format(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complimentary-tonight",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(log['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "united-demand",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path('~/Research/SensoryMotorPred_Data').expanduser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparable-chambers",
   "metadata": {},
   "outputs": [],
   "source": [
    "h5file = (base_dir / 'PredCoding_data_logs.h5').as_posix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offensive-haiti",
   "metadata": {},
   "outputs": [],
   "source": [
    "ioh5.save(h5file,log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plain-armstrong",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = ioh5.load(h5file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honest-animation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

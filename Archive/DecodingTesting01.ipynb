{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dedicated-juice",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gorgeous-brand",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import glob\n",
    "import sys \n",
    "import yaml \n",
    "import glob\n",
    "import h5py \n",
    "import ray\n",
    "import logging \n",
    "import json\n",
    "import gc\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "# import io_dict_to_hdf5 as ioh5\n",
    "import xarray as xr\n",
    "\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from scipy import interpolate \n",
    "from scipy import signal\n",
    "from pathlib import Path\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.ndimage import shift as imshift\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, Subset\n",
    "torch.backends.cudnn.benchmark = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "sys.path.append(str(Path('.').absolute().parent))\n",
    "from utils import *\n",
    "import io_dict_to_hdf5 as ioh5\n",
    "from format_data import load_ephys_data_aligned\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "FigPath = check_path(Path('~/Research/SensoryMotorPred_Data').expanduser(),'Figures/Decoding')\n",
    "\n",
    "ray.init(\n",
    "    ignore_reinit_error=True,\n",
    "    logging_level=logging.ERROR,\n",
    ")\n",
    "print(f'Dashboard URL: http://{ray.get_dashboard_url()}')\n",
    "print('Dashboard URL: http://localhost:{}'.format(ray.get_dashboard_url().split(':')[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lonely-james",
   "metadata": {},
   "source": [
    "# Gather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ongoing-savannah",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = Path('~/Research/SensoryMotorPred_Data/data/070921/J553RT/fm1').expanduser()\n",
    "with open(save_dir / 'file_dict.json','r') as fp:\n",
    "    file_dict = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clear-sweet",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dict = {'cell': 0,\n",
    " 'drop_slow_frames': True,\n",
    " 'ephys': '/home/seuss/Goeppert/freely_moving_ephys/ephys_recordings/070921/J553RT/fm1/070921_J553RT_control_Rig2_fm1_ephys_merge.json',\n",
    " 'ephys_bin': '/home/seuss/Goeppert/freely_moving_ephys/ephys_recordings/070921/J553RT/fm1/070921_J553RT_control_Rig2_fm1_Ephys.bin',\n",
    " 'eye': '/home/seuss/Goeppert/freely_moving_ephys/ephys_recordings/070921/J553RT/fm1/070921_J553RT_control_Rig2_fm1_REYE.nc',\n",
    " 'imu': '/home/seuss/Goeppert/freely_moving_ephys/ephys_recordings/070921/J553RT/fm1/070921_J553RT_control_Rig2_fm1_imu.nc',\n",
    " 'mapping_json': '/home/seuss/Research/Github/FreelyMovingEphys/probes/channel_maps.json',\n",
    " 'mp4': True,\n",
    " 'name': '070921_J553RT_control_Rig2_fm1',\n",
    " 'probe_name': 'DB_P128-6',\n",
    " 'save': '/home/seuss/Goeppert/freely_moving_ephys/ephys_recordings/070921/J553RT/fm1',\n",
    " 'speed': None,\n",
    " 'stim_type': 'light',\n",
    " 'top': '/home/seuss/Goeppert/freely_moving_ephys/ephys_recordings/070921/J553RT/fm1/070921_J553RT_control_Rig2_fm1_TOP1.nc',\n",
    " 'world': '/home/seuss/Goeppert/freely_moving_ephys/ephys_recordings/070921/J553RT/fm1/070921_J553RT_control_Rig2_fm1_world.nc'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strategic-andrew",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dt = .1\n",
    "data = load_ephys_data_aligned(file_dict, save_dir, model_dt=model_dt)\n",
    "nan_idxs = []\n",
    "for key in data.keys():\n",
    "    nan_idxs.append(np.where(np.isnan(data[key]))[0])\n",
    "good_idxs = np.ones(len(data['model_active']),dtype=bool)\n",
    "good_idxs[data['model_active']<.5] = False\n",
    "good_idxs[np.unique(np.hstack(nan_idxs))] = False\n",
    "\n",
    "raw_nsp = data['model_nsp'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caring-hypothesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in data.keys():\n",
    "    if (key != 'model_nsp') & (key != 'model_active'):\n",
    "#         movement_times = (data['model_active']>.5) & (~np.isnan(data[key]))\n",
    "        data[key] = data[key][good_idxs] # interp_nans(data[key]).astype(float)\n",
    "    elif (key == 'model_nsp'):\n",
    "        data[key] = data[key][good_idxs]\n",
    "locals().update(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removable-freight",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data['model_active'], bins=100)\n",
    "plt.axvline(x=.5)\n",
    "# movement_times = (data['model_active']>.5) & (~np.isnan(data['model_th']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facial-romance",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Print memory of local variables #####\n",
    "for name, size in sorted(((name, sys.getsizeof(value)) for name, value in locals().items()), key= lambda x: -x[1])[:10]:\n",
    "    print(\"{:>30}: {:>8}\".format(name, sizeof_fmt(size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contrary-jaguar",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dth = np.diff(model_th,append=0)\n",
    "model_dphi = np.diff(model_phi,append=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joint-spyware",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Group shuffle #####\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "gss = GroupShuffleSplit(n_splits=1, train_size=.7, random_state=42)\n",
    "nT = model_nsp.shape[0]\n",
    "frac = .2\n",
    "groups = np.hstack([i*np.ones(int((frac*i)*nT) - int((.2*(i-1))*nT)) for i in range(1,int(1/frac)+1)])\n",
    "for train_idx, test_idx in gss.split(np.arange(len(model_nsp)), groups=groups):\n",
    "    print(\"TRAIN:\", train_idx, \"TEST:\", test_idx)\n",
    "    \n",
    "model_vid_sm = (model_vid_sm - np.mean(model_vid_sm,axis=0))/np.std(model_vid_sm,axis=0) \n",
    "model_th = (model_th - np.mean(model_th,axis=0))/np.std(model_th,axis=0) \n",
    "model_phi = (model_phi - np.mean(model_phi,axis=0))/np.std(model_phi,axis=0) \n",
    "model_roll = (model_roll - np.mean(model_roll,axis=0))/np.std(model_roll,axis=0) \n",
    "model_pitch = (model_pitch - np.mean(model_pitch,axis=0))/np.std(model_pitch,axis=0) \n",
    "\n",
    "\n",
    "train_vid = model_vid_sm[train_idx]\n",
    "test_vid = model_vid_sm[test_idx]\n",
    "train_nsp = model_nsp[train_idx]\n",
    "test_nsp = model_nsp[test_idx]\n",
    "train_th = model_th[train_idx]\n",
    "test_th = model_th[test_idx]\n",
    "train_phi = model_phi[train_idx]\n",
    "test_phi = model_phi[test_idx]\n",
    "train_roll = model_roll[train_idx]\n",
    "test_roll = model_roll[test_idx]\n",
    "train_pitch = model_pitch[train_idx]\n",
    "test_pitch = model_pitch[test_idx]\n",
    "train_t = model_t[train_idx]\n",
    "test_t = model_t[test_idx]\n",
    "train_dth = model_dth[train_idx]\n",
    "test_dth = model_dth[test_idx]\n",
    "train_dphi = model_dphi[train_idx]\n",
    "test_dphi = model_dphi[test_idx]\n",
    "train_gz = model_gz[train_idx]\n",
    "test_gz = model_gz[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solved-drain",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_vid[:1000,5,10])\n",
    "plt.plot(train_vid[:1000,11,10])\n",
    "plt.plot(train_vid[:1000,10,11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polyphonic-integrity",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(train_phi,train_pitch, alpha=.1)\n",
    "# plt.scatter(train_dphi,train_roll, alpha=.1)\n",
    "# plt.scatter(train_dth,train_roll, alpha=.1)\n",
    "# plt.scatter(train_dth,train_pitch, alpha=.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "domestic-england",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Tuning curve for theta\n",
    "def tuning_curve(model_nsp, var, model_dt = .025, N_bins=10):\n",
    "    var_range = np.linspace(np.nanmean(var)-2*np.nanstd(var), np.nanmean(var)+2*np.nanstd(var),N_bins)\n",
    "    tuning = np.zeros((model_nsp.shape[-1],len(var_range)-1))\n",
    "    tuning_std = np.zeros((model_nsp.shape[-1],len(var_range)-1))\n",
    "    for n in range(model_nsp.shape[-1]):\n",
    "        for j in range(len(var_range)-1):\n",
    "            usePts = (var>=var_range[j]) & (var<var_range[j+1])\n",
    "            tuning[n,j] = np.nanmean(model_nsp[usePts,n])/model_dt\n",
    "            tuning_std[n,j] = (np.nanstd(model_nsp[usePts,n])/model_dt)/ np.sqrt(np.count_nonzero(usePts))\n",
    "    return tuning, tuning_std, var_range[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "touched-viking",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning, tuning_std, var_range = tuning_curve(train_nsp, train_th, N_bins=10, model_dt=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heard-substitute",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 22\n",
    "fig, axs = plt.subplots(1,figsize=(7,5))\n",
    "axs.errorbar(var_range,tuning[n], yerr=tuning_std[n])\n",
    "axs.set_ylim(bottom=0)\n",
    "axs.set_xlabel('Eye Theta')\n",
    "axs.set_ylabel('Spikes/s')\n",
    "axs.set_title('Neuron: {}'.format(n))\n",
    "plt.tight_layout()\n",
    "fig.savefig(FigPath/'ExampleTuningCurve.png',bbox_inches='tight',transparent=False, facecolor='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "burning-senegal",
   "metadata": {},
   "source": [
    "## GLM Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legitimate-farming",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.linalg as linalg\n",
    "import scipy.sparse as sparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extensive-racing",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vid = model_vid_sm\n",
    "model_dt = .1\n",
    "nks = np.shape(model_vid)[1:]; nk = nks[0]*nks[1]\n",
    "nT = np.shape(model_nsp)[0]\n",
    "x = model_vid.reshape(model_nsp.shape[0], -1).copy()\n",
    "# image dimensions\n",
    "n_units = np.shape(model_nsp)[1]\n",
    "# subtract mean and renormalize -- necessary? \n",
    "mn_img = np.mean(x,axis=0)\n",
    "x = x-mn_img\n",
    "x = x/np.std(x,axis =0)\n",
    "x = np.append(x,np.ones((nT,1)), axis = 1) # append column of ones\n",
    "\n",
    "# set up prior matrix (regularizer)\n",
    "# L2 prior\n",
    "Imat = np.eye(nk)\n",
    "Imat = linalg.block_diag(Imat,np.zeros((1,1)))\n",
    "# smoothness prior\n",
    "consecutive = np.ones((nk, 1))\n",
    "consecutive[nks[1]-1::nks[1]] = 0\n",
    "diff = np.zeros((1,2))\n",
    "diff[0,0] = -1\n",
    "diff[0,1]= 1\n",
    "Dxx = sparse.diags((consecutive @ diff).T, np.array([0, 1]), (nk-1,nk))\n",
    "Dxy = sparse.diags((np.ones((nk,1))@ diff).T, np.array([0, nks[1]]), (nk-nks[1], nk))\n",
    "Dx = Dxx.T @ Dxx + Dxy.T @ Dxy\n",
    "D  = linalg.block_diag(Dx.toarray(),np.zeros((1,1)))      \n",
    "# summed prior matrix\n",
    "# Cinv = D + Imat\n",
    "Cinv = Imat\n",
    "lag_list = [ -4, -2, 0 , 2, 4]\n",
    "lambdas = 1024 * (2**np.arange(0,16))\n",
    "nlam = len(lambdas)\n",
    "# set up empty arrays for receptive field and cross correlation\n",
    "sta_all = np.zeros((n_units, len(lag_list), nks[0], nks[1]))\n",
    "cc_all = np.zeros((n_units,len(lag_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "korean-korea",
   "metadata": {},
   "source": [
    "## PCA on Vid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manual-sample",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pcs = pca.fit_transform(model_vid_sm.reshape(-1,model_vid.shape[1]*model_vid.shape[2]))\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "comp_to_keep = np.where(np.cumsum(pca.explained_variance_ratio_)>.9)[0][0]\n",
    "plt.axvline(x=comp_to_keep)\n",
    "pca = PCA(n_components=comp_to_keep)\n",
    "pcs = pca.fit_transform(model_vid_sm.reshape(-1,model_vid.shape[1]*model_vid.shape[2]))\n",
    "print('keep {} PCs'.format(comp_to_keep))\n",
    "# recon = pca.inverse_transform(pcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collaborative-blend",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vid_sm.shape,model_th.shape,model_phi.shape,model_roll.shape,model_pitch.shape,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medical-quick",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vid_sm = (model_vid_sm - np.mean(model_vid_sm,axis=0))/np.std(model_vid_sm,axis=0) \n",
    "model_th = (model_th - np.mean(model_th,axis=0))/np.std(model_th,axis=0) \n",
    "model_phi = (model_phi - np.mean(model_phi,axis=0))/np.std(model_phi,axis=0) \n",
    "model_roll = (model_roll - np.mean(model_roll,axis=0))/np.std(model_roll,axis=0) \n",
    "model_pitch = (model_pitch - np.mean(model_pitch,axis=0))/np.std(model_pitch,axis=0) \n",
    "\n",
    "\n",
    "\n",
    "model_dth = np.diff(model_th,append=0)\n",
    "model_dphi = np.diff(model_phi,append=0)\n",
    "train_vid, test_vid, train_nsp, test_nsp, train_th, test_th, train_phi, test_phi, train_roll, test_roll, train_pitch, test_pitch, train_t, test_t, train_dth, test_dth, train_dphi, test_dphi, train_pcs,test_pcs = \\\n",
    "train_test_split(model_vid_sm, model_nsp, model_th, model_phi, model_roll, model_pitch, model_t, model_dth, model_dphi, pcs, train_size=.7, shuffle=False, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charitable-province",
   "metadata": {},
   "source": [
    "# Ridge/Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chicken-keeping",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model as lm # MultiTaskLassoCV, RidgeCV, MultiTaskElasticNetCV, LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import itertools\n",
    "model_type = 'ridgecv'\n",
    "if model_type == 'elasticnetcv':\n",
    "    model = make_pipeline(StandardScaler(), lm.ElasticNetCV()) # lm.RidgeCV(alphas=np.arange(100,10000,1000))) #  #MultiOutputRegressor(lm.Ridge(),n_jobs=-1)) \n",
    "elif model_type == 'ridgecv':\n",
    "    model = make_pipeline(StandardScaler(), lm.RidgeCV(alphas=lambdas))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hawaiian-pension",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def run_model(train_nsp, test_nsp, train_data, test_data, move_train, move_test, celln, lag, bin_length=80, model_dt=.1):\n",
    "    sps_train = np.roll(train_nsp[:,celln],-lag)\n",
    "    sps_test = np.roll(test_nsp[:,celln],-lag)\n",
    "\n",
    "    #calculate a few terms\n",
    "    x_train = train_data.reshape(train_data.shape[0],-1) #train_pcs\n",
    "    x_train = np.append(x_train, np.ones((x_train.shape[0],1)), axis = 1) # append column of ones\n",
    "    x_train = np.concatenate((x_train, move_train2),axis=1)\n",
    "\n",
    "    x_test = test_data.reshape(test_data.shape[0],-1) #test_pcs\n",
    "    x_test = np.append(x_test,np.ones((x_test.shape[0],1)), axis = 1) # append column of ones\n",
    "    x_test = np.concatenate((x_test, move_test2),axis=1)\n",
    "\n",
    "    model.fit(x_train,sps_train)\n",
    "\n",
    "    sp_pred = model.predict(x_test)\n",
    "    sp_smooth = (np.convolve(sps_test, np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "    pred_smooth = (np.convolve(sp_pred, np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "    cc = np.corrcoef(sp_smooth, pred_smooth)[0,1]\n",
    "    sta = pca.inverse_transform(model[model_type].coef_[:-move_train.shape[-1]]).reshape(20,30)\n",
    "    return sp_smooth, pred_smooth, cc, sta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consolidated-fight",
   "metadata": {},
   "outputs": [],
   "source": [
    "model[model_type].coef_[:-move_train2.shape[-1]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "urban-incidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "celln =22\n",
    "bin_length = 80\n",
    "\n",
    "titles = np.array(['th','phi','roll','pitch'])\n",
    "cc_all = np.zeros((15,len(lag_list)))\n",
    "sta_all = np.zeros((15,len(lag_list),20,30))\n",
    "move_train = np.hstack((train_th[:,np.newaxis],train_phi[:,np.newaxis],train_roll[:,np.newaxis],train_pitch[:,np.newaxis]))#,train_dth[:,np.newaxis], train_dphi[:,np.newaxis]))\n",
    "move_test = np.hstack((test_th[:,np.newaxis],test_phi[:,np.newaxis],test_roll[:,np.newaxis],test_pitch[:,np.newaxis]))#,test_dth[:,np.newaxis], test_dphi[:,np.newaxis]))\n",
    "sp_smooth_all = np.zeros((15,len(lag_list),move_test.shape[0]))\n",
    "pred_smooth_all = np.zeros((15,len(lag_list),move_test.shape[0]))\n",
    "titles_all = []\n",
    "\n",
    "model_ind = 0\n",
    "for n in range(1,5):\n",
    "    perms = np.array(list(itertools.combinations([0,1,2,3], n)))\n",
    "    for ind in range(perms.shape[0]):\n",
    "        move_train2 = move_train[:,perms[ind]]\n",
    "        move_test2 = move_test[:,perms[ind]]\n",
    "        for lag_ind, lag in enumerate(lag_list):\n",
    "            \n",
    "            sps_train = np.roll(train_nsp[:,celln],-lag)\n",
    "            sps_test = np.roll(test_nsp[:,celln],-lag)\n",
    "\n",
    "            #calculate a few terms\n",
    "            x_train = train_vid.reshape(train_vid.shape[0],-1) #train_pcs\n",
    "#             x_train = np.append(x_train, np.ones((x_train.shape[0],1)), axis = 1) # append column of ones\n",
    "            x_train = np.concatenate((x_train, move_train2),axis=1)\n",
    "\n",
    "            x_test = test_vid.reshape(test_vid.shape[0],-1) #test_pcs\n",
    "#             x_test = np.append(x_test,np.ones((x_test.shape[0],1)), axis = 1) # append column of ones\n",
    "            x_test = np.concatenate((x_test, move_test2),axis=1)\n",
    "\n",
    "            model.fit(x_train,sps_train)\n",
    "\n",
    "            sp_pred = model.predict(x_test)\n",
    "            sp_smooth = (np.convolve(sps_test, np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "            pred_smooth = (np.convolve(sp_pred, np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "            sp_smooth_all[model_ind,lag_ind] = sp_smooth\n",
    "            pred_smooth_all[model_ind,lag_ind] = pred_smooth\n",
    "            cc_all[model_ind,lag_ind] = np.corrcoef(sp_smooth, pred_smooth)[0,1]\n",
    "            sta_all[model_ind,lag_ind] = model[model_type].coef_[:-move_train2.shape[-1]].reshape(20,30)# pca.inverse_transform(model[model_type].coef_[:-move_train2.shape[-1]]).reshape(20,30)\n",
    "        titles_all.append('_'.join([t for t in titles[perms[ind]]]))\n",
    "        model_ind+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subtle-pixel",
   "metadata": {},
   "outputs": [],
   "source": [
    "with PdfPages(FigPath/ 'ModelSelection_{}_rawvid.pdf'.format(model_type)) as pdf:\n",
    "    for model_ind,title in enumerate(titles_all):\n",
    "        fig, axs = plt.subplots(2,len(lag_list), figsize=(np.floor(7.5*len(lag_list)).astype(int),10))\n",
    "        for lag_ind in range(axs.shape[-1]):\n",
    "            axs[0,lag_ind].plot(sp_smooth_all[model_ind,lag_ind],'k',label='smoothed FR')\n",
    "            axs[0,lag_ind].plot(pred_smooth_all[model_ind,lag_ind],'r', label='pred FR')\n",
    "            axs[0,lag_ind].set_title('cc={:.2f}'.format(cc_all[model_ind,lag_ind]))\n",
    "            axs[1,lag_ind].imshow(sta_all[model_ind,lag_ind])\n",
    "            axs[1,lag_ind].set_title('lag={:d}'.format(lag_list[lag_ind]))\n",
    "            axs[1,lag_ind].axis('off')\n",
    "            plt.suptitle(title)\n",
    "            plt.tight_layout()\n",
    "        pdf.savefig()\n",
    "        plt.close()\n",
    "print('Done Plotting!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varying-hindu",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suitable-creature",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unravel_index(np.argmax(cc_all),shape=cc_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "employed-klein",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_all.shape, sta_all.shape, pred_smooth_all.shape, sp_smooth_all.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technological-watch",
   "metadata": {},
   "source": [
    "# GLM with eye/head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designing-printer",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Group shuffle #####\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "gss = GroupShuffleSplit(n_splits=1, train_size=.7, random_state=42)\n",
    "nT = model_nsp.shape[0]\n",
    "frac = .2\n",
    "groups = np.hstack([i*np.ones(int((frac*i)*nT) - int((.2*(i-1))*nT)) for i in range(1,int(1/frac)+1)])\n",
    "for train_idx, test_idx in gss.split(np.arange(len(model_nsp)), groups=groups):\n",
    "    print(\"TRAIN:\", train_idx, \"TEST:\", test_idx)\n",
    "    \n",
    "train_vid = model_vid_sm[train_idx]\n",
    "test_vid = model_vid_sm[test_idx]\n",
    "train_nsp = model_nsp[train_idx]\n",
    "test_nsp = model_nsp[test_idx]\n",
    "train_th = model_th[train_idx]\n",
    "test_th = model_th[test_idx]\n",
    "train_phi = model_phi[train_idx]\n",
    "test_phi = model_phi[test_idx]\n",
    "train_roll = model_roll[train_idx]\n",
    "test_roll = model_roll[test_idx]\n",
    "train_pitch = model_pitch[train_idx]\n",
    "test_pitch = model_pitch[test_idx]\n",
    "train_t = model_t[train_idx]\n",
    "test_t = model_t[test_idx]\n",
    "train_dth = model_dth[train_idx]\n",
    "test_dth = model_dth[test_idx]\n",
    "train_dphi = model_dphi[train_idx]\n",
    "test_dphi = model_dphi[test_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considered-local",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vid_sm = (model_vid_sm - np.mean(model_vid_sm,axis=0))/np.std(model_vid_sm,axis=0) \n",
    "model_th = (model_th - np.mean(model_th,axis=0))/np.std(model_th,axis=0) \n",
    "model_phi = (model_phi - np.mean(model_phi,axis=0))/np.std(model_phi,axis=0) \n",
    "model_roll = (model_roll - np.mean(model_roll,axis=0))/np.std(model_roll,axis=0) \n",
    "model_pitch = (model_pitch - np.mean(model_pitch,axis=0))/np.std(model_pitch,axis=0) \n",
    "\n",
    "\n",
    "\n",
    "model_dth = np.diff(model_th,append=0)\n",
    "model_dphi = np.diff(model_phi,append=0)\n",
    "train_vid, test_vid, train_nsp, test_nsp, train_th, test_th, train_phi, test_phi, train_roll, test_roll, train_pitch, test_pitch, train_t, test_t, train_dth, test_dth, train_dphi, test_dphi, train_pcs,test_pcs = \\\n",
    "train_test_split(model_vid_sm, model_nsp, model_th, model_phi, model_roll, model_pitch, model_t, model_dth, model_dphi, pcs, test_size=.3, shuffle=False, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacterial-advertiser",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Train_Test Split with sklearn #####\n",
    "model_vid = model_vid_sm\n",
    "model_dt = .1\n",
    "nks = np.shape(model_vid)[1:]; nk = nks[0]*nks[1]\n",
    "nT = np.shape(model_nsp)[0]\n",
    "x = model_vid.reshape(model_nsp.shape[0], -1).copy()\n",
    "# image dimensions\n",
    "n_units = np.shape(model_nsp)[1]\n",
    "\n",
    "titles = np.array(['th','phi','roll','pitch'])\n",
    "move_train = np.hstack((train_th[:,np.newaxis],train_phi[:,np.newaxis],train_roll[:,np.newaxis],train_pitch[:,np.newaxis],train_dth[:,np.newaxis],train_dphi[:,np.newaxis]))\n",
    "move_test = np.hstack((test_th[:,np.newaxis],test_phi[:,np.newaxis],test_roll[:,np.newaxis],test_pitch[:,np.newaxis],test_dth[:,np.newaxis],test_dphi[:,np.newaxis]))\n",
    "\n",
    "perms = np.array([2,3]) #np.array(list(itertools.combinations([0,1,2,3], n)))\n",
    "move_train = move_train[:,perms]\n",
    "move_test = move_test[:,perms]\n",
    "\n",
    "# set up prior matrix (regularizer)\n",
    "# L2 prior\n",
    "Imat = np.eye(nk)\n",
    "Imat = linalg.block_diag(Imat,np.zeros((1+move_test.shape[-1],1+move_test.shape[-1])))\n",
    "# smoothness prior\n",
    "consecutive = np.ones((nk, 1))\n",
    "consecutive[nks[1]-1::nks[1]] = 0\n",
    "diff = np.zeros((1,2))\n",
    "diff[0,0] = -1\n",
    "diff[0,1]= 1\n",
    "Dxx = sparse.diags((consecutive @ diff).T, np.array([0, 1]), (nk-1,nk))\n",
    "Dxy = sparse.diags((np.ones((nk,1))@ diff).T, np.array([0, nks[1]]), (nk-nks[1], nk))\n",
    "Dx = Dxx.T @ Dxx + Dxy.T @ Dxy\n",
    "D  = linalg.block_diag(Dx.toarray(),np.zeros((1+move_test.shape[-1],1+move_test.shape[-1])))   \n",
    "# summed prior matrix\n",
    "# Cinv = D + Imat\n",
    "Cinv = Imat\n",
    "\n",
    "lag_list = [ -4, -2, 0 , 2, 4]\n",
    "lambdas = 1024 * (2**np.arange(0,16))\n",
    "nlam = len(lambdas)\n",
    "# set up empty arrays for receptive field and cross correlation\n",
    "sta_all = np.zeros((n_units, len(lag_list), nks[0], nks[1]))\n",
    "cc_all = np.zeros((n_units,len(lag_list)))\n",
    "\n",
    "celln = 51\n",
    "fig, axs = plt.subplots(2,len(lag_list), figsize=(np.floor(7.5*len(lag_list)).astype(int),10))\n",
    "for lag_ind, lag in enumerate(lag_list):\n",
    "    \n",
    "    sps_train = np.roll(train_nsp[:,celln],-lag)\n",
    "    sps_test = np.roll(test_nsp[:,celln],-lag)\n",
    "    \n",
    "   \n",
    "    #calculate a few terms\n",
    "    x_train = train_vid.reshape(train_vid.shape[0],-1)\n",
    "    x_train = np.append(x_train, np.ones((x_train.shape[0],1)), axis = 1) # append column of ones\n",
    "    x_train = np.concatenate((x_train,move_train),axis=1)\n",
    "\n",
    "    x_test = test_vid.reshape(test_vid.shape[0],-1)\n",
    "    x_test = np.append(x_test,np.ones((x_test.shape[0],1)), axis = 1) # append column of ones\n",
    "    x_test = np.concatenate((x_test,move_test),axis=1)\n",
    "    \n",
    "    XXtr = x_train.T @ x_train\n",
    "    XYtr = x_train.T @ sps_train\n",
    "    \n",
    "    msetrain = np.zeros((nlam,1))\n",
    "    msetest = np.zeros((nlam,1))\n",
    "    w_ridge = np.zeros((nk+1+move_test.shape[1],nlam))\n",
    "    # initial guess\n",
    "    # loop over regularization strength\n",
    "    for l in range(len(lambdas)):  \n",
    "        # calculate MAP estimate               \n",
    "        w = np.linalg.solve(XXtr + lambdas[l]*Cinv, XYtr) # equivalent of \\ (left divide) in matlab\n",
    "        w_ridge[:,l] = w\n",
    "        # calculate test and training rms error\n",
    "        msetrain[l] = np.mean((sps_train - x_train@w)**2)\n",
    "        msetest[l] = np.mean((sps_test - x_test@w)**2)\n",
    "    # select best cross-validated lambda for RF\n",
    "    best_lambda = np.argmin(msetest)\n",
    "    w = w_ridge[:,best_lambda]\n",
    "    ridge_rf = w_ridge[:,best_lambda]\n",
    "    sta_all[celln,lag_ind,:,:] = np.reshape(w[:-(1+move_test.shape[-1])],nks)\n",
    "    # plot predicted vs actual firing rate\n",
    "    # predicted firing rate\n",
    "    sp_pred = x_test@ridge_rf\n",
    "    # bin the firing rate to get smooth rate vs time\n",
    "    bin_length = 80\n",
    "    sp_smooth = (np.convolve(sps_test, np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "    pred_smooth = (np.convolve(sp_pred, np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "    # a few diagnostics\n",
    "    err = np.mean((sp_smooth-pred_smooth)**2)\n",
    "    cc = np.corrcoef(sp_smooth, pred_smooth)\n",
    "    cc_all[celln,lag_ind] = cc[0,1]\n",
    "\n",
    "    axs[0,lag_ind].plot(sp_smooth,'k',label='smoothed FR')\n",
    "    axs[0,lag_ind].plot(pred_smooth,'r', label='pred FR')\n",
    "    axs[0,lag_ind].set_title('cc={:.2f}'.format(cc_all[celln,lag_ind]))\n",
    "    axs[1,lag_ind].imshow(sta_all[celln,lag_ind])\n",
    "    axs[1,lag_ind].set_title('lag={:d}'.format(lag_list[lag_ind]))\n",
    "    axs[1,lag_ind].axis('off')\n",
    "    plt.suptitle('No Smoothness w/ movements')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assigned-publication",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(msetrain)\n",
    "plt.plot(msetest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concerned-housing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##### Test_Train split with test=first, train=later #####\n",
    "# model_vid = model_vid_sm\n",
    "# model_dt = .1\n",
    "# nks = np.shape(model_vid)[1:]; nk = nks[0]*nks[1]\n",
    "# nT = np.shape(model_nsp)[0]\n",
    "# x = model_vid.reshape(model_nsp.shape[0], -1).copy()\n",
    "# # image dimensions\n",
    "# n_units = np.shape(model_nsp)[1]\n",
    "# # subtract mean and renormalize -- necessary? \n",
    "# mn_img = np.mean(x,axis=0)\n",
    "# x = x-mn_img\n",
    "# x = x/np.std(x,axis =0)\n",
    "# x = np.append(x,np.ones((nT,1)), axis = 1) # append column of ones\n",
    "# test_frac = 0.7\n",
    "# ntest = int(nT*test_frac)\n",
    "# titles = np.array(['th','phi','roll','pitch'])\n",
    "# move_train = np.hstack((model_th[:ntest,np.newaxis],model_phi[:ntest,np.newaxis],model_roll[:ntest,np.newaxis],model_pitch[:ntest,np.newaxis],model_dth[:ntest,np.newaxis],model_dphi[:ntest,np.newaxis]))\n",
    "# move_test = np.hstack((model_th[ntest:,np.newaxis],model_phi[ntest:,np.newaxis],model_roll[ntest:,np.newaxis],model_pitch[ntest:,np.newaxis],model_dth[ntest:,np.newaxis],model_dphi[ntest:,np.newaxis]))\n",
    "\n",
    "# perms = np.array([2,3]) #np.array(list(itertools.combinations([0,1,2,3], n)))\n",
    "# move_train = move_train[:,perms]\n",
    "# move_test = move_test[:,perms]\n",
    "\n",
    "\n",
    "# # set up prior matrix (regularizer)\n",
    "# # L2 prior\n",
    "# Imat = np.eye(nk)\n",
    "# Imat = linalg.block_diag(Imat,np.zeros((1+move_test.shape[-1],1+move_test.shape[-1])))\n",
    "# # smoothness prior\n",
    "# consecutive = np.ones((nk, 1))\n",
    "# consecutive[nks[1]-1::nks[1]] = 0\n",
    "# diff = np.zeros((1,2))\n",
    "# diff[0,0] = -1\n",
    "# diff[0,1]= 1\n",
    "# Dxx = sparse.diags((consecutive @ diff).T, np.array([0, 1]), (nk-1,nk))\n",
    "# Dxy = sparse.diags((np.ones((nk,1))@ diff).T, np.array([0, nks[1]]), (nk-nks[1], nk))\n",
    "# Dx = Dxx.T @ Dxx + Dxy.T @ Dxy\n",
    "# D  = linalg.block_diag(Dx.toarray(),np.zeros((1+move_test.shape[-1],1+move_test.shape[-1])))   \n",
    "# # summed prior matrix\n",
    "# # Cinv = D + Imat\n",
    "# Cinv = Imat\n",
    "\n",
    "# lag_list = [ -4, -2, 0 , 2, 4]\n",
    "# lambdas = 1024 * (2**np.arange(0,16))\n",
    "# nlam = len(lambdas)\n",
    "# # set up empty arrays for receptive field and cross correlation\n",
    "# sta_all = np.zeros((n_units, len(lag_list), nks[0], nks[1]))\n",
    "# cc_all = np.zeros((n_units,len(lag_list)))\n",
    "\n",
    "# celln = 51\n",
    "# fig, axs = plt.subplots(2,len(lag_list), figsize=(np.floor(7.5*len(lag_list)).astype(int),10))\n",
    "# # iterate through timing lags\n",
    "# for lag_ind, lag in enumerate(lag_list):\n",
    "#     sps = np.roll(model_nsp.T[celln,:],-lag)\n",
    "#     nT = len(sps)\n",
    "#     #split training and test data\n",
    "#     test_frac = 0.7\n",
    "#     ntest = int(nT*test_frac)\n",
    "#     x_train = x[:ntest,:] ; sps_train = sps[:ntest]\n",
    "#     x_test = x[ntest:,:]; sps_test = sps[ntest:]\n",
    "\n",
    "#     x_train = np.concatenate((x_train,move_train),axis=1)\n",
    "#     x_test = np.concatenate((x_test,move_test),axis=1)\n",
    "\n",
    "#     #calculate a few terms\n",
    "#     sta = x_train.T@sps_train/np.sum(sps_train)\n",
    "#     XXtr = x_train.T @ x_train\n",
    "#     XYtr = x_train.T @sps_train\n",
    "#     msetrain = np.zeros((nlam,1))\n",
    "#     msetest = np.zeros((nlam,1))\n",
    "#     w_ridge = np.zeros((nk+1+move_test.shape[-1],nlam))\n",
    "#     # initial guess\n",
    "#     w = sta\n",
    "#     # loop over regularization strength\n",
    "#     for l in range(len(lambdas)):  \n",
    "#         # calculate MAP estimate               \n",
    "#         w = np.linalg.solve(XXtr + lambdas[l]*Cinv, XYtr) # equivalent of \\ (left divide) in matlab\n",
    "#         w_ridge[:,l] = w\n",
    "#         # calculate test and training rms error\n",
    "#         msetrain[l] = np.mean((sps_train - x_train@w)**2)\n",
    "#         msetest[l] = np.mean((sps_test - x_test@w)**2)\n",
    "#     # select best cross-validated lambda for RF\n",
    "#     best_lambda = np.argmin(msetest)\n",
    "#     w = w_ridge[:,best_lambda]\n",
    "#     ridge_rf = w_ridge[:,best_lambda]\n",
    "#     sta_all[celln,lag_ind,:,:] = np.reshape(w[:nk],nks)\n",
    "#     # plot predicted vs actual firing rate\n",
    "#     # predicted firing rate\n",
    "#     sp_pred = x_test@ridge_rf\n",
    "#     # bin the firing rate to get smooth rate vs time\n",
    "#     bin_length = 80\n",
    "#     sp_smooth = (np.convolve(sps_test, np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "#     pred_smooth = (np.convolve(sp_pred, np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "#     # a few diagnostics\n",
    "#     err = np.mean((sp_smooth-pred_smooth)**2)\n",
    "#     cc = np.corrcoef(sp_smooth, pred_smooth)\n",
    "#     cc_all[celln,lag_ind] = cc[0,1]\n",
    "    \n",
    "\n",
    "#     axs[0,lag_ind].plot(sp_smooth,'k',label='smoothed FR')\n",
    "#     axs[0,lag_ind].plot(pred_smooth,'r', label='pred FR')\n",
    "#     axs[0,lag_ind].set_title('cc={:.2f}'.format(cc_all[celln,lag_ind]))\n",
    "#     axs[1,lag_ind].imshow(sta_all[celln,lag_ind])\n",
    "#     axs[1,lag_ind].set_title('lag={:d}'.format(lag_list[lag_ind]))\n",
    "#     axs[1,lag_ind].axis('off')\n",
    "#     plt.suptitle('No Smoothness splitdata pipeline')\n",
    "#     plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romance-stuart",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(msetrain)\n",
    "plt.plot(msetest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "young-recipient",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_vid = model_vid_sm\n",
    "model_dt = .1\n",
    "nks = np.shape(model_vid)[1:]; nk = nks[0]*nks[1]\n",
    "nT = np.shape(model_nsp)[0]\n",
    "x = model_vid.reshape(model_nsp.shape[0], -1).copy()\n",
    "# image dimensions\n",
    "n_units = np.shape(model_nsp)[1]\n",
    "# subtract mean and renormalize -- necessary? \n",
    "mn_img = np.mean(x,axis=0)\n",
    "x = x-mn_img\n",
    "x = x/np.std(x,axis =0)\n",
    "x = np.append(x,np.ones((nT,1)), axis = 1) # append column of ones\n",
    "\n",
    "test_frac = 0.3\n",
    "ntest = int(nT*test_frac)\n",
    "titles = np.array(['th','phi','roll','pitch'])\n",
    "move_train = np.hstack((model_th[ntest:,np.newaxis],model_phi[ntest:,np.newaxis],model_roll[ntest:,np.newaxis],model_pitch[ntest:,np.newaxis],model_dth[ntest:,np.newaxis],model_dphi[ntest:,np.newaxis]))\n",
    "move_test = np.hstack((model_th[:ntest,np.newaxis],model_phi[:ntest,np.newaxis],model_roll[:ntest,np.newaxis],model_pitch[:ntest,np.newaxis],model_dth[:ntest,np.newaxis],model_dphi[:ntest,np.newaxis]))\n",
    "\n",
    "perms = np.array([2,3]) #np.array(list(itertools.combinations([0,1,2,3], n)))\n",
    "move_train = move_train[:,perms]\n",
    "move_test = move_test[:,perms]\n",
    "\n",
    "# set up prior matrix (regularizer)\n",
    "# L2 prior\n",
    "Imat = np.eye(nk)\n",
    "Imat = linalg.block_diag(Imat,np.zeros((1+move_test.shape[-1],1+move_test.shape[-1])))\n",
    "# smoothness prior\n",
    "consecutive = np.ones((nk, 1))\n",
    "consecutive[nks[1]-1::nks[1]] = 0\n",
    "diff = np.zeros((1,2))\n",
    "diff[0,0] = -1\n",
    "diff[0,1]= 1\n",
    "Dxx = sparse.diags((consecutive @ diff).T, np.array([0, 1]), (nk-1,nk))\n",
    "Dxy = sparse.diags((np.ones((nk,1))@ diff).T, np.array([0, nks[1]]), (nk-nks[1], nk))\n",
    "Dx = Dxx.T @ Dxx + Dxy.T @ Dxy\n",
    "D  = linalg.block_diag(Dx.toarray(),np.zeros((1+move_test.shape[-1],1+move_test.shape[-1])))   \n",
    "# summed prior matrix\n",
    "# Cinv = D + Imat\n",
    "Cinv = Imat\n",
    "\n",
    "lag_list = [ -4, -2, 0 , 2, 4]\n",
    "lambdas = 1024 * (2**np.arange(0,16))\n",
    "nlam = len(lambdas)\n",
    "# set up empty arrays for receptive field and cross correlation\n",
    "sta_all = np.zeros((n_units, len(lag_list), nks[0], nks[1]))\n",
    "cc_all = np.zeros((n_units,len(lag_list)))\n",
    "\n",
    "celln = 51\n",
    "fig, axs = plt.subplots(2,len(lag_list), figsize=(np.floor(7.5*len(lag_list)).astype(int),10))\n",
    "# iterate through timing lags\n",
    "for lag_ind, lag in enumerate(lag_list):\n",
    "    sps = np.roll(model_nsp.T[celln,:],-lag)\n",
    "    nT = len(sps)\n",
    "    #split training and test data\n",
    "\n",
    "    x_train = x[ntest:,:] ; sps_train = sps[ntest:]\n",
    "    x_test = x[:ntest,:]; sps_test = sps[:ntest]\n",
    "    \n",
    "    \n",
    "    x_train = np.concatenate((x_train,move_train),axis=1) # x_train*(1+alpha*model_th)\n",
    "    x_test = np.concatenate((x_test,move_test),axis=1)\n",
    "\n",
    "    #calculate a few terms\n",
    "    sta = x_train.T@sps_train/np.sum(sps_train)\n",
    "    XXtr = x_train.T @ x_train \n",
    "    XYtr = x_train.T @sps_train\n",
    "    msetrain = np.zeros((nlam,1))\n",
    "    msetest = np.zeros((nlam,1))\n",
    "    w_ridge = np.zeros((nk+1+move_test.shape[-1],nlam))\n",
    "    # initial guess\n",
    "    w = sta\n",
    "    # loop over regularization strength\n",
    "    for l in range(len(lambdas)):  \n",
    "        # calculate MAP estimate               \n",
    "        w = np.linalg.solve(XXtr + lambdas[l]*Cinv, XYtr) # equivalent of \\ (left divide) in matlab\n",
    "        w_ridge[:,l] = w\n",
    "        # calculate test and training rms error\n",
    "        msetrain[l] = np.mean((sps_train - x_train@w)**2)\n",
    "        msetest[l] = np.mean((sps_test - x_test@w)**2)\n",
    "    # select best cross-validated lambda for RF\n",
    "    best_lambda = np.argmin(msetest)\n",
    "    w = w_ridge[:,best_lambda]\n",
    "    ridge_rf = w_ridge[:,best_lambda]\n",
    "    sta_all[celln,lag_ind,:,:] = np.reshape(w[:nk],nks)\n",
    "    # plot predicted vs actual firing rate\n",
    "    # predicted firing rate\n",
    "    sp_pred = x_test@ridge_rf\n",
    "    # bin the firing rate to get smooth rate vs time\n",
    "    bin_length = 40\n",
    "    sp_smooth = (np.convolve(sps_test, np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "    pred_smooth = (np.convolve(sp_pred, np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "    # a few diagnostics\n",
    "    err = np.mean((sp_smooth-pred_smooth)**2)\n",
    "    cc = np.corrcoef(sp_smooth, pred_smooth)\n",
    "    cc_all[celln,lag_ind] = cc[0,1]\n",
    "    \n",
    "\n",
    "    axs[0,lag_ind].plot(sp_smooth,'k',label='smoothed FR')\n",
    "    axs[0,lag_ind].plot(pred_smooth,'r', label='pred FR')\n",
    "    axs[0,lag_ind].set_title('cc={:.2f}'.format(cc_all[celln,lag_ind]))\n",
    "    axs[1,lag_ind].imshow(sta_all[celln,lag_ind])\n",
    "    axs[1,lag_ind].set_title('lag={:d}'.format(lag_list[lag_ind]))\n",
    "    axs[1,lag_ind].axis('off')\n",
    "    plt.suptitle('With/out Smoothness splitdata pipeline')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operational-onion",
   "metadata": {},
   "outputs": [],
   "source": [
    "w[-move_test.shape[-1]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peaceful-centre",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(msetrain)\n",
    "plt.plot(msetest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corrected-directory",
   "metadata": {},
   "source": [
    "## GLM Movement Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stopped-secret",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'ridgecv'\n",
    "if model_type == 'elasticnetcv':\n",
    "    model = make_pipeline(StandardScaler(), lm.ElasticNetCV()) # lm.RidgeCV(alphas=np.arange(100,10000,1000))) #  #MultiOutputRegressor(lm.Ridge(),n_jobs=-1)) \n",
    "elif model_type == 'ridgecv':\n",
    "    model = make_pipeline(StandardScaler(), lm.RidgeCV(alphas=lambdas))\n",
    "\n",
    "\n",
    "test_frac = 0.3\n",
    "ntest = int(nT*test_frac)\n",
    "titles = np.array(['th','phi','roll','pitch','dth','dphi'])\n",
    "move_train = np.hstack((model_th[ntest:,np.newaxis],model_phi[ntest:,np.newaxis],model_roll[ntest:,np.newaxis],model_pitch[ntest:,np.newaxis],model_dth[ntest:,np.newaxis],model_dphi[ntest:,np.newaxis]))\n",
    "move_test = np.hstack((model_th[:ntest,np.newaxis],model_phi[:ntest,np.newaxis],model_roll[:ntest,np.newaxis],model_pitch[:ntest,np.newaxis],model_dth[:ntest,np.newaxis],model_dphi[:ntest,np.newaxis]))\n",
    "sps_smooth_all = np.zeros((15,len(lag_list),move_test.shape[0]))\n",
    "pred_smooth_all = np.zeros((15,len(lag_list),move_test.shape[0]))\n",
    "cc_all = np.zeros((n_units,15,len(lag_list)))\n",
    "model_coef_all= [] # = np.zeros((15,len(lag_list)))\n",
    "titles_all = []\n",
    "\n",
    "celln = 51\n",
    "bin_length = 80\n",
    "model_ind = 0\n",
    "with PdfPages(FigPath/ 'ModelSelection_{}_MoveOnly.pdf'.format(model_type)) as pdf:\n",
    "\n",
    "    for n in range(1,5):\n",
    "        perms = np.array(list(itertools.combinations([0,1,2,3], n)))\n",
    "        for ind in range(perms.shape[0]):\n",
    "            fig, axs = plt.subplots(1,len(lag_list), figsize=(np.floor(7.5*len(lag_list)).astype(int),5))\n",
    "            move_train2 = move_train[:,perms[ind]]\n",
    "            move_test2 = move_test[:,perms[ind]]\n",
    "            # iterate through timing lags\n",
    "            for lag_ind, lag in enumerate(lag_list):\n",
    "                sps = np.roll(model_nsp.T[celln,:],-lag)\n",
    "                nT = len(sps)\n",
    "                sps_train = sps[ntest:]\n",
    "                sps_test = sps[:ntest]\n",
    "\n",
    "                model.fit(move_train2,sps_train)\n",
    "                sps_pred = model.predict(move_test2)\n",
    "\n",
    "                sps_smooth = (np.convolve(sps_test, np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "                pred_smooth = (np.convolve(sps_pred, np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "                cc_all[celln,model_ind,lag_ind] = np.corrcoef(sps_smooth, pred_smooth)[0,1]\n",
    "                sps_smooth_all[model_ind,lag_ind] = sps_smooth\n",
    "                pred_smooth_all[model_ind,lag_ind] = pred_smooth\n",
    "                model_coef_all.append(model[model_type].coef_)\n",
    "                \n",
    "                axs[lag_ind].plot(sps_smooth,'k',label='smoothed FR')\n",
    "                axs[lag_ind].plot(pred_smooth,'r', label='pred FR')\n",
    "                axs[lag_ind].set_title('cc={:.3f}'.format(cc_all[celln,model_ind,lag_ind]))\n",
    "            #     axs[1,lag_ind].imshow(sta_all[celln,lag_ind])\n",
    "            #     axs[1,lag_ind].set_title('lag={:d}'.format(lag_list[lag_ind]))\n",
    "            #     axs[1,lag_ind].axis('off')\n",
    "            #     plt.suptitle('No Smoothness splitdata pipeline')\n",
    "                \n",
    "            titles_all.append('_'.join([t for t in titles[perms[ind]]]))\n",
    "            plt.suptitle(titles_all[-1])\n",
    "            plt.tight_layout()\n",
    "            pdf.savefig()\n",
    "            \n",
    "            model_ind+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removable-ivory",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unravel_index(np.argmax(cc_all),shape=cc_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "happy-karma",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vid = model_vid_sm\n",
    "model_dt = .1\n",
    "nks = np.shape(model_vid)[1:]; nk = nks[0]*nks[1]\n",
    "nT = np.shape(model_nsp)[0]\n",
    "x = model_vid.reshape(model_nsp.shape[0], -1).copy()\n",
    "# image dimensions\n",
    "n_units = np.shape(model_nsp)[1]\n",
    "# subtract mean and renormalize -- necessary? \n",
    "mn_img = np.mean(x,axis=0)\n",
    "x = x-mn_img\n",
    "x = x/np.std(x,axis =0)\n",
    "x = np.append(x,np.ones((nT,1)), axis = 1) # append column of ones\n",
    "\n",
    "# set up prior matrix (regularizer)\n",
    "# L2 prior\n",
    "Imat = np.eye(nk)\n",
    "Imat = linalg.block_diag(Imat,np.zeros((1,1)))\n",
    "# smoothness prior\n",
    "consecutive = np.ones((nk, 1))\n",
    "consecutive[nks[1]-1::nks[1]] = 0\n",
    "diff = np.zeros((1,2))\n",
    "diff[0,0] = -1\n",
    "diff[0,1]= 1\n",
    "Dxx = sparse.diags((consecutive @ diff).T, np.array([0, 1]), (nk-1,nk))\n",
    "Dxy = sparse.diags((np.ones((nk,1))@ diff).T, np.array([0, nks[1]]), (nk-nks[1], nk))\n",
    "Dx = Dxx.T @ Dxx + Dxy.T @ Dxy\n",
    "D  = linalg.block_diag(Dx.toarray(),np.zeros((1,1)))   \n",
    "# summed prior matrix\n",
    "Cinv = D + Imat\n",
    "# Cinv = Imat\n",
    "\n",
    "lag_list = [ -4, -2, 0 , 2, 4]\n",
    "lambdas = 1024 * (2**np.arange(0,16))\n",
    "nlam = len(lambdas)\n",
    "# set up empty arrays for receptive field and cross correlation\n",
    "sta_all = np.zeros((n_units, len(lag_list), nks[0], nks[1]))\n",
    "cc_all = np.zeros((n_units,len(lag_list)))\n",
    "\n",
    "celln = 51\n",
    "# iterate through timing lags\n",
    "fig, axs = plt.subplots(2,len(lag_list), figsize=(np.floor(7.5*len(lag_list)).astype(int),10))\n",
    "for lag_ind, lag in enumerate(lag_list):\n",
    "    sps = np.roll(model_nsp.T[celln,:],-lag)\n",
    "    nT = len(sps)\n",
    "    #split training and test data\n",
    "    test_frac = 0.3\n",
    "    ntest = int(nT*test_frac)\n",
    "    x_train = x[ntest:,:] ; sps_train = sps[ntest:]\n",
    "    x_test = x[:ntest,:]; sps_test = sps[:ntest]\n",
    "    #calculate a few terms\n",
    "    sta = x_train.T@sps_train/np.sum(sps_train)\n",
    "    XXtr = x_train.T @ x_train\n",
    "    XYtr = x_train.T @sps_train\n",
    "    msetrain = np.zeros((nlam,1))\n",
    "    msetest = np.zeros((nlam,1))\n",
    "    w_ridge = np.zeros((nk+1,nlam))\n",
    "    # initial guess\n",
    "    w = sta\n",
    "    # loop over regularization strength\n",
    "    for l in range(len(lambdas)):  \n",
    "        # calculate MAP estimate               \n",
    "        w = np.linalg.solve(XXtr + lambdas[l]*Cinv, XYtr) # equivalent of \\ (left divide) in matlab\n",
    "        w_ridge[:,l] = w\n",
    "        # calculate test and training rms error\n",
    "        msetrain[l] = np.mean((sps_train - x_train@w)**2)\n",
    "        msetest[l] = np.mean((sps_test - x_test@w)**2)\n",
    "    # select best cross-validated lambda for RF\n",
    "    best_lambda = np.argmin(msetest)\n",
    "    w = w_ridge[:,best_lambda]\n",
    "    ridge_rf = w_ridge[:,best_lambda]\n",
    "    sta_all[celln,lag_ind,:,:] = np.reshape(w[:-1],nks)\n",
    "    # plot predicted vs actual firing rate\n",
    "    # predicted firing rate\n",
    "    sp_pred = x_test@ridge_rf\n",
    "    # bin the firing rate to get smooth rate vs time\n",
    "    bin_length = 80\n",
    "    sp_smooth = (np.convolve(sps_test, np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "    pred_smooth = (np.convolve(sp_pred, np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "    # a few diagnostics\n",
    "    err = np.mean((sp_smooth-pred_smooth)**2)\n",
    "    cc = np.corrcoef(sp_smooth, pred_smooth)\n",
    "    cc_all[celln,lag_ind] = cc[0,1]\n",
    "    \n",
    "    axs[0,lag_ind].plot(sp_smooth,'k',label='smoothed FR')\n",
    "    axs[0,lag_ind].plot(pred_smooth,'r', label='pred FR')\n",
    "    axs[0,lag_ind].set_title('cc={:.2f}'.format(cc_all[celln,lag_ind]))\n",
    "    axs[1,lag_ind].imshow(sta_all[celln,lag_ind])\n",
    "    axs[1,lag_ind].set_title('lag={:d}'.format(lag_list[lag_ind]))\n",
    "    axs[1,lag_ind].axis('off')\n",
    "    plt.suptitle('Current_pipeline')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sitting-bridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(msetrain)\n",
    "plt.plot(msetest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documentary-impact",
   "metadata": {},
   "source": [
    "# Parallel Processing GLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intensive-sleeve",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def do_glm_fit_mult(train_nsp, test_nsp, train_data, test_data, move_train, move_test, celln, perms, lag, Cinv, lambdas, bin_length=80, model_dt=.1):\n",
    "    sps_train = np.roll(train_nsp[:,celln],-lag)\n",
    "    sps_test = np.roll(test_nsp[:,celln],-lag)\n",
    "    move_train = move_train[:,perms]\n",
    "    move_test = move_test[:,perms]\n",
    "    x_train = train_data.reshape(train_data.shape[0],-1) #train_pcs\n",
    "    x_train = np.append(x_train, np.ones((x_train.shape[0],1)), axis = 1) # append column of ones\n",
    "    x_train = np.concatenate((x_train, move_train),axis=1)\n",
    "\n",
    "    x_test = test_data.reshape(test_data.shape[0],-1) #test_pcs\n",
    "    x_test = np.append(x_test,np.ones((x_test.shape[0],1)), axis = 1) # append column of ones\n",
    "    x_test = np.concatenate((x_test, move_test),axis=1)\n",
    "    \n",
    "    nlam = len(lambdas)\n",
    "    XXtr = x_train.T @ x_train\n",
    "    XYtr = x_train.T @ sps_train\n",
    "    msetrain = np.zeros((nlam,1))\n",
    "    msetest = np.zeros((nlam,1))\n",
    "    w_ridge = np.zeros((x_train.shape[-1],nlam))\n",
    "    \n",
    "    # loop over regularization strength\n",
    "    for l in range(len(lambdas)):  \n",
    "        # calculate MAP estimate               \n",
    "        w = np.linalg.solve(XXtr + lambdas[l]*Cinv, XYtr) # equivalent of \\ (left divide) in matlab\n",
    "        w_ridge[:,l] = w\n",
    "        # calculate test and training rms error\n",
    "        msetrain[l] = np.mean((sps_train - x_train@w)**2)\n",
    "        msetest[l] = np.mean((sps_test - x_test@w)**2)\n",
    "        \n",
    "    # select best cross-validated lambda for RF\n",
    "    best_lambda = np.argmin(msetest)\n",
    "    w = w_ridge[:,best_lambda]\n",
    "    ridge_rf = w_ridge[:,best_lambda]\n",
    "    sta_all = np.reshape(w[:-(1+move_test.shape[-1])],nks)\n",
    "    # plot predicted vs actual firing rate\n",
    "    # predicted firing rate\n",
    "    sp_pred = x_test@ridge_rf\n",
    "    # bin the firing rate to get smooth rate vs time\n",
    "    sp_smooth = (np.convolve(sps_test, np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "    pred_smooth = (np.convolve(sp_pred, np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "    # a few diagnostics\n",
    "    err = np.mean((sp_smooth-pred_smooth)**2)\n",
    "    cc = np.corrcoef(sp_smooth, pred_smooth)\n",
    "    cc_all = cc[0,1]\n",
    "    return cc_all, sta_all, sp_smooth, pred_smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deadly-fitness",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@ray.remote\n",
    "def do_glm_fit(train_nsp, test_nsp, train_data, test_data, move_train, move_test, celln, perms, lag, Cinv, lambdas, bin_length=40, model_dt=.1):\n",
    "\n",
    "    sps_train = np.roll(train_nsp[:,celln],-lag)\n",
    "    sps_test = np.roll(test_nsp[:,celln],-lag)\n",
    "    move_train = move_train[:,perms]\n",
    "    move_test = move_test[:,perms]\n",
    "    x_train = train_data.reshape(train_data.shape[0],-1) #train_pcs\n",
    "    x_train = np.append(x_train, np.ones((x_train.shape[0],1)), axis = 1) # append column of ones\n",
    "    x_train = np.concatenate((x_train, move_train),axis=1)\n",
    "\n",
    "    x_test = test_data.reshape(test_data.shape[0],-1) #test_pcs\n",
    "    x_test = np.append(x_test,np.ones((x_test.shape[0],1)), axis = 1) # append column of ones\n",
    "    x_test = np.concatenate((x_test, move_test),axis=1)\n",
    "    \n",
    "    nlam = len(lambdas)\n",
    "#     sta = x_train.T@ sps_train/np.sum(sps_train)\n",
    "    XXtr = x_train.T @ x_train\n",
    "    XYtr = x_train.T @ sps_train\n",
    "    msetrain = np.zeros((nlam,1))\n",
    "    msetest = np.zeros((nlam,1))\n",
    "    w_ridge = np.zeros((x_train.shape[-1],nlam))\n",
    "    # initial guess\n",
    "#     w = sta\n",
    "    # loop over regularization strength\n",
    "    for l in range(len(lambdas)):  \n",
    "        # calculate MAP estimate               \n",
    "        w = np.linalg.solve(XXtr + lambdas[l]*Cinv, XYtr) # equivalent of \\ (left divide) in matlab\n",
    "        w_ridge[:,l] = w\n",
    "        # calculate test and training rms error\n",
    "        msetrain[l] = np.mean((sps_train - x_train@w)**2)\n",
    "        msetest[l] = np.mean((sps_test - x_test@w)**2)\n",
    "    # select best cross-validated lambda for RF\n",
    "    best_lambda = np.argmin(msetest)\n",
    "    w = w_ridge[:,best_lambda]\n",
    "    ridge_rf = w_ridge[:,best_lambda]\n",
    "    sta_all = np.reshape(w[:-(1+move_test.shape[-1])],nks)\n",
    "    # plot predicted vs actual firing rate\n",
    "    # predicted firing rate\n",
    "    sp_pred = x_test@ridge_rf\n",
    "    # bin the firing rate to get smooth rate vs time\n",
    "#     bin_length = 80\n",
    "    sp_smooth = (np.convolve(sps_test, np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "    pred_smooth = (np.convolve(sp_pred, np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "    # a few diagnostics\n",
    "    err = np.mean((sp_smooth-pred_smooth)**2)\n",
    "    cc = np.corrcoef(sp_smooth, pred_smooth)\n",
    "    cc_all = cc[0,1]\n",
    "    return cc_all, sta_all, sp_smooth, pred_smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "soviet-landing",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "titles = np.array(['th','phi','roll','pitch'])\n",
    "titles_all = []\n",
    "for n in range(1,5):\n",
    "    perms = np.array(list(itertools.combinations([0,1,2,3], n)))\n",
    "    for ind in range(perms.shape[0]):\n",
    "        titles_all.append('_'.join([t for t in titles[perms[ind]]]))\n",
    "        \n",
    "move_train = np.hstack((train_th[:,np.newaxis],train_phi[:,np.newaxis],train_roll[:,np.newaxis],train_pitch[:,np.newaxis],train_dth[:,np.newaxis],train_dphi[:,np.newaxis]))\n",
    "move_test = np.hstack((test_th[:,np.newaxis],test_phi[:,np.newaxis],test_roll[:,np.newaxis],test_pitch[:,np.newaxis],test_dth[:,np.newaxis],test_dphi[:,np.newaxis]))\n",
    "\n",
    "lag_list = [ -4, -2, 0 , 2, 4]\n",
    "lambdas = 1024 * (2**np.arange(0,16))\n",
    "\n",
    "train_nsp_r = ray.put(train_nsp)\n",
    "test_nsp_r = ray.put(test_nsp)\n",
    "train_data_r = ray.put(train_vid)\n",
    "test_data_r = ray.put(test_vid)\n",
    "move_train_r = ray.put(move_train)\n",
    "move_test_r = ray.put(move_test)\n",
    "result_ids = []\n",
    "celln = 51\n",
    "for celln in range(train_nsp.shape[1]):\n",
    "    for n in range(1,5):\n",
    "        perms = np.array(list(itertools.combinations([0,1,2,3], n)))\n",
    "        for ind in range(perms.shape[0]):\n",
    "        \n",
    "            move_train2 = move_train[:,perms[ind]]\n",
    "            move_test2 = move_test[:,perms[ind]]\n",
    "\n",
    "            # set up prior matrix (regularizer)\n",
    "            # L2 prior\n",
    "            Imat = np.eye(nk)\n",
    "            Imat = linalg.block_diag(Imat,np.zeros((1+move_test2.shape[-1],1+move_test2.shape[-1])))\n",
    "            # smoothness prior\n",
    "            consecutive = np.ones((nk, 1))\n",
    "            consecutive[nks[1]-1::nks[1]] = 0\n",
    "            diff = np.zeros((1,2))\n",
    "            diff[0,0] = -1\n",
    "            diff[0,1]= 1\n",
    "            Dxx = sparse.diags((consecutive @ diff).T, np.array([0, 1]), (nk-1,nk))\n",
    "            Dxy = sparse.diags((np.ones((nk,1))@ diff).T, np.array([0, nks[1]]), (nk-nks[1], nk))\n",
    "            Dx = Dxx.T @ Dxx + Dxy.T @ Dxy\n",
    "            D  = linalg.block_diag(Dx.toarray(),np.zeros((1+move_test2.shape[-1],1+move_test2.shape[-1])))   \n",
    "            # summed prior matrix\n",
    "            # Cinv = D + Imat\n",
    "            Cinv = Imat\n",
    "\n",
    "            for lag_ind, lag in enumerate(lag_list):    \n",
    "                result_ids.append(do_glm_fit.remote(train_nsp_r, test_nsp_r, train_data_r, test_data_r, move_train_r, move_test_r, celln, perms[ind], lag, Cinv, lambdas))\n",
    "                      \n",
    "results_p = ray.get(result_ids)\n",
    "print('GLM: ', time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "buried-mumbai",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_all = np.stack([results_p[i][0] for i in range(len(results_p))])\n",
    "sta_all = np.stack([results_p[i][1] for i in range(len(results_p))])\n",
    "sp_smooth = np.stack([results_p[i][2] for i in range(len(results_p))])\n",
    "pred_smooth = np.stack([results_p[i][3] for i in range(len(results_p))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applied-venue",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_all = cc_all.reshape((model_nsp.shape[1],len(titles_all),len(lag_list),) + cc_all.shape[1:])\n",
    "sta_all = sta_all.reshape((model_nsp.shape[1],len(titles_all),len(lag_list),) + sta_all.shape[1:])\n",
    "sp_smooth = sp_smooth.reshape((model_nsp.shape[1],len(titles_all),len(lag_list),) + sp_smooth.shape[1:])\n",
    "pred_smooth = pred_smooth.reshape((model_nsp.shape[1],len(titles_all),len(lag_list),) + pred_smooth.shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "musical-foundation",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_cells, m_models, m_lags = np.where(cc_all==np.max(cc_all,axis=(-2,-1), keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mechanical-twins",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcc = cc_all[m_cells,m_models,m_lags]\n",
    "msta = sta_all[m_cells,m_models,m_lags]\n",
    "msp = sp_smooth[m_cells,m_models,m_lags]\n",
    "mpred = pred_smooth[m_cells,m_models,m_lags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacterial-botswana",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.imshow(msta, animation_frame=0, binary_string=False,color_continuous_scale='RdBu_r')\n",
    "fig.update_layout(width=500,\n",
    "                  height=500,\n",
    "                 )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "located-haiti",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Explore Neurons #####\n",
    "ind = 25\n",
    "fig, axs = plt.subplots(1,2, figsize=((15,5))) #np.floor(7.5*len(model_nsp)).astype(int)\n",
    "axs[0].plot(msp[ind],'k',label='test FR')\n",
    "axs[0].plot(mpred[ind],'r', label='pred FR')\n",
    "axs[0].legend()\n",
    "axs[0].set_title('cc={:.2f}, {}, \\n lag={:d}'.format(mcc[ind],titles_all[m_models[ind]],lag_list[m_lags[ind]]))\n",
    "axs[1].imshow(msta[ind],cmap='seismic')\n",
    "axs[1].axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conceptual-spirituality",
   "metadata": {},
   "outputs": [],
   "source": [
    "with PdfPages(FigPath/ 'MaxCC_{}.pdf'.format(model_type)) as pdf:\n",
    "    for ind in range(0,model_nsp.shape[0]):\n",
    "        \n",
    "        fig, axs = plt.subplots(10,2, figsize=((15,3*10))) #np.floor(7.5*len(model_nsp)).astype(int)\n",
    "        axs[ind,0].plot(msp[ind],'k',label='smoothed FR')\n",
    "        axs[ind,0].plot(mpred[ind],'r', label='pred FR')\n",
    "        axs[ind,0].set_title('cc={:.2f}, {}, \\n lag={:d}'.format(mcc[ind],titles_all[m_models[ind]],lag_list[m_lags[ind]]))\n",
    "        axs[ind,1].imshow(msta[ind])\n",
    "        axs[ind,1].axis('off')\n",
    "        plt.tight_layout()\n",
    "        pdf.savefig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passive-fifty",
   "metadata": {},
   "outputs": [],
   "source": [
    "celln=33\n",
    "model_ind=13\n",
    "fig, axs = plt.subplots(2,len(lag_list), figsize=(np.floor(7.5*len(lag_list)).astype(int),10))\n",
    "for lag_ind, lag in enumerate(lag_list):\n",
    "    axs[0,lag_ind].plot(sp_smooth[celln,model_ind,lag_ind],'k',label='smoothed FR')\n",
    "    axs[0,lag_ind].plot(pred_smooth[celln,model_ind,lag_ind],'r', label='pred FR')\n",
    "    axs[0,lag_ind].set_title('cc={:.2f}'.format(cc_all[celln,model_ind,lag_ind]))\n",
    "    axs[1,lag_ind].imshow(sta_all[celln,model_ind,lag_ind])\n",
    "    axs[1,lag_ind].set_title('lag={:d}'.format(lag_list[lag_ind]))\n",
    "    axs[1,lag_ind].axis('off')\n",
    "    plt.suptitle(titles_all[model_ind])\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separate-lloyd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_all.reshape(n_units,len(lag_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "early-stereo",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def do_glm_fit(model_nsp, x, celln, lag, model_dt, lambdas, lag_list, test_frac=.3, bin_length=80):\n",
    "    sps = np.roll(model_nsp[celln,:],-lag)\n",
    "    nT = len(sps)\n",
    "    #split training and test data\n",
    "#     test_frac = 0.3\n",
    "    ntest = int(nT*test_frac)\n",
    "    x_train = x[ntest:,:] ; sps_train = sps[ntest:]\n",
    "    x_test = x[:ntest,:]; sps_test = sps[:ntest]\n",
    "    #calculate a few terms\n",
    "    sta = x_train.T@ sps_train/np.sum(sps_train)\n",
    "    XXtr = x_train.T @ x_train\n",
    "    XYtr = x_train.T @ sps_train\n",
    "    msetrain = np.zeros((nlam,1))\n",
    "    msetest = np.zeros((nlam,1))\n",
    "    w_ridge = np.zeros((nk+1,nlam))\n",
    "    # initial guess\n",
    "    w = sta\n",
    "    # loop over regularization strength\n",
    "    for l in range(len(lambdas)):  \n",
    "        # calculate MAP estimate               \n",
    "        w = np.linalg.solve(XXtr + lambdas[l]*Cinv, XYtr) # equivalent of \\ (left divide) in matlab\n",
    "        w_ridge[:,l] = w\n",
    "        # calculate test and training rms error\n",
    "        msetrain[l] = np.mean((sps_train - x_train@w)**2)\n",
    "        msetest[l] = np.mean((sps_test - x_test@w)**2)\n",
    "    # select best cross-validated lambda for RF\n",
    "    best_lambda = np.argmin(msetest)\n",
    "    w = w_ridge[:,best_lambda]\n",
    "    ridge_rf = w_ridge[:,best_lambda]\n",
    "    sta_all = np.reshape(w[:-1],nks)\n",
    "    # plot predicted vs actual firing rate\n",
    "    # predicted firing rate\n",
    "    sp_pred = x_test@ridge_rf\n",
    "    # bin the firing rate to get smooth rate vs time\n",
    "#     bin_length = 80\n",
    "    sp_smooth = (np.convolve(sps_test, np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "    pred_smooth = (np.convolve(sp_pred, np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "    # a few diagnostics\n",
    "    err = np.mean((sp_smooth-pred_smooth)**2)\n",
    "    cc = np.corrcoef(sp_smooth, pred_smooth)\n",
    "    cc_all = cc[0,1]\n",
    "    return cc_all, sta_all, sp_smooth, pred_smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "difficult-founder",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extreme-webcam",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "model_nsp_r = ray.put(model_nsp.T)\n",
    "x_r = ray.put(x)\n",
    "model_dt_r = ray.put(model_dt)\n",
    "result_ids = []\n",
    "[result_ids.append(do_glm_fit.remote(model_nsp_r, x_r, celln, lag, model_dt_r, lambdas, lag_list, test_frac=.3, bin_length=80)) for celln in range(model_nsp.shape[1]) for lag in lag_list]\n",
    "results_p = ray.get(result_ids)\n",
    "print('GLM: ', time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "drawn-bargain",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_all = np.stack([results_p[i][0] for i in range(len(results_p))])\n",
    "sta_all = np.stack([results_p[i][1] for i in range(len(results_p))])\n",
    "sp_smooth = np.stack([results_p[i][2] for i in range(len(results_p))])\n",
    "pred_smooth = np.stack([results_p[i][3] for i in range(len(results_p))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forward-engineering",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_all = cc_all.reshape((model_nsp.shape[1],len(lag_list),) + cc_all.shape[1:])\n",
    "sta_all = sta_all.reshape((model_nsp.shape[1],len(lag_list),) + sta_all.shape[1:])\n",
    "sp_smooth = sp_smooth.reshape((model_nsp.shape[1],len(lag_list),) + sp_smooth.shape[1:])\n",
    "pred_smooth = pred_smooth.reshape((model_nsp.shape[1],len(lag_list),) + pred_smooth.shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "married-perspective",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_all.shape,sta_all.shape,sp_smooth.shape,pred_smooth.shape,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metallic-crisis",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(sta_all[51,2])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serial-auckland",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vid = model_vid_sm\n",
    "model_dt = .1\n",
    "nks = np.shape(model_vid)[1:]; nk = nks[0]*nks[1]\n",
    "nT = np.shape(model_nsp)[0]\n",
    "x = model_vid.reshape(model_nsp.shape[0], -1).copy()\n",
    "# image dimensions\n",
    "n_units = np.shape(model_nsp)[1]\n",
    "# subtract mean and renormalize -- necessary? \n",
    "mn_img = np.mean(x,axis=0)\n",
    "x = x-mn_img\n",
    "x = x/np.std(x,axis =0)\n",
    "x = np.append(x,np.ones((nT,1)), axis = 1) # append column of ones\n",
    "\n",
    "# set up prior matrix (regularizer)\n",
    "# L2 prior\n",
    "Imat = np.eye(nk)\n",
    "Imat = linalg.block_diag(Imat,np.zeros((1,1)))\n",
    "# smoothness prior\n",
    "consecutive = np.ones((nk, 1))\n",
    "consecutive[nks[1]-1::nks[1]] = 0\n",
    "diff = np.zeros((1,2))\n",
    "diff[0,0] = -1\n",
    "diff[0,1]= 1\n",
    "Dxx = sparse.diags((consecutive @ diff).T, np.array([0, 1]), (nk-1,nk))\n",
    "Dxy = sparse.diags((np.ones((nk,1))@ diff).T, np.array([0, nks[1]]), (nk-nks[1], nk))\n",
    "Dx = Dxx.T @ Dxx + Dxy.T @ Dxy\n",
    "D  = linalg.block_diag(Dx.toarray(),np.zeros((1,1)))   \n",
    "# summed prior matrix\n",
    "Cinv = D + Imat\n",
    "lag_list = [ -4, -2, 0 , 2, 4]\n",
    "lambdas = 1024 * (2**np.arange(0,16))\n",
    "nlam = len(lambdas)\n",
    "# set up empty arrays for receptive field and cross correlation\n",
    "sta_all = np.zeros((n_units, len(lag_list), nks[0], nks[1]))\n",
    "cc_all = np.zeros((n_units,len(lag_list)))\n",
    "# iterate through units\n",
    "for celln in tqdm(range(n_units)):\n",
    "    \n",
    "    # iterate through timing lags\n",
    "    for lag_ind, lag in enumerate(lag_list):\n",
    "        sps = np.roll(model_nsp.T[celln,:],-lag)\n",
    "        nT = len(sps)\n",
    "        #split training and test data\n",
    "        test_frac = 0.3\n",
    "        ntest = int(nT*test_frac)\n",
    "        x_train = x[ntest:,:] ; sps_train = sps[ntest:]\n",
    "        x_test = x[:ntest,:]; sps_test = sps[:ntest]\n",
    "        #calculate a few terms\n",
    "        sta = x_train.T@sps_train/np.sum(sps_train)\n",
    "        XXtr = x_train.T @ x_train\n",
    "        XYtr = x_train.T @sps_train\n",
    "        msetrain = np.zeros((nlam,1))\n",
    "        msetest = np.zeros((nlam,1))\n",
    "        w_ridge = np.zeros((nk+1,nlam))\n",
    "        # initial guess\n",
    "        w = sta\n",
    "        # loop over regularization strength\n",
    "        for l in range(len(lambdas)):  \n",
    "            # calculate MAP estimate               \n",
    "            w = np.linalg.solve(XXtr + lambdas[l]*Cinv, XYtr) # equivalent of \\ (left divide) in matlab\n",
    "            w_ridge[:,l] = w\n",
    "            # calculate test and training rms error\n",
    "            msetrain[l] = np.mean((sps_train - x_train@w)**2)\n",
    "            msetest[l] = np.mean((sps_test - x_test@w)**2)\n",
    "        # select best cross-validated lambda for RF\n",
    "        best_lambda = np.argmin(msetest)\n",
    "        w = w_ridge[:,best_lambda]\n",
    "        ridge_rf = w_ridge[:,best_lambda]\n",
    "        sta_all[celln,lag_ind,:,:] = np.reshape(w[:-1],nks)\n",
    "        # plot predicted vs actual firing rate\n",
    "        # predicted firing rate\n",
    "        sp_pred = x_test@ridge_rf\n",
    "        # bin the firing rate to get smooth rate vs time\n",
    "        bin_length = 80\n",
    "        sp_smooth = (np.convolve(sps_test, np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "        pred_smooth = (np.convolve(sp_pred, np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "        # a few diagnostics\n",
    "        err = np.mean((sp_smooth-pred_smooth)**2)\n",
    "        cc = np.corrcoef(sp_smooth, pred_smooth)\n",
    "        cc_all[celln,lag_ind] = cc[0,1]\n",
    "        \n",
    "# # figure of receptive fields\n",
    "# fig = plt.figure(figsize=(25,np.int(np.ceil(n_units/3))),dpi=50)\n",
    "# for celln in tqdm(range(n_units)):\n",
    "#     for lag_ind, lag in enumerate(lag_list):\n",
    "#         crange = np.max(np.abs(sta_all[celln,:,:,:]))\n",
    "#         plt.subplot(n_units,6,(celln*6)+lag_ind + 1)  \n",
    "#         plt.imshow(sta_all[celln, lag_ind, :, :], vmin=-crange, vmax=crange, cmap='jet')\n",
    "#         plt.title('cc={:.2f}'.format (cc_all[celln,lag_ind]),fontsize=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empirical-narrative",
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure of receptive fields\n",
    "fig = plt.figure(figsize=(25,256),dpi=50)\n",
    "for celln in tqdm(range(n_units)):\n",
    "    for lag_ind, lag in enumerate(lag_list):\n",
    "        crange = np.max(np.abs(sta_all[celln,:,:,:]))\n",
    "        plt.subplot(n_units,6,(celln*6)+lag_ind + 1)  \n",
    "        plt.imshow(sta_all[celln, lag_ind, :, :], vmin=-crange, vmax=crange, cmap='jet')\n",
    "        plt.title('cc={:.2f}'.format (cc_all[celln,lag_ind]),fontsize=5)\n",
    "        plt.axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "difficult-hampton",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(save_dir/'STA1_5.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordered-recognition",
   "metadata": {},
   "source": [
    "# Linear Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "processed-innocent",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from ray.util.joblib import register_ray\n",
    "from sklearn import linear_model as lm # MultiTaskLassoCV, RidgeCV, MultiTaskElasticNetCV, LinearRegression\n",
    "from sklearn import svm\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooperative-swaziland",
   "metadata": {},
   "source": [
    "## Regression on Movement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cellular-shanghai",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = train_th #np.stack((train_th, train_phi),axis=1) # StandardScaler().fit_transform() # train_vid.reshape(train_vid.shape[0],-1)#[:,10:11] # np.stack((train_roll, train_pitch),axis=1) # \n",
    "Y_test = test_th #np.stack((test_th, test_phi),axis=1) # StandardScaler().fit_transform() # test_vid.reshape(test_vid.shape[0],-1)#[:,10:11] # np.stack((test_roll, test_pitch),axis=1) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detailed-anaheim",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'ridgecv'\n",
    "if model_type == 'elasticnetcv':\n",
    "    model = make_pipeline(StandardScaler(), lm.ElasticNetCV()) # lm.RidgeCV(alphas=np.arange(100,10000,1000))) #  #MultiOutputRegressor(lm.Ridge(),n_jobs=-1)) \n",
    "elif model_type == 'ridgecv':\n",
    "    model = make_pipeline(StandardScaler(), lm.RidgeCV(alphas=np.arange(100,10000,1000)))\n",
    "\n",
    "model.fit(train_nsp, Y_train)\n",
    "pred_train = model.predict(train_nsp)\n",
    "pred_test = model.predict(test_nsp)\n",
    "train_score = model.score(train_nsp,Y_train)\n",
    "test_score = model.score(test_nsp, Y_test)\n",
    "print('Train Score:', train_score, 'Test Score:', test_score)\n",
    "# print(model['model_type'].coef_[22])\n",
    "\n",
    "##### Flip test and train #####\n",
    "# model2 = make_pipeline(StandardScaler(), lm.RidgeCV())\n",
    "# model2.fit(test_nsp, Y_test)\n",
    "# pred_train = model2.predict(test_nsp)\n",
    "# pred_test = model2.predict(train_nsp)\n",
    "# train_score = model2.score(test_nsp,Y_test)\n",
    "# test_score = model2.score(train_nsp, Y_train)\n",
    "# print('Train Score:', train_score, 'Test Score:', test_score)\n",
    "# print(model2['ridgecv'].coef_[22])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "featured-italic",
   "metadata": {},
   "outputs": [],
   "source": [
    "model[model_type].alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metallic-correlation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flush-interest",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blind-fireplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 0\n",
    "dt = 10000\n",
    "plt.plot(np.arange(t,t+dt),Y_train[t:t+dt])\n",
    "plt.plot(np.arange(t,t+dt), pred_train[t:t+dt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rotary-proceeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 100\n",
    "dt = 100\n",
    "fig, axs = plt.subplots(1,figsize=(7,5))\n",
    "cc = np.corrcoef(Y_test,pred_test)[0,1]\n",
    "axs.plot(np.arange(t,t+dt)*model_dt,Y_test[t:t+dt], 'k', label='Ground Truth')\n",
    "axs.plot(np.arange(t,t+dt)*model_dt,pred_test[t:t+dt], 'r', label='Prediction')\n",
    "axs.set_title('CorrCoeff: {:.02f}'.format(cc))\n",
    "axs.set_xlabel('Time (s)')\n",
    "# axs.set_ylabel('Eye Phi Angle')\n",
    "axs.legend()\n",
    "plt.tight_layout()\n",
    "# fig.savefig(FigPath/'LinearRegressionExample_phi.png',bbox_inches='tight',transparent=False, facecolor='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unauthorized-thirty",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(Y_train,pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "derived-violin",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(Y_test,pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "european-kitty",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Y_test,pred_test, alpha=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "together-stress",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(model['elasticnetcv'].coef_)\n",
    "plt.plot(model['ridgecv'].coef_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "settled-machinery",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test.shape,Y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pointed-senior",
   "metadata": {},
   "source": [
    "# Autocorrelation of the th, vid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "found-novelty",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import uniform_filter1d "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enabling-disease",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(test_th, test_nsp[:,22])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stock-terminal",
   "metadata": {},
   "outputs": [],
   "source": [
    "xcorr_data = plt.xcorr(test_th, test_nsp[:,22], maxlags=100)\n",
    "lags, xscore = xcorr_data[0], xcorr_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "material-michigan",
   "metadata": {},
   "outputs": [],
   "source": [
    "lags[np.argmax(xscore)], xscore[np.argmax(xscore)],lags[np.argmin(xscore)], xscore[np.argmin(xscore)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sitting-wilson",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.acorr(train_th, maxlags=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "molecular-brown",
   "metadata": {},
   "source": [
    "## Regression on Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "great-vocabulary",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vid, test_vid, train_nsp, test_nsp, train_th, test_th, train_phi, test_phi, train_roll, test_roll, train_pitch, test_pitch, train_t, test_t, train_dth, test_dth, train_dphi, test_dphi = \\\n",
    "train_test_split(model_vid_sm, model_nsp, model_th, model_phi, model_roll, model_pitch, model_t, model_dth, model_dphi, train_size=.6, shuffle=False, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complimentary-disorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = train_vid.reshape(train_vid.shape[0],-1)#[:,10:11] # np.stack((train_roll, train_pitch),axis=1) # \n",
    "Y_test = test_vid.reshape(test_vid.shape[0],-1)#[:,10:11] # np.stack((test_roll, test_pitch),axis=1) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behavioral-firmware",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def multi_regression(train_nsp,Y_train,test_nsp,Y_test,idx,model_type):\n",
    "    if model_type == 'elasticnetcv':\n",
    "        model = make_pipeline(StandardScaler(), lm.ElasticNetCV()) # lm.RidgeCV(alphas=np.arange(100,10000,1000))) #  #MultiOutputRegressor(lm.Ridge(),n_jobs=-1)) \n",
    "    elif model_type == 'ridgecv':\n",
    "        model = make_pipeline(StandardScaler(), lm.RidgeCV(alphas=np.arange(100,10000,1000)))\n",
    "        \n",
    "    # MultiTaskElasticNetCV(n_jobs=-1)) # RidgeCV()# MultiTaskLassoCV(n_jobs=-1) # RidgeCV() # LinearRegression(n_jobs=-1) #\n",
    "    # register_ray()\n",
    "    # with joblib.parallel_backend('ray'):\n",
    "    model.fit(train_nsp, Y_train[:,idx])\n",
    "    pred_train = model.predict(train_nsp)\n",
    "    pred_test = model.predict(test_nsp)\n",
    "    model_coeff = model[model_type].coef_\n",
    "#     print('Train Score:', model.score(train_nsp,Y_train), 'Test Score:', model.score(test_nsp, Y_test))\n",
    "    train_score = np.corrcoef(pred_train,Y_train[:,idx])[0,1]\n",
    "    test_score = np.corrcoef(pred_test, Y_test[:,idx])[0,1]\n",
    "    alphas = model[model_type].alpha_\n",
    "    return pred_train, pred_test, train_score, test_score, model_coeff, alphas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fancy-advisory",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'elasticnetcv'\n",
    "\n",
    "start = time.time()\n",
    "train_nsp_r = ray.put(train_nsp)\n",
    "Y_train_r = ray.put(Y_train)\n",
    "test_nsp_r = ray.put(test_nsp)\n",
    "Y_test_r = ray.put(Y_test)\n",
    "result_ids = []\n",
    "[result_ids.append(multi_regression.remote(train_nsp_r,Y_train_r,test_nsp_r,Y_test_r,idx,model_type)) for idx in range(0, train_vid.shape[-1]*train_vid.shape[-2])]\n",
    "results_p = ray.get(result_ids)\n",
    "print('MultiReg Time: ', time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sorted-cooling",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = np.stack([results_p[i][0] for i in range(len(results_p))])\n",
    "pred_test = np.stack([results_p[i][1] for i in range(len(results_p))])\n",
    "train_scores = np.array([results_p[i][2] for i in range(len(results_p))])\n",
    "test_scores = np.array([results_p[i][3] for i in range(len(results_p))])\n",
    "model_coeff = np.array([results_p[i][4] for i in range(len(results_p))])\n",
    "alphas = np.array([results_p[i][5] for i in range(len(results_p))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "connected-screen",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = pred_train.T.reshape(pred_train.shape[-1],train_vid.shape[1],train_vid.shape[2])\n",
    "pred_test = pred_test.T.reshape(pred_test.shape[-1],test_vid.shape[1],test_vid.shape[2])\n",
    "model_coeff = model_coeff.T.reshape(train_nsp.shape[-1],train_vid.shape[1],train_vid.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experienced-subdivision",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ElasticNet_data = {\n",
    "#                 'train_scores': train_scores,\n",
    "#                 'test_scores': test_scores,\n",
    "#                 'pred_train': pred_train,\n",
    "#                 'pred_test': pred_test, }\n",
    "# ioh5.save(save_dir/'ElasticNet_data.h5',ElasticNet_data)\n",
    "\n",
    "# Ridge_data = ioh5.load(save_dir/'RidgeData.h5')\n",
    "# locals().update(ElasticNet_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brutal-provision",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotly.express as px\n",
    "# t = 200\n",
    "# dt = 500\n",
    "# comb = np.concatenate((pred_train[t:t+dt,np.newaxis,:,:], train_vid[t:t+dt,np.newaxis,:,:]),axis=1)\n",
    "\n",
    "# fig = px.imshow(comb, animation_frame=0, facet_col=1, binary_string=False)\n",
    "# fig.update_layout(width=1000,\n",
    "#                   height=500,\n",
    "#                  )\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beautiful-purple",
   "metadata": {},
   "source": [
    "Need to look at decoding weights and see if they resempble receptive fields?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olympic-volleyball",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interpreted-province",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "african-optimum",
   "metadata": {},
   "source": [
    "## Plotting Decoded Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "speaking-person",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torchvision\n",
    "from scipy.ndimage import uniform_filter1d "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geological-filing",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 2000 #2000\n",
    "dt = 50\n",
    "im_grid = torchvision.utils.make_grid(torch.from_numpy(pred_test[t:t+dt,np.newaxis,:,:]),nrow=10,normalize=False)[0]\n",
    "im_grid2 = torchvision.utils.make_grid(torch.from_numpy(test_vid[t:t+dt,np.newaxis,:,:]),nrow=10,normalize=False)[0]\n",
    "fig, axs = plt.subplots(2,1,figsize=(20,10))\n",
    "axs[0].imshow(im_grid, cmap='gray')#.permute(1,2,0))\n",
    "axs[0].set_title('Decoding Prediction')\n",
    "axs[1].imshow(im_grid2, cmap='gray')#.permute(1,2,0))\n",
    "axs[1].set_title('Actual Frame')\n",
    "plt.tight_layout()\n",
    "fig.savefig(FigPath/'DecodedMontage_{}.png'.format(model_type),bbox_inches='tight',transparent=False, facecolor='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "korean-davis",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_grid = torchvision.utils.make_grid(torch.from_numpy(model_coeff[:,np.newaxis]),nrow=10,normalize=False)[0]\n",
    "fig, axs = plt.subplots(1,1,figsize=(10,10))\n",
    "axs.imshow(im_grid, cmap='gray')#.permute(1,2,0))\n",
    "axs.set_title('Decoding Coeff')\n",
    "plt.tight_layout()\n",
    "fig.savefig(FigPath/'DecodingWeights_{}.png'.format(model_type),bbox_inches='tight',transparent=False, facecolor='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deadly-psychiatry",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf = 2\n",
    "pred_test_norm = normimgs(pred_test)\n",
    "pred_test_up = np.zeros((pred_test.shape[0],sf*pred_test.shape[1],sf*pred_test.shape[2]))\n",
    "test_vid_norm = normimgs(test_vid)\n",
    "test_vid_up = np.zeros((test_vid.shape[0],sf*test_vid.shape[1],sf*test_vid.shape[2]))\n",
    "pred_train_norm = normimgs(pred_train)\n",
    "pred_train_up = np.zeros((pred_train.shape[0],sf*pred_train.shape[1],sf*pred_train.shape[2]))\n",
    "train_vid_norm = normimgs(train_vid)\n",
    "train_vid_up = np.zeros((train_vid.shape[0],sf*train_vid.shape[1],sf*train_vid.shape[2]))\n",
    "for n in range(pred_test.shape[0]):\n",
    "    pred_test_up[n] = cv2.resize(pred_test_norm[n],(sf*pred_test.shape[2],sf*pred_test.shape[1]))\n",
    "    test_vid_up[n] = cv2.resize(test_vid_norm[n],(sf*test_vid.shape[2],sf*test_vid.shape[1]))\n",
    "    pred_train_up[n] = cv2.resize(pred_train_norm[n],(sf*pred_train.shape[2],sf*pred_train.shape[1]))\n",
    "    train_vid_up[n] = cv2.resize(train_vid_norm[n],(sf*train_vid.shape[2],sf*train_vid.shape[1]))\n",
    "\n",
    "cond = 'test'\n",
    "if cond == 'train':\n",
    "    tot_samps = np.stack((pred_train_up, train_vid_up))\n",
    "else:\n",
    "    tot_samps = np.stack((pred_test_up, test_vid_up))\n",
    "tot_samps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "studied-cholesterol",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example Frames Video\n",
    "# t = 0\n",
    "# dt = pred_test.shape[0]\n",
    "# # comb = np.concatenate((normimgs(pred_test),normimgs(test_vid)),axis=2)\n",
    "# comb = np.concatenate((pred_test_up,test_vid_up),axis=2).astype(np.uint8)\n",
    "# # comb = (comb - np.min(comb,axis=(-1,-2))[:,np.newaxis,np.newaxis])/(np.max(comb,axis=(-1,-2))-np.min(comb,axis=(-1,-2)))[:,np.newaxis,np.newaxis]\n",
    "# # comb = (comb*255).astype(np.uint8)\n",
    "\n",
    "# FPS = 10\n",
    "# out = cv2.VideoWriter(os.path.join(FigPath,'Frames_ExVid.avi'), cv2.VideoWriter_fourcc('M', 'J', 'P', 'G'), FPS, (comb.shape[-1], comb.shape[-2]),0)\n",
    "            \n",
    "# for fm in tqdm(range(comb.shape[0])):\n",
    "#     out.write(comb[fm])\n",
    "# out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "different-latex",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###### Grab data of longest continuous sequence ######\n",
    "# def func1(a,b):\n",
    "#     # \"Enclose\" mask with sentients to catch shifts later on\n",
    "#     mask = np.r_[False,a,False]\n",
    "\n",
    "#     # Get the shifting indices\n",
    "#     idx = np.flatnonzero(mask[1:] != mask[:-1])\n",
    "\n",
    "#     s0,s1 = idx[::2], idx[1::2]\n",
    "#     idx_b = np.r_[0,(s1-s0).cumsum()]\n",
    "#     out = []\n",
    "#     for (i,j,k,l) in zip(s0,s1-1,idx_b[:-1],idx_b[1:]):\n",
    "#         out.append(((i, j), b[k:l]))\n",
    "#     return out\n",
    "\n",
    "# train_idxs,test_idxs = train_test_split(good_idxs,train_size=.6,random_state=0)\n",
    "\n",
    "# out = func1(test_idxs,np.arange(test_idxs.shape[0]))\n",
    "\n",
    "# max_seqn = 0\n",
    "# for n in range(len(out)):\n",
    "#     if len(out[n][1]) > max_seqn:\n",
    "#         max_seq = np.arange(out[n][0][0],out[n][0][1])\n",
    "#         max_seqn = len(out[n][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opening-joshua",
   "metadata": {},
   "outputs": [],
   "source": [
    "win_size = 3\n",
    "tot_samps2 = uniform_filter1d(tot_samps,win_size,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detected-estimate",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 500\n",
    "dt = 100\n",
    "plt.plot(tot_samps2[0,t:t+dt,5,10])\n",
    "plt.plot(tot_samps[1,t:t+dt,5,10])\n",
    "plt.plot(tot_samps2[1,t:t+dt,5,10])\n",
    "plt.legend(['Pred','Actual','Actual_smoothed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desperate-entertainment",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.animation import FuncAnimation, PillowWriter, FFMpegWriter\n",
    "from matplotlib import colors\n",
    "def init():\n",
    "    for n in range(2):\n",
    "        axs[n].axis('off')\n",
    "    plt.tight_layout()\n",
    "\n",
    "def update(t):\n",
    "    for n in range(2):\n",
    "        ims[n].set_data(tot_samps2[n,t])\n",
    "    plt.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thirty-specification",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t = 0# max_seq[0]\n",
    "lat_dims = 2\n",
    "x,y = [],[]\n",
    "fig, axs = plt.subplots(1,2,figsize=(8,4))   #8,16,figsize=(50,30)  \n",
    "axs = axs.flatten()\n",
    "ims = []\n",
    "titles = ['Pred','Actual']\n",
    "for n in range(2):\n",
    "    ims.append(axs[n].imshow(tot_samps2[n,t],cmap='gray',norm=colors.Normalize()))\n",
    "    axs[n].axis('off')\n",
    "    axs[n].set_title('{}'.format(titles[n]))\n",
    "plt.tight_layout()\n",
    "# fig.savefig(os.path.join(FigurePath,'testimg.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "destroyed-project",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# writervideo = PillowWriter(fps=60)  \n",
    "ani = FuncAnimation(fig, update, tqdm(range(tot_samps2.shape[1])), init_func=init)  #range(tot_samps.shape[1])\n",
    "plt.show()\n",
    "vpath = check_path(FigPath,'version_{:d}'.format(0))\n",
    "vname =  'DecodedVideo_{}_upsampled{:d}_smoothed{:d}_{}.mp4'.format(model_type,sf, win_size,cond)\n",
    "writervideo = FFMpegWriter(fps=10) \n",
    "ani.save(os.path.join(vpath,vname), writer=writervideo)\n",
    "print('DONE!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "toxic-entrepreneur",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores = train_scores.reshape((train_vid.shape[-2],train_vid.shape[-1]))\n",
    "test_scores = test_scores.reshape((test_vid.shape[-2],test_vid.shape[-1]))\n",
    "fig, axs = plt.subplots(2,1,figsize=(10,10))\n",
    "im1 = axs[0].imshow(train_scores, vmin=0, vmax=.55)\n",
    "axs[0].set_title('Train Correlation Map')\n",
    "add_colorbar(im1)\n",
    "im2 = axs[1].imshow(test_scores, vmin=0, vmax=.55)\n",
    "axs[1].set_title('Test Correlation Map')\n",
    "add_colorbar(im2)\n",
    "plt.tight_layout()\n",
    "fig.savefig(FigPath/'DecodingScores_{}.png'.format(model_type),bbox_inches='tight',transparent=False, facecolor='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manual-religious",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 2000\n",
    "dt = 20\n",
    "comb = np.concatenate((np.concatenate((pred_test[t:t+dt,:,:], test_vid[t:t+dt,:,:]),axis=1)),axis=1)\n",
    "fig, ax = plt.subplots(1,figsize=(25,20))\n",
    "ax.imshow(comb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temporal-dover",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "christian-singles",
   "metadata": {},
   "source": [
    "# Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "static-rugby",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_roll = train_roll/np.max(train_roll)\n",
    "# train_roll -=train_roll[0]\n",
    "# train_pitch = train_pitch/np.max(train_pitch)\n",
    "# train_pitch -=train_pitch[0]\n",
    "# test_roll = test_roll/np.max(test_roll)\n",
    "# test_roll -=test_roll[0]\n",
    "# test_pitch = test_pitch/np.max(test_pitch)\n",
    "# test_pitch -=test_pitch[0]\n",
    "\n",
    "# Y_train = torch.from_numpy(np.stack((train_roll, train_pitch),axis=1)).float()\n",
    "# Y_test  = torch.from_numpy(np.stack((test_roll, test_pitch),axis=1)).float()\n",
    "\n",
    "Y_train = torch.from_numpy(train_roll[:,np.newaxis]).float() #train_vid.reshape(train_vid.shape[0],-1)).float()#[:,10:11] # np.stack((train_roll, train_pitch),axis=1) # \n",
    "Y_test = torch.from_numpy(test_roll[:,np.newaxis]).float() #test_vid.reshape(test_vid.shape[0],-1)).float()#[:,10:11] # np.stack((test_roll, test_pitch),axis=1) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charming-luxury",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecodingDataset(Dataset):\n",
    "    def __init__(self, data, output, N_fm, transform=None):\n",
    "        \n",
    "        self.data = data\n",
    "        self.output = output\n",
    "        self.transform = transform\n",
    "        self.N_fm = N_fm\n",
    "\n",
    "    def __len__(self):\n",
    "        return(self.data.shape[0])\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        if idx < self.N_fm:\n",
    "            idx = self.N_fm\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        sample = torch.from_numpy(self.data[idx-self.N_fm:idx]).float()\n",
    "        gt = torch.from_numpy(self.output[idx-self.N_fm:idx,:]).float()\n",
    "        return sample.view(-1), gt.view(-1)\n",
    "    \n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print            \n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "    \n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "skilled-component",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_fm=1\n",
    "batch_size = 1024\n",
    "in_neurons = train_nsp.shape[1]*N_fm\n",
    "out_neurons = 2048\n",
    "out_dims = Y_train.shape[-1]*N_fm\n",
    "NEpochs = 500\n",
    "# train_dataset = DecodingDataset(train_nsp, np.stack((train_roll, train_pitch),axis=1), N_fm=N_fm)\n",
    "# test_dataset = DecodingDataset(test_nsp, np.stack((test_roll, test_pitch),axis=1), N_fm=N_fm)\n",
    "train_dataset = TensorDataset(torch.from_numpy(train_nsp).float(),Y_train)\n",
    "test_dataset  = TensorDataset(torch.from_numpy(test_nsp).float(),Y_test)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, drop_last=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nutritional-budapest",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(in_neurons,out_neurons),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(out_neurons,out_neurons),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(out_neurons,out_dims)).to(device)\n",
    "optimizer = optim.AdamW(params=model.parameters(), lr=.0001)\n",
    "criteria = nn.MSELoss()\n",
    "early_stopping = EarlyStopping(path=save_dir/'checkpoint.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legal-staff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_loss = []\n",
    "test_tot_loss = []\n",
    "for epoch in tqdm(range(NEpochs)):\n",
    "    epoch_loss = []\n",
    "    for batch, y in train_dataloader:\n",
    "        pred = model(batch.to(device))\n",
    "        loss = criteria(pred.to(device),y.to(device))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss.append(loss.item())\n",
    "    tot_loss.append(np.mean(epoch_loss))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        test_epoch_loss = []\n",
    "        for batch, y in test_dataloader:\n",
    "            pred = model(batch.to(device))\n",
    "            loss = criteria(pred.to(device),y.to(device))\n",
    "            test_epoch_loss.append(loss.item())\n",
    "        test_tot_loss.append(np.mean(test_epoch_loss))\n",
    "    early_stopping(np.mean(test_epoch_loss), model)\n",
    "    if early_stopping.early_stop == True:\n",
    "        print('Stopped Early!')\n",
    "        break\n",
    "    print('Epoch:', epoch, 'Epoch_Loss_Avg: ', np.mean(epoch_loss), 'Test_Epoch_Loss_Avg: ', np.mean(test_epoch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "planned-editor",
   "metadata": {},
   "outputs": [],
   "source": [
    "wind = np.arange(0,1000)\n",
    "fig, ax = plt.subplots(2,1,figsize=(20,10))\n",
    "ax[0].plot(Y_train[wind,0],'b-', label='roll')\n",
    "ax[0].plot(pred[wind,0].cpu().detach(),'r-', label='pred_roll')\n",
    "ax[1].plot(Y_train[wind,1],'b-', label='pitch')\n",
    "ax[1].plot(pred[wind,1].cpu().detach(),'r-', label='pred_pitch')\n",
    "ax[0].legend()\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "large-arena",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    predt = []\n",
    "    for batch, y in test_dataloader:\n",
    "        pred = model(batch.to(device))\n",
    "        predt.append(pred.cpu().numpy())\n",
    "    predt = np.concatenate(predt,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "responsible-deputy",
   "metadata": {},
   "outputs": [],
   "source": [
    "wind = np.arange(0,1000)\n",
    "fig, ax = plt.subplots(2,1,figsize=(20,10))\n",
    "ax[0].plot(Y_test[wind,0],'b-', label='roll')\n",
    "ax[0].plot(predt[wind,0],'r-', label='pred_roll')\n",
    "ax[1].plot(Y_test[wind,1],'b-', label='pitch')\n",
    "ax[1].plot(predt[wind,1],'r-', label='pred_pitch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signal-strike",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endangered-columbia",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

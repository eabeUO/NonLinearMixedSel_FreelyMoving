{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "absent-defendant",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sufficient-looking",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import glob\n",
    "import sys \n",
    "import yaml \n",
    "import glob\n",
    "import h5py \n",
    "import ray\n",
    "import logging \n",
    "import json\n",
    "import gc\n",
    "import cv2\n",
    "import time\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "# import io_dict_to_hdf5 as ioh5\n",
    "import xarray as xr\n",
    "import scipy.linalg as linalg\n",
    "import scipy.sparse as sparse\n",
    "\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from scipy import signal\n",
    "from pathlib import Path\n",
    "from scipy.optimize import minimize_scalar\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.ndimage import shift as imshift\n",
    "from sklearn.model_selection import train_test_split, GroupShuffleSplit\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import linear_model as lm \n",
    "from scipy.stats import binned_statistic\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import r2_score, mean_poisson_deviance\n",
    "from pyglmnet import GLMCV\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import torch.nn.functional as F\n",
    "# from torchvision import transforms\n",
    "# from torch.utils.data import Dataset, DataLoader, TensorDataset, Subset\n",
    "# torch.backends.cudnn.benchmark = True\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "sys.path.append(str(Path('.').absolute().parent))\n",
    "from utils import *\n",
    "import io_dict_to_hdf5 as ioh5\n",
    "from format_data import load_ephys_data_aligned\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "FigPath = check_path(Path('~/Research/SensoryMotorPred_Data').expanduser(),'Figures/Encoding')\n",
    "\n",
    "ray.init(\n",
    "    ignore_reinit_error=True,\n",
    "    logging_level=logging.ERROR,\n",
    ")\n",
    "print(f'Dashboard URL: http://{ray.get_dashboard_url()}')\n",
    "print('Dashboard URL: http://localhost:{}'.format(ray.get_dashboard_url().split(':')[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "critical-concentration",
   "metadata": {},
   "source": [
    "# Gather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nearby-teaching",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_test(file_dict, save_dir, model_dt=.1, frac=.1, train_size=.7, do_shuffle=False, do_norm=False, free_move=True, has_imu=True, has_mouse=False,):\n",
    "    ##### Load in preprocessed data #####\n",
    "    data = load_ephys_data_aligned(file_dict, save_dir, model_dt=model_dt, free_move=free_move, has_imu=has_imu, has_mouse=has_mouse,)\n",
    "    if free_move:\n",
    "        ##### Find 'good' timepoints when mouse is active #####\n",
    "        nan_idxs = []\n",
    "        for key in data.keys():\n",
    "            nan_idxs.append(np.where(np.isnan(data[key]))[0])\n",
    "        good_idxs = np.ones(len(data['model_active']),dtype=bool)\n",
    "        good_idxs[data['model_active']<.5] = False\n",
    "        good_idxs[np.unique(np.hstack(nan_idxs))] = False\n",
    "    else:\n",
    "        good_idxs = np.where((np.abs(data['model_th'])<10) & (np.abs(data['model_phi'])<10))[0]\n",
    "    \n",
    "    data['raw_nsp'] = data['model_nsp'].copy()\n",
    "    ##### return only active data #####\n",
    "    for key in data.keys():\n",
    "        if (key != 'model_nsp') & (key != 'model_active') & (key != 'unit_nums'):\n",
    "            data[key] = data[key][good_idxs] # interp_nans(data[key]).astype(float)\n",
    "        elif (key == 'model_nsp'):\n",
    "            data[key] = data[key][good_idxs]\n",
    "        elif (key == 'unit_nums'):\n",
    "            pass\n",
    "    gss = GroupShuffleSplit(n_splits=1, train_size=train_size, random_state=42)\n",
    "    nT = data['model_nsp'].shape[0]\n",
    "    groups = np.hstack([i*np.ones(int((frac*i)*nT) - int((frac*(i-1))*nT)) for i in range(1,int(1/frac)+1)])\n",
    "\n",
    "    for train_idx, test_idx in gss.split(np.arange(len(data['model_nsp'])), groups=groups):\n",
    "        print(\"TRAIN:\", len(train_idx), \"TEST:\", len(test_idx))\n",
    "\n",
    "\n",
    "    data['model_dth'] = np.diff(data['model_th'],append=0)\n",
    "    data['model_dphi'] = np.diff(data['model_phi'],append=0)\n",
    "\n",
    "    data['model_vid_sm'] = (data['model_vid_sm'] - np.mean(data['model_vid_sm'],axis=0))/np.nanstd(data['model_vid_sm'],axis=0)\n",
    "    data['model_vid_sm'][np.isnan(data['model_vid_sm'])]=0\n",
    "    if do_norm:\n",
    "        data['model_th'] = (data['model_th'] - np.mean(data['model_th'],axis=0))/np.std(data['model_th'],axis=0) \n",
    "        data['model_phi'] = (data['model_phi'] - np.mean(data['model_phi'],axis=0))/np.std(data['model_phi'],axis=0) \n",
    "        if free_move:\n",
    "            data['model_roll'] = (data['model_roll'] - np.mean(data['model_roll'],axis=0))/np.std(data['model_roll'],axis=0) \n",
    "            data['model_pitch'] = (data['model_pitch'] - np.mean(data['model_pitch'],axis=0))/np.std(data['model_pitch'],axis=0) \n",
    "\n",
    "    ##### Split Data by train/test #####\n",
    "    data_train_test = {\n",
    "        'train_vid': data['model_vid_sm'][train_idx],\n",
    "        'test_vid': data['model_vid_sm'][test_idx],\n",
    "        'train_nsp': shuffle(data['model_nsp'][train_idx],random_state=42) if do_shuffle else data['model_nsp'][train_idx],\n",
    "        'test_nsp': shuffle(data['model_nsp'][test_idx],random_state=42) if do_shuffle else data['model_nsp'][test_idx],\n",
    "        'train_th': data['model_th'][train_idx],\n",
    "        'test_th': data['model_th'][test_idx],\n",
    "        'train_phi': data['model_phi'][train_idx],\n",
    "        'test_phi': data['model_phi'][test_idx],\n",
    "        'train_roll': data['model_roll'][train_idx] if free_move else [],\n",
    "        'test_roll': data['model_roll'][test_idx] if free_move else [],\n",
    "        'train_pitch': data['model_pitch'][train_idx] if free_move else [],\n",
    "        'test_pitch': data['model_pitch'][test_idx] if free_move else [],\n",
    "        'train_t': data['model_t'][train_idx],\n",
    "        'test_t': data['model_t'][test_idx],\n",
    "        'train_dth': data['model_dth'][train_idx],\n",
    "        'test_dth': data['model_dth'][test_idx],\n",
    "        'train_dphi': data['model_dphi'][train_idx],\n",
    "        'test_dphi': data['model_dphi'][test_idx],\n",
    "        'train_gz': data['model_gz'][train_idx] if free_move else [],\n",
    "        'test_gz': data['model_gz'][test_idx] if free_move else [],\n",
    "    }\n",
    "\n",
    "    d1 = data\n",
    "    d1.update(data_train_test)\n",
    "    return d1,train_idx,test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educational-impossible",
   "metadata": {},
   "outputs": [],
   "source": [
    "free_move = True\n",
    "if free_move:\n",
    "    stim_type = 'fm1'\n",
    "else:\n",
    "    stim_type = 'hf1_wn' # 'fm1' # \n",
    "\n",
    "data_dir  = Path('~/Goeppert/freely_moving_ephys/ephys_recordings/070921/J553RT').expanduser() / stim_type\n",
    "save_dir  = check_path(Path('~/Research/SensoryMotorPred_Data/data/070921/J553RT/').expanduser(), stim_type)\n",
    "FigPath = check_path(FigPath, stim_type)\n",
    "save_dir,data_dir,FigPath\n",
    "# with open(save_dir / 'file_dict.json','r') as fp:\n",
    "#     file_dict = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "economic-ranking",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dict = {'cell': 0,\n",
    " 'drop_slow_frames': True,\n",
    " 'ephys': list(data_dir.glob('*ephys_merge.json'))[0].as_posix(),\n",
    " 'ephys_bin': list(data_dir.glob('*Ephys.bin'))[0].as_posix(),\n",
    " 'eye': list(data_dir.glob('*REYE.nc'))[0].as_posix(),\n",
    " 'imu': list(data_dir.glob('*imu.nc'))[0].as_posix() if stim_type=='fm1' else None,\n",
    " 'mapping_json': '/home/seuss/Research/Github/FreelyMovingEphys/probes/channel_maps.json',\n",
    " 'mp4': True,\n",
    " 'name': '070921_J553RT_control_Rig2_'+stim_type,\n",
    " 'probe_name': 'DB_P128-6',\n",
    " 'save': data_dir.as_posix(),\n",
    " 'speed': list(data_dir.glob('*speed.nc'))[0].as_posix() if stim_type=='hf1_wn' else None,\n",
    " 'stim_type': 'light',\n",
    " 'top': list(data_dir.glob('*TOP1.nc'))[0].as_posix() if stim_type=='fm1' else None,\n",
    " 'world': list(data_dir.glob('*world.nc'))[0].as_posix(),}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "square-chapel",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dt = .05\n",
    "do_shuffle=False\n",
    "do_norm = False\n",
    "data,train_idx,test_idx = load_train_test(file_dict, save_dir, model_dt=model_dt, do_shuffle=do_shuffle, do_norm=do_norm,free_move=free_move, has_imu=free_move, has_mouse=False)\n",
    "locals().update(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enclosed-advocate",
   "metadata": {},
   "source": [
    "# Testing Tuning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acceptable-humanitarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Tuning curve for theta\n",
    "def tuning_curve(model_nsp, var, model_dt = .025, N_bins=10, Nstds=3):\n",
    "    var_range = np.linspace(np.nanmean(var)-Nstds*np.nanstd(var), np.nanmean(var)+Nstds*np.nanstd(var),N_bins)\n",
    "    tuning = np.zeros((model_nsp.shape[-1],len(var_range)-1))\n",
    "    tuning_std = np.zeros((model_nsp.shape[-1],len(var_range)-1))\n",
    "    for n in range(model_nsp.shape[-1]):\n",
    "        for j in range(len(var_range)-1):\n",
    "            usePts = (var>=var_range[j]) & (var<var_range[j+1])\n",
    "            tuning[n,j] = np.nanmean(model_nsp[usePts,n])/model_dt\n",
    "            tuning_std[n,j] = (np.nanstd(model_nsp[usePts,n])/model_dt)/ np.sqrt(np.count_nonzero(usePts))\n",
    "    return tuning, tuning_std, var_range[:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grateful-aggregate",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning, tuning_std, var_range = tuning_curve(test_nsp, test_pitch, N_bins=10, model_dt=model_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ready-thomson",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 51\n",
    "fig, axs = plt.subplots(1,figsize=(7,5))\n",
    "axs.errorbar(var_range-var_range.mean(),tuning[n], yerr=tuning_std[n])\n",
    "axs.set_ylim(bottom=0)\n",
    "axs.set_xlabel('Eye Phi')\n",
    "axs.set_ylabel('Spikes/s')\n",
    "axs.set_title('Neuron: {} 3stds'.format(n))\n",
    "plt.tight_layout()\n",
    "# fig.savefig(FigPath/'ExampleTuningCurve.png',bbox_inches='tight',transparent=False, facecolor='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharing-pressing",
   "metadata": {},
   "source": [
    "# Testing Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transparent-jackson",
   "metadata": {},
   "outputs": [],
   "source": [
    "dth = np.diff(model_th,append=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "objective-desktop",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,3,figsize=(25,8))\n",
    "\n",
    "# dth vs gz\n",
    "ax=axs[0]\n",
    "ax.scatter(dth, model_gz, s=5, alpha=.5)\n",
    "ax.set_title(r'$\\Delta\\theta$ vs. $g_z$')\n",
    "ax.set_xlabel(r'$\\Delta\\theta$')\n",
    "ax.set_ylabel(r'$g_z$')\n",
    "# ax.axis('equal')\n",
    "\n",
    "ax=axs[1]\n",
    "ax.scatter(model_pitch, model_th, s=5, alpha=.5)\n",
    "ax.set_title(r'Pitch vs. $\\theta$')\n",
    "ax.set_xlabel(r'Pitch')\n",
    "ax.set_ylabel(r'$\\theta$')\n",
    "ax.axis('equal')\n",
    "ax=axs[2]\n",
    "ax.scatter(model_roll, model_phi, s=5, alpha=.5)\n",
    "ax.set_title(r'Roll vs. $\\phi$')\n",
    "ax.set_xlabel(r'Roll')\n",
    "ax.set_ylabel(r'$\\phi$')\n",
    "ax.axis('equal')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caring-arrest",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "portable-bottom",
   "metadata": {},
   "source": [
    "# Parallel Processing GLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laden-webster",
   "metadata": {},
   "source": [
    "## Vis Only sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marine-profile",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def do_glm_fit_vis_skl(train_nsp, test_nsp, x_train, x_test, celln, model_type, lag_list, bin_length=40, model_dt=.1):\n",
    "    \n",
    "    ##### Format data #####\n",
    "    nt_glm_lag = len(lag_list)\n",
    "    \n",
    "    # Shift spikes by -lag for GLM fits\n",
    "    sps_train = train_nsp[:,celln] # np.roll(train_nsp[:,celln],-lag)\n",
    "    sps_test = test_nsp[:,celln] # np.roll(test_nsp[:,celln],-lag)\n",
    "\n",
    "\n",
    "    if model_type == 'elasticnetcv':\n",
    "        model = lm.ElasticNetCV(l1_ratio=[.05, .01, .5, .7]) # lm.RidgeCV(alphas=np.arange(100,10000,1000))) #  #MultiOutputRegressor(lm.Ridge(),n_jobs=-1)) \n",
    "        model.fit(x_train,sps_train)\n",
    "        sta_all = np.reshape(model.coef_,(nt_glm_lag,)+nks)\n",
    "        sp_pred = model.predict(x_test)\n",
    "    elif model_type == 'ridgecv':\n",
    "        lambdas = 1024 * (2**np.arange(0,16))\n",
    "        model = lm.RidgeCV(alphas=lambdas)\n",
    "        model.fit(x_train,sps_train)\n",
    "        sta_all = np.reshape(model.coef_,(nt_glm_lag,)+nks)\n",
    "        sp_pred = model.predict(x_test)\n",
    "    else:\n",
    "    #     lambdas = 2048 * (2**np.arange(0,16))\n",
    "        lambdas = 2**np.arange(0,16)\n",
    "        nlam = len(lambdas)\n",
    "        # Initialze mse traces for regularization cross validation\n",
    "        msetrain = np.zeros((nlam,1))\n",
    "        msetest = np.zeros((nlam,1))\n",
    "        pred_all =np.zeros((x_test.shape[0],nlam)) \n",
    "        w_ridge = np.zeros((x_train.shape[-1],nlam))\n",
    "        w_intercept = np.zeros((nlam,1))\n",
    "        # loop over regularization strength\n",
    "        for l in range(len(lambdas)):\n",
    "            model = lm.PoissonRegressor(alpha=lambdas[l],max_iter=300)\n",
    "            # calculate MAP estimate               \n",
    "            model.fit(x_train,sps_train)\n",
    "            w_ridge[:,l] = model.coef_\n",
    "            w_intercept[l] = model.intercept_\n",
    "            pred_all[:,l] = model.predict(x_test)\n",
    "            # calculate test and training rms error\n",
    "            msetrain[l] = mean_poisson_deviance(sps_train,model.predict(x_train)) #np.mean((sps_train - model.predict(x_train))**2)\n",
    "            msetest[l] = mean_poisson_deviance(sps_test,pred_all[:,l]) # np.mean((sps_test - model.predict(x_test))**2)\n",
    "        # select best cross-validated lambda for RF\n",
    "        best_lambda = np.argmin(msetest)\n",
    "        w = w_ridge[:,best_lambda]\n",
    "        intercept= w_intercept[best_lambda]\n",
    "        ridge_rf = w_ridge[:,best_lambda]\n",
    "        sta_all = np.reshape(w,(nt_glm_lag,)+nks)\n",
    "        sp_pred = pred_all[:,best_lambda]\n",
    "    #     model = make_pipeline(StandardScaler(), lm.PoissonRegressor(alpha=lambdas[best_lambda]))\n",
    "    #     model.fit(x_train,sps_train)\n",
    "    # predicted firing rate\n",
    "    # bin the firing rate to get smooth rate vs time\n",
    "    sp_smooth = (np.convolve(sps_test, np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "    pred_smooth = (np.convolve(sp_pred, np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "    # a few diagnostics\n",
    "    err = np.mean((sp_smooth-pred_smooth)**2)\n",
    "    cc = np.corrcoef(sp_smooth[bin_length:-bin_length], pred_smooth[bin_length:-bin_length])\n",
    "    cc_all = cc[0,1]\n",
    "    r2_all = r2_score(sp_smooth,pred_smooth)\n",
    "    return cc_all, sta_all, sps_test, sp_pred, r2_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "challenging-header",
   "metadata": {},
   "outputs": [],
   "source": [
    "lag=150 # in ms\n",
    "nt_glm_lag = 5\n",
    "# minlag = int(-lag//(1000*model_dt)); maxlag=int((lag//(1000*model_dt))+1)\n",
    "lag_list = np.array([-1,0,1,2,3]) #,np.arange(minlag,maxlag,np.floor((maxlag-minlag)/nt_glm_lag).astype(int))\n",
    "nt_glm_lag = len(lag_list)\n",
    "print(lag_list,1000*lag_list*model_dt)\n",
    "do_shuffle=False\n",
    "model_type = 'poissonregressor'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "permanent-arrival",
   "metadata": {},
   "outputs": [],
   "source": [
    "for do_shuffle in [False,True]:\n",
    "    # Load Data\n",
    "    data, train_idx, test_idx = load_train_test(file_dict, save_dir, model_dt=model_dt, do_shuffle=do_shuffle, do_norm=True,free_move=free_move, has_imu=free_move, has_mouse=False)\n",
    "    locals().update(data)\n",
    "\n",
    "    ##### Start GLM Parallel Processing #####\n",
    "    start = time.time()\n",
    "    nks = np.shape(train_vid)[1:]; nk = nks[0]*nks[1]*nt_glm_lag\n",
    "    # Reshape data (video) into (T*n)xN array\n",
    "#     x_train = train_vid.reshape(train_vid.shape[0],-1)\n",
    "#     x_train = np.hstack([np.roll(x_train, nframes, axis=0) for nframes in lag_list]) # nt_glm_lag\n",
    "#     x_test = test_vid.reshape(test_vid.shape[0],-1) \n",
    "#     x_test = np.hstack([np.roll(x_test, nframes, axis=0) for nframes in lag_list])#\n",
    "    rolled_vid = np.hstack([np.roll(model_vid_sm, nframes, axis=0) for nframes in lag_list]) # nt_glm_lag\n",
    "    x_train = rolled_vid[train_idx].reshape(len(train_idx),-1)\n",
    "    x_test = rolled_vid[test_idx].reshape(len(test_idx),-1)\n",
    "    \n",
    "    # Put data into shared memory for parallization \n",
    "    train_nsp_r = ray.put(train_nsp)\n",
    "    test_nsp_r = ray.put(test_nsp)\n",
    "    train_data_r = ray.put(x_train)\n",
    "    test_data_r = ray.put(x_test)\n",
    "    result_ids = []\n",
    "    # Loop over parameters appending process ids\n",
    "    for celln in range(train_nsp.shape[1]):\n",
    "        result_ids.append(do_glm_fit_vis_skl.remote(train_nsp_r, test_nsp_r, train_data_r, test_data_r, celln, model_type, lag_list, model_dt=model_dt))\n",
    "\n",
    "    print('N_proc:', len(result_ids))\n",
    "    results_p = ray.get(result_ids)\n",
    "    print('GLM: ', time.time()-start)\n",
    "\n",
    "    ##### Gather Data and Find Max CC Model #####\n",
    "    mcc = np.stack([results_p[i][0] for i in range(len(results_p))])\n",
    "    msta = np.stack([results_p[i][1] for i in range(len(results_p))])\n",
    "    msp = np.stack([results_p[i][2] for i in range(len(results_p))])\n",
    "    mpred = np.stack([results_p[i][3] for i in range(len(results_p))])\n",
    "    mr2 = np.stack([results_p[i][4] for i in range(len(results_p))])\n",
    "\n",
    "    nt_glm_lag = len(lag_list)\n",
    "    GLM_Data = {'mcc': mcc,\n",
    "                'msta': msta,\n",
    "                'msp': msp,\n",
    "                'mpred': mpred,\n",
    "                'mr2':mr2,}\n",
    "    if do_shuffle:\n",
    "        ioh5.save(save_dir/'GLM_{}_Data_VisOnly_notsmooth_dt{:03d}_T{:02d}_shuffled.h5'.format(model_type,int(model_dt*1000), nt_glm_lag), GLM_Data)\n",
    "    else:\n",
    "        ioh5.save(save_dir/'GLM_{}_Data_VisOnly_notsmooth_dt{:03d}_T{:02d}.h5'.format(model_type,int(model_dt*1000), nt_glm_lag), GLM_Data)\n",
    "        \n",
    "    del train_nsp_r, test_nsp_r, train_data_r, test_data_r, result_ids, results_p, mcc, msta, msp, mpred, mr2,\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arranged-apartment",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_length=40\n",
    "celln = np.argmax(mcc)\n",
    "fig, axs = plt.subplots(1,nt_glm_lag+1,figsize=(30,5))\n",
    "predcell = mpred[celln]/model_dt\n",
    "nspcell = msp[celln]/model_dt\n",
    "msp_smooth=(np.convolve(msp[celln], np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "pred_smooth=(np.convolve(mpred[celln], np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "axs[0].plot(msp_smooth,'k',label='test FR')\n",
    "axs[0].plot(pred_smooth,'r', label='pred FR')\n",
    "axs[0].set_xlabel('Frame #')\n",
    "axs[0].set_ylabel('Firing Rate (spks/s)')\n",
    "axs[0].legend()\n",
    "axs[0].set_title('Smoothed FRs')\n",
    "\n",
    "crange = np.max(np.abs(msta[celln]))\n",
    "for axn,n in enumerate(range(1,nt_glm_lag+1)):\n",
    "    img = axs[n].imshow(msta[celln,axn],cmap='seismic',vmin=-crange,vmax=crange)\n",
    "    axs[n].axis('off')\n",
    "    axs[n].set_title('Lag: {}ms'.format(int(1000*lag_list[axn]*model_dt)))\n",
    "cbar = add_colorbar(img,) # orientation='horizontal',location='bottom'\n",
    "# cbar.set_ticks([-crange,crange])\n",
    "# cbar.set_ticklabels(['Dark','Light'])\n",
    "plt.suptitle('Celln:{}, cc={:.03f}, r2={:.03}'.format(celln,mcc[celln],mr2[celln]),y=.8,fontsize=20)\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signed-sussex",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2,figsize=(10,5))\n",
    "axs[0].hist(mcc,bins=20,color='k',alpha=.5,label='Test CC')\n",
    "# axs[0].hist(mcc,bins=20,color='r', alpha=.5,label='Shuffled CC')\n",
    "axs[0].set_xlabel('Corr. Coeff.')\n",
    "axs[0].legend(fontsize=12)\n",
    "axs[1].hist(mr2,bins=20,color='k',alpha=.5,label='Test $r^2$')\n",
    "# axs[1].hist(mr2,bins=20,color='r', alpha=.5,label='Shuffled $r^2$')\n",
    "axs[1].set_xlabel('$R^2$')\n",
    "axs[1].legend(fontsize=12)\n",
    "plt.tight_layout()\n",
    "# fig.savefig(FigPath/'CC_comparison_VisOnly_notsmooth_{}_dt{:03d}_T{:02d}.png'.format(model_type,int(model_dt*1000), nt_glm_lag), facecolor='white', transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strange-advertiser",
   "metadata": {},
   "source": [
    "### Plotting Temporal Fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "democratic-consultancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_add(alpha,stat_range,stat_all):\n",
    "    return np.mean((stat_range - (stat_all+alpha))**2)\n",
    "\n",
    "def f_mult(alpha,stat_range,stat_all):\n",
    "    return np.mean((stat_range - (stat_all*alpha))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "typical-variance",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_length=40\n",
    "data, train_idx, test_idx = load_train_test(file_dict, save_dir, model_dt=model_dt, do_shuffle=False, do_norm=False,free_move=free_move, has_imu=free_move, has_mouse=False)\n",
    "locals().update(data)\n",
    "GLM_Vis = ioh5.load(save_dir/'GLM_{}_Data_VisOnly_notsmooth_dt{:03d}_T{:02d}.h5'.format(model_type,int(model_dt*1000), nt_glm_lag))\n",
    "locals().update(GLM_Vis)\n",
    "##### Explore Neurons #####\n",
    "colors = plt.cm.cool(np.linspace(0,1,4))\n",
    "clrs = ['blue','orange','green','red']\n",
    "# Initialize movement combinations\n",
    "titles = np.array(['Theta','Phi','Roll','Pitch']) # 'dg_p','dg_n' 'roll','pitch'\n",
    "titles_all = []\n",
    "for n in range(1,len(titles)+1):\n",
    "    perms = np.array(list(itertools.combinations(np.arange(len(titles)), n)))\n",
    "    for ind in range(perms.shape[0]):\n",
    "        titles_all.append('_'.join([t for t in titles[perms[ind]]]))\n",
    "\n",
    "train_dgaze_p = train_dth + np.diff(train_gz,append=0)\n",
    "train_dgaze_n = train_dth - np.diff(train_gz,append=0)\n",
    "test_dgaze_p = test_dth + np.diff(test_gz,append=0)\n",
    "test_dgaze_n = test_dth - np.diff(test_gz,append=0)\n",
    "# move_train = np.hstack((train_th[:,np.newaxis],train_phi[:,np.newaxis],train_roll[:,np.newaxis],train_pitch[:,np.newaxis], train_dth[:,np.newaxis],train_dphi[:,np.newaxis]))\n",
    "move_test = np.hstack((test_th[:,np.newaxis],test_phi[:,np.newaxis],test_roll[:,np.newaxis],test_pitch[:,np.newaxis])) #,test_dgaze_p[:,np.newaxis],test_dgaze_n[:,np.newaxis]))#\n",
    "# move_test = np.hstack((test_th[:,np.newaxis],test_phi[:,np.newaxis],test_dgaze_p[:,np.newaxis],test_dgaze_n[:,np.newaxis]))# test_dth[:,np.newaxis],test_dphi[:,np.newaxis]))\n",
    "model_move = np.hstack((model_th[:,np.newaxis],model_phi[:,np.newaxis],model_roll[:,np.newaxis],model_pitch[:,np.newaxis])) #,test_dgaze_p[:,np.newaxis],test_dgaze_n[:,np.newaxis]))#\n",
    "model_move = model_move - np.mean(model_move,axis=0)\n",
    "move_test = move_test - np.mean(move_test,axis=0)\n",
    "# Create all tuning curves for plotting\n",
    "N_bins=10\n",
    "ncells = model_nsp.shape[-1]\n",
    "ax_ylims = np.zeros((model_nsp.shape[-1],len(titles)))\n",
    "tuning_curves = np.zeros((model_nsp.shape[-1],len(titles),N_bins-1))\n",
    "tuning_stds = np.zeros((model_nsp.shape[-1],len(titles),N_bins-1))\n",
    "var_ranges = np.zeros((len(titles),N_bins-1))\n",
    "for modeln in range(len(titles)):\n",
    "    metric = move_test[:,modeln]\n",
    "    tuning, tuning_std, var_range = tuning_curve(test_nsp, metric, N_bins=N_bins, model_dt=model_dt)\n",
    "    tuning_curves[:,modeln] = tuning\n",
    "    tuning_stds[:,modeln] = tuning_std\n",
    "    ax_ylims[:,modeln] = np.max(tuning,axis=1)\n",
    "    var_ranges[modeln] = var_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "linear-jonathan",
   "metadata": {},
   "outputs": [],
   "source": [
    "celln=21\n",
    "predcell = mpred[celln]/model_dt\n",
    "nspcell = msp[celln]/model_dt\n",
    "fig, axs = plt.subplots()\n",
    "top_yaxs = np.max(ax_ylims[celln])+2*np.max(tuning_stds[celln])\n",
    "for i, modeln in enumerate(range(2,len(titles))):\n",
    "    metric = move_test[:,modeln]\n",
    "    #     metric = metric[(metric>var_range[modeln,0])&(metric<var_range[modeln,-1\n",
    "    nranges = np.round(np.quantile(metric,quartiles),decimals=1)\n",
    "    stat_range, edges, _ = binned_statistic(metric,test_nsp[:,celln],statistic='mean',bins=nranges)\n",
    "    edge_mids = np.quantile(metric,spk_percentile2)#np.array([(edges[i]+edges[i+1])/2 for i in range(len(edges)-1)])\n",
    "    cmap = mpl.colors.ListedColormap(colors, N=colors.shape[0])\n",
    "    norm = mpl.colors.BoundaryNorm(boundaries=np.floor(nranges), ncolors=len(cmap.colors))\n",
    "\n",
    "    #     axs.imshow(np.round(nranges,decimals=1)[None,:-1],aspect='auto',cmap=cmap,norm=norm,extent=(nranges[0],nranges[-1],(i)*top_yaxs/2,(i+1)*top_yaxs/2),alpha=.8)\n",
    "    axs.errorbar(var_ranges[modeln], tuning_curves[celln,modeln], yerr=tuning_stds[celln,modeln], label=titles[modeln], c=clrs[modeln],lw=4,elinewidth=3)\n",
    "    axs.scatter(edge_mids,stat_range/model_dt,c=clrs[modeln])\n",
    "    for m in range(len(nranges)-1):\n",
    "        print(nranges[m], nranges[m+1], ((i)*top_yaxs/2),((i+1)*top_yaxs/2))\n",
    "        axs.axvspan(nranges[m], nranges[m+1],ymin=i*1/2,ymax=(i+1)*1/2,alpha=0.8, color=colors[m],zorder=0)\n",
    "axs.set_xlim(-60,60)\n",
    "axs.set_ylim(0,13)\n",
    "axs.set_xlabel('Angle ($ ^{\\degree}$)')\n",
    "axs.set_ylabel('Spikes/s')\n",
    "axs.set_title('Eye Tuning Curves')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biological-facial",
   "metadata": {},
   "outputs": [],
   "source": [
    "celln= 121 #np.argmax(mr2)\n",
    "bin_length=40\n",
    "ncells=model_nsp.shape[-1]\n",
    "colors = plt.cm.cool(np.linspace(0,1,4))\n",
    "clrs = ['blue','orange','green','red']\n",
    "# quartiles = np.arange(.1,1,.2)#[0,.25,.5,.75,1]\n",
    "quartiles = np.arange(0,1.25,.25)\n",
    "# lag=150 # in ms\n",
    "# nt_glm_lag = 5\n",
    "# minlag = int(-lag//(1000*model_dt)); maxlag=int((lag//(1000*model_dt))+1)\n",
    "lag_list = np.arange(-1,4,dtype=int) #np.arange(minlag,maxlag,np.floor((maxlag-minlag)/nt_glm_lag).astype(int))\n",
    "nt_glm_lag = len(lag_list)\n",
    "\n",
    "fig, axs = plt.subplots(3,5, figsize=((35,15))) \n",
    "gs = axs[0,0].get_gridspec()\n",
    "gs_sub = gs[0,:].subgridspec(1,nt_glm_lag)\n",
    "for ax in axs[0,:]:\n",
    "    ax.remove()\n",
    "top_grid = np.zeros((nt_glm_lag),dtype=object)\n",
    "for ind in range(nt_glm_lag):\n",
    "    top_grid[ind] = fig.add_subplot(gs_sub[0,ind])\n",
    "\n",
    "predcell = mpred[celln]/model_dt\n",
    "nspcell = msp[celln]/model_dt\n",
    "msp_smooth=(np.convolve(msp[celln], np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "pred_smooth=(np.convolve(mpred[celln], np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "# Set up predicted spike range between 1-99th percentile\n",
    "stat_bins = 5\n",
    "pred_range = np.quantile(predcell,[.1,.9])\n",
    "msp_range = np.quantile(nspcell,[.01,1])\n",
    "spike_percentiles = np.arange(0,1.25,.25)\n",
    "spike_percentiles[-1]=.99\n",
    "spk_percentile2 = np.arange(.125,1.125,.25)\n",
    "pred_rangelin = np.quantile(predcell,spike_percentiles)\n",
    "xbin_pts = np.quantile(predcell,spk_percentile2)\n",
    "stat_bins = len(pred_rangelin) #5\n",
    "\n",
    "\n",
    "axs[1,0].plot(msp_smooth,'k',label='test FR')\n",
    "axs[1,0].plot(pred_smooth,'r', label='pred FR')\n",
    "axs[1,0].set_xlabel('Frame #')\n",
    "axs[1,0].set_ylabel('Firing Rate (spks/s)')\n",
    "axs[1,0].legend()\n",
    "axs[1,0].set_title('Smoothed FRs')\n",
    "\n",
    "crange = np.max(np.abs(msta[celln]))\n",
    "for n in range(nt_glm_lag):\n",
    "    img = top_grid[n].imshow(msta[celln,n],cmap='RdBu_r',vmin=-crange,vmax=crange)\n",
    "    top_grid[n].axis('off')\n",
    "    top_grid[n].set_title('Lag:{:03d} ms'.format(int(1000*lag_list[n]*model_dt)))\n",
    "    top_grid[n].axis('off')\n",
    "add_colorbar(img)\n",
    "\n",
    "# Eye Tuning Curve\n",
    "top_yaxs = np.max(ax_ylims[celln])+2*np.max(tuning_stds[celln])\n",
    "for i,modeln in enumerate(range(len(titles)-2)):\n",
    "    metric = move_test[:,modeln]\n",
    "    nranges = np.quantile(metric,quartiles)\n",
    "    stat_range, edges, _ = binned_statistic(metric,test_nsp[:,celln],statistic='mean',bins=nranges)\n",
    "    edge_mids = np.quantile(metric,spk_percentile2)#np.array([(edges[i]+edges[i+1])/2 for i in range(len(edges)-1)])\n",
    "    cmap = mpl.colors.ListedColormap(colors, N=colors.shape[0])\n",
    "    norm = mpl.colors.BoundaryNorm(boundaries=np.floor(nranges), ncolors=len(cmap.colors))\n",
    "    for m in range(len(nranges)-1):\n",
    "        axs[1,1].axvspan(nranges[m], nranges[m+1],ymin=i*1/2,ymax=(i+1)*1/2,alpha=0.8, color=colors[m],zorder=0)\n",
    "    #     axs[1,1].errorbar(var_ranges[modeln],tuning_curves[celln,modeln], yerr=tuning_stds[celln,modeln],label=titles[modeln],c=clrs[modeln],lw=4,elinewidth=3)\n",
    "    axs[1,1].plot(edge_mids,stat_range/model_dt,'.-', ms=20, lw=4,c=clrs[modeln])\n",
    "\n",
    "axs[1,1].set_ylim(bottom=0,top=np.max(ax_ylims,axis=1)[celln]+2*np.max(tuning_stds,axis=(1,2))[celln])\n",
    "axs[1,1].set_xlim(-30,30)\n",
    "axs[1,1].set_xlabel('Angle ($ ^{\\degree}$)')\n",
    "axs[1,1].set_ylabel('Spikes/s')\n",
    "axs[1,1].set_title('Eye Tuning Curves')\n",
    "lines = axs[1,1].get_lines()\n",
    "legend1 = axs[1,1].legend([lines[0]],[titles[2]],bbox_to_anchor=(1.01, .2), fontsize=12)\n",
    "legend2 = axs[1,1].legend([lines[1]],[titles[3]],bbox_to_anchor=(1.01, 1), fontsize=12)\n",
    "axs[1,1].add_artist(legend1)\n",
    "\n",
    "# Head Tuning Curves\n",
    "top_yaxs = np.max(ax_ylims[celln])+2*np.max(tuning_stds[celln])\n",
    "for i, modeln in enumerate(range(2,len(titles))):\n",
    "    metric = move_test[:,modeln]\n",
    "#     nranges = np.round(np.quantile(var_ranges[modeln],quartiles),decimals=1)\n",
    "    nranges = np.round(np.quantile(metric,quartiles),decimals=1)\n",
    "    stat_range, edges, _ = binned_statistic(metric,test_nsp[:,celln],statistic='mean',bins=nranges)\n",
    "    edge_mids = np.quantile(metric,spk_percentile2)#np.array([(edges[i]+edges[i+1])/2 for i in range(len(edges)-1)])\n",
    "    cmap = mpl.colors.ListedColormap(colors, N=colors.shape[0])\n",
    "    norm = mpl.colors.BoundaryNorm(boundaries=np.floor(nranges), ncolors=len(cmap.colors))\n",
    "    for m in range(len(nranges)-1):\n",
    "        axs[1,2].axvspan(nranges[m], nranges[m+1],ymin=i*1/2,ymax=(i+1)*1/2,alpha=0.8, color=colors[m],zorder=0)\n",
    "#     axs[1,2].errorbar(var_ranges[modeln], tuning_curves[celln,modeln], yerr=tuning_stds[celln,modeln], label=titles[modeln], c=clrs[modeln],lw=4,elinewidth=3)\n",
    "    axs[1,2].plot(edge_mids,stat_range/model_dt,'.-', ms=20, lw=4,c=clrs[modeln])\n",
    "\n",
    "axs[1,2].set_ylim(bottom=0,top=top_yaxs)\n",
    "axs[1,2].set_xlim(-30,30)\n",
    "axs[1,2].set_xlabel('Angle ($ ^{\\degree}$)')\n",
    "axs[1,2].set_ylabel('Spikes/s')\n",
    "axs[1,2].set_title('Head Tuning Curves')\n",
    "lines = axs[1,2].get_lines()\n",
    "legend1 = axs[1,2].legend([lines[0]],[titles[2]],bbox_to_anchor=(1.01, .2), fontsize=12)\n",
    "legend2 = axs[1,2].legend([lines[1]],[titles[3]],bbox_to_anchor=(1.01, 1), fontsize=12)\n",
    "axs[1,2].add_artist(legend1)\n",
    "\n",
    "# axs[1,2].legend(bbox_to_anchor=(1.01, 1), fontsize=12)\n",
    "\n",
    "\n",
    "# pred_rangelin = np.linspace(pred_range[0],pred_range[1],stat_bins)\n",
    "axs[1,3].scatter(mpred[celln]/model_dt,msp[celln]/model_dt,c='k',s=15)\n",
    "axs[1,3].plot(np.linspace(msp_range[0],msp_range[1]),np.linspace(msp_range[0],msp_range[1]),'k--',zorder=0)\n",
    "axs[1,3].set_xlabel('Predicted Spike Rate')\n",
    "axs[1,3].set_ylabel('Actual Spike Rate')\n",
    "cbar = add_colorbar(img)\n",
    "# cbar.set_label('count')\n",
    "\n",
    "hist,xedges,yedges,img =axs[1,4].hist2d(mpred[celln]/model_dt,msp[celln]/model_dt,range=np.vstack((pred_range,msp_range)))#pred_smooth,msp_smooth)\n",
    "axs[1,4].set_xlabel('Predicted Spike Rate')\n",
    "axs[1,4].set_ylabel('Actual Spike Rate')\n",
    "cbar = add_colorbar(img)\n",
    "cbar.set_label('count')\n",
    "\n",
    "\n",
    "mse_add = np.zeros((ncells,len(titles),len(quartiles)-1))\n",
    "mse_mult = np.zeros((ncells,len(titles),len(quartiles)-1))\n",
    "alpha_add = np.zeros((ncells,len(titles),len(quartiles)-1))\n",
    "alpha_mult = np.zeros((ncells,len(titles),len(quartiles)-1))\n",
    "\n",
    "traces = np.zeros((ncells,len(titles),len(quartiles)-1,stat_bins-1)) # (model_type,quartile,FR)\n",
    "traces_mean = np.zeros((ncells,len(titles),stat_bins-1)) # (model_type,quartile,FR)\n",
    "edges_all = np.zeros((ncells,len(titles),len(quartiles)-1,stat_bins-1)) # (model_type,quartile,FR)\n",
    "# df_traces = pd.DataFrame([],columns=['modeln','quartile','FR']) \n",
    "for modeln in range(len(titles)):\n",
    "    metric = move_test[:,modeln]\n",
    "    nranges = np.quantile(metric,quartiles)# np.linspace(np.nanmean(metric)-2*np.nanstd(metric), np.nanmean(metric)+2*np.nanstd(metric),N_bins)\n",
    "    stat_all, edges, _ = binned_statistic(predcell,nspcell, statistic='mean',bins=pred_rangelin)\n",
    "    edge_mids = xbin_pts#np.array([(edges[i]+edges[i+1])/2 for i in range(len(edges)-1)])\n",
    "    traces_mean[celln,modeln]=stat_all\n",
    "    max_fr = np.max(stat_all)\n",
    "#     axs[1,modeln].set_xlim(0,pred_range[1]+np.std(pred_range))\n",
    "#     axs[1,modeln].set_ylim(0,np.max(stat)+np.std(stat))\n",
    "\n",
    "    for n in range(len(nranges)-1):\n",
    "        ind = np.where(((metric<=nranges[n+1])&(metric>nranges[n])))[0]\n",
    "        pred = predcell[ind]\n",
    "        sp = nspcell[ind]\n",
    "\n",
    "        stat_range, edges, _ = binned_statistic(pred, sp, statistic='mean',bins=pred_rangelin)\n",
    "        edge_mids = xbin_pts #np.array([(edges[i]+edges[i+1])/2 for i in range(len(edges)-1)])\n",
    "        traces[celln,modeln,n]=stat_range\n",
    "        edges_all[celln,modeln,n]=edge_mids\n",
    "        res_add = minimize_scalar(f_add,args=(stat_range/max_fr, stat_all/max_fr))\n",
    "        res_mult = minimize_scalar(f_mult,args=(stat_range/max_fr, stat_all/max_fr))\n",
    "        mse_add[celln, modeln, n] = res_add.fun\n",
    "        mse_mult[celln, modeln, n] = res_mult.fun\n",
    "        alpha_add[celln, modeln, n] = res_add.x\n",
    "        alpha_mult[celln, modeln, n] = res_mult.x\n",
    "\n",
    "        axs[2,modeln].plot(edge_mids, stat_range,'.-', c=colors[n],label='{:.02f} : {:.02f}'.format(nranges[n],nranges[n+1]),lw=4,ms=20,alpha=.9)\n",
    "        axs[2,modeln].set_title('Metric: {}'.format(titles[modeln]), color=clrs[modeln])\n",
    "        axs[2,modeln].set_xlabel('Predicted Spike Rate')\n",
    "        axs[2,modeln].set_ylabel('Actual Spike Rate')\n",
    "    \n",
    "    lim_max = np.max(traces[celln,modeln])+np.std(traces_mean[celln,modeln])\n",
    "    lim_min = np.min(traces[celln,modeln])-np.std(traces_mean[celln,modeln])\n",
    "    lims = (0, lim_max) if (lim_min)<0 else (lim_min,lim_max) \n",
    "    axs[2,modeln].plot(np.linspace(lims[0],lims[1]),np.linspace(lims[0],lims[1]),'k--',zorder=0)\n",
    "\n",
    "    axs[2,modeln].plot(edge_mids, stat_all,'.-', c='k', lw=5, ms=20, label='All_data', alpha=.8)\n",
    "    axs[2,modeln].legend(bbox_to_anchor=(1.01, 1), fontsize=12)\n",
    "#     axs[2,modeln].axis('equal')\n",
    "#     axs[2,modeln].set_xlim(left=0)\n",
    "    axs[2,modeln].set(xlim=lims, ylim=lims)\n",
    "\n",
    "#     axs[2,modeln].set_xlim([0,xbin_pts[-1]])\n",
    "#     axs[2,modeln].set_ylim(bottom=0)\n",
    "\n",
    "dmodel = mse_add[celln]-mse_mult[celln]\n",
    "crange = np.max(np.abs(dmodel))\n",
    "im = axs[2,-1].imshow(dmodel,cmap='seismic',vmin=-crange,vmax=crange)\n",
    "axs[2,-1].set_yticks(np.arange(0,4))\n",
    "axs[2,-1].set_yticklabels(titles)\n",
    "axs[2,-1].set_ylabel('Movement Model')\n",
    "axs[2,-1].set_xticks(np.arange(0,4))\n",
    "axs[2,-1].set_xticklabels(['.25','.5','.75','1'])\n",
    "axs[2,-1].set_xlabel('Quantile Range')\n",
    "axs[2,-1].set_title('$MSE_{add}$ - $MSE_{mult}$')\n",
    "cbar = add_colorbar(im)\n",
    "\n",
    "plt.suptitle('Celln:{}, r2={:.03f}, dev={:.03f}'.format(celln,mcc[celln]**2,mr2[celln]),y=1,fontsize=30)\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "# fig.savefig(FigPath/'CellSummary_N{}_T{:02d}.png'.format(celln,nt_glm_lag), facecolor='white', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offshore-miami",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_length=40\n",
    "cc_all = np.zeros(mpred.shape[0])\n",
    "cc_all2 = np.zeros(mpred.shape[0])\n",
    "for celln in range(mpred.shape[0]):\n",
    "    msp_smooth=(np.convolve(msp[celln], np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "    pred_smooth=(np.convolve(mpred[celln], np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "    cc_all[celln] = np.corrcoef(msp_smooth[bin_length:-bin_length],pred_smooth[bin_length:-bin_length])[0,1]\n",
    "    cc_all2[celln] = np.corrcoef(msp_smooth,pred_smooth)[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concerned-lighter",
   "metadata": {},
   "outputs": [],
   "source": [
    "GLM_Vis = ioh5.load(save_dir/'GLM_{}_Data_VisOnly_notsmooth_dt{:03d}_T{:02d}.h5'.format(model_type,int(model_dt*1000), nt_glm_lag))\n",
    "locals().update(GLM_Vis)\n",
    "data, train_idx, test_idx = load_train_test(file_dict, save_dir, model_dt=model_dt, do_shuffle=do_shuffle, do_norm=False,free_move=free_move, has_imu=free_move, has_mouse=False)\n",
    "locals().update(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detailed-economy",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_length=40\n",
    "do_shuffle = False\n",
    "ncells=model_nsp.shape[-1]\n",
    "colors = plt.cm.cool(np.linspace(0,1,4))\n",
    "clrs = ['blue','orange','green','red']\n",
    "for model_dt in [.05]: #\n",
    "    lag=150 # in ms\n",
    "    nt_glm_lag = 5\n",
    "#     minlag = int(-lag//(1000*model_dt)); maxlag=int((lag//(1000*model_dt))+1)\n",
    "    lag_list = np.arange(-1,4,dtype=int) #np.arange(minlag,maxlag,np.floor((maxlag-minlag)/nt_glm_lag).astype(int))\n",
    "    nt_glm_lag = len(lag_list)\n",
    "    if do_shuffle:\n",
    "        GLM_Vis_shuff = ioh5.load(save_dir/'GLM_{}_Data_VisOnly_notsmooth_dt{:03d}_T{:02d}_shuffled.h5'.format(model_type,int(model_dt*1000), nt_glm_lag))\n",
    "        locals().update(GLM_Vis_shuff)\n",
    "    else:\n",
    "        GLM_Vis = ioh5.load(save_dir/'GLM_{}_Data_VisOnly_notsmooth_dt{:03d}_T{:02d}.h5'.format(model_type,int(model_dt*1000), nt_glm_lag))\n",
    "        locals().update(GLM_Vis)\n",
    "    \n",
    "    data, train_idx, test_idx = load_train_test(file_dict, save_dir, model_dt=model_dt, do_shuffle=do_shuffle, do_norm=False,free_move=free_move, has_imu=free_move, has_mouse=False)\n",
    "    locals().update(data)\n",
    "    ##### Explore Neurons #####\n",
    "    # Initialize movement combinations\n",
    "    titles = np.array(['th','phi','roll','pitch']) # 'dg_p','dg_n' 'roll','pitch' 'dg_p','dg_n']) #\n",
    "    titles_all = []\n",
    "    for n in range(1,len(titles)+1):\n",
    "        perms = np.array(list(itertools.combinations(np.arange(len(titles)), n)))\n",
    "        for ind in range(perms.shape[0]):\n",
    "            titles_all.append('_'.join([t for t in titles[perms[ind]]]))\n",
    "\n",
    "    # train_dgaze_p = train_dth + np.diff(train_gz,append=0)\n",
    "    # train_dgaze_n = train_dth - np.diff(train_gz,append=0)\n",
    "    # test_dgaze_p = test_dth + np.diff(test_gz,append=0)\n",
    "    # test_dgaze_n = test_dth - np.diff(test_gz,append=0)\n",
    "    # move_train = np.hstack((train_th[:,np.newaxis],train_phi[:,np.newaxis],train_roll[:,np.newaxis],train_pitch[:,np.newaxis], train_dth[:,np.newaxis],train_dphi[:,np.newaxis]))\n",
    "    move_test = np.hstack((test_th[:,np.newaxis],test_phi[:,np.newaxis],test_roll[:,np.newaxis],test_pitch[:,np.newaxis])) #,test_dgaze_p[:,np.newaxis],test_dgaze_n[:,np.newaxis]))#\n",
    "    # move_test = np.hstack((test_th[:,np.newaxis],test_phi[:,np.newaxis],test_dgaze_p[:,np.newaxis],test_dgaze_n[:,np.newaxis]))# test_dth[:,np.newaxis],test_dphi[:,np.newaxis]))\n",
    "    move_test = move_test - np.mean(move_test,axis=0)\n",
    "    # Create all tuning curves for plotting\n",
    "    N_bins=10\n",
    "    ncells = model_nsp.shape[-1]\n",
    "    ax_ylims = np.zeros((model_nsp.shape[-1],len(titles)))\n",
    "    tuning_curves = np.zeros((model_nsp.shape[-1],len(titles),N_bins-1))\n",
    "    tuning_stds = np.zeros((model_nsp.shape[-1],len(titles),N_bins-1))\n",
    "    var_ranges = np.zeros((len(titles),N_bins-1))\n",
    "    for modeln in range(len(titles)):\n",
    "        metric = move_test[:,modeln]\n",
    "        tuning, tuning_std, var_range = tuning_curve(test_nsp, metric, N_bins=N_bins, model_dt=model_dt)\n",
    "        tuning_curves[:,modeln] = tuning\n",
    "        tuning_stds[:,modeln] = tuning_std\n",
    "        ax_ylims[:,modeln] = np.max(tuning,axis=1)\n",
    "        var_ranges[modeln] = var_range\n",
    "    cc_all = np.zeros(test_nsp.shape[1])\n",
    "    quartiles = np.arange(0,1.25,.25)\n",
    "    if do_shuffle:\n",
    "        pdf_name = FigPath/ 'GLM_{}_dt{:03d}_T{:02d}_cellsummary_sig_shuff.pdf'.format(model_type,int(model_dt*1000),nt_glm_lag)\n",
    "    else:\n",
    "        pdf_name = FigPath/ 'GLM_{}_dt{:03d}_T{:02d}_cellsummary_sig.pdf'.format(model_type,int(model_dt*1000),nt_glm_lag)\n",
    "    with PdfPages(pdf_name) as pdf:\n",
    "        for celln in tqdm(range(msp.shape[0])):\n",
    "            if mcc[celln]>.25:\n",
    "                fig, axs = plt.subplots(3,5, figsize=((35,15))) \n",
    "                gs = axs[0,0].get_gridspec()\n",
    "                gs_sub = gs[0,:].subgridspec(1,nt_glm_lag)\n",
    "                for ax in axs[0,:]:\n",
    "                    ax.remove()\n",
    "                top_grid = np.zeros((nt_glm_lag),dtype=object)\n",
    "                for ind in range(nt_glm_lag):\n",
    "                    top_grid[ind] = fig.add_subplot(gs_sub[0,ind])\n",
    "\n",
    "                predcell = mpred[celln]/model_dt\n",
    "                nspcell = msp[celln]/model_dt\n",
    "                msp_smooth=(np.convolve(msp[celln], np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "                pred_smooth=(np.convolve(mpred[celln], np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "                axs[1,0].plot(msp_smooth,'k',label='test FR')\n",
    "                axs[1,0].plot(pred_smooth,'r', label='pred FR')\n",
    "                axs[1,0].set_xlabel('Frame #')\n",
    "                axs[1,0].set_ylabel('Firing Rate (spks/s)')\n",
    "                axs[1,0].legend()\n",
    "                axs[1,0].set_title('Smoothed FRs')\n",
    "\n",
    "                crange = np.max(np.abs(msta[celln]))\n",
    "                for n in range(nt_glm_lag):\n",
    "                    img = top_grid[n].imshow(msta[celln,n],cmap='RdBu_r',vmin=-crange,vmax=crange)\n",
    "                    top_grid[n].axis('off')\n",
    "                    top_grid[n].set_title('Lag:{:03d} ms'.format(int(1000*lag_list[n]*model_dt)))\n",
    "                    top_grid[n].axis('off')\n",
    "                add_colorbar(img)\n",
    "\n",
    "                # Eye Tuning Curve\n",
    "                top_yaxs = np.max(ax_ylims[celln])+2*np.max(tuning_stds[celln])\n",
    "                for i,modeln in enumerate(range(len(titles)-2)):\n",
    "                    metric = move_test[:,modeln]\n",
    "                    nranges = np.quantile(metric,quartiles)\n",
    "                    stat_range, edges, _ = binned_statistic(metric,test_nsp[:,celln],statistic='mean',bins=nranges)\n",
    "                    edge_mids = np.quantile(metric,spk_percentile2)#np.array([(edges[i]+edges[i+1])/2 for i in range(len(edges)-1)])\n",
    "                    cmap = mpl.colors.ListedColormap(colors, N=colors.shape[0])\n",
    "                    norm = mpl.colors.BoundaryNorm(boundaries=np.floor(nranges), ncolors=len(cmap.colors))\n",
    "                    for m in range(len(nranges)-1):\n",
    "                        axs[1,1].axvspan(nranges[m], nranges[m+1],ymin=i*1/2,ymax=(i+1)*1/2,alpha=0.8, color=colors[m],zorder=0)\n",
    "                    #     axs[1,1].errorbar(var_ranges[modeln],tuning_curves[celln,modeln], yerr=tuning_stds[celln,modeln],label=titles[modeln],c=clrs[modeln],lw=4,elinewidth=3)\n",
    "                    axs[1,1].plot(edge_mids,stat_range/model_dt,'.-', ms=20, lw=4,c=clrs[modeln])\n",
    "\n",
    "                axs[1,1].set_ylim(bottom=0,top=np.max(ax_ylims,axis=1)[celln]+2*np.max(tuning_stds,axis=(1,2))[celln])\n",
    "                axs[1,1].set_xlim(-30,30)\n",
    "                axs[1,1].set_xlabel('Angle ($ ^{\\degree}$)')\n",
    "                axs[1,1].set_ylabel('Spikes/s')\n",
    "                axs[1,1].set_title('Eye Tuning Curves')\n",
    "                lines = axs[1,1].get_lines()\n",
    "                legend1 = axs[1,1].legend([lines[0]],[titles[2]],bbox_to_anchor=(1.01, .2), fontsize=12)\n",
    "                legend2 = axs[1,1].legend([lines[1]],[titles[3]],bbox_to_anchor=(1.01, 1), fontsize=12)\n",
    "                axs[1,1].add_artist(legend1)\n",
    "\n",
    "                # Head Tuning Curves\n",
    "                top_yaxs = np.max(ax_ylims[celln])+2*np.max(tuning_stds[celln])\n",
    "                for i, modeln in enumerate(range(2,len(titles))):\n",
    "                    metric = move_test[:,modeln]\n",
    "                #     nranges = np.round(np.quantile(var_ranges[modeln],quartiles),decimals=1)\n",
    "                    nranges = np.round(np.quantile(metric,quartiles),decimals=1)\n",
    "                    stat_range, edges, _ = binned_statistic(metric,test_nsp[:,celln],statistic='mean',bins=nranges)\n",
    "                    edge_mids = np.quantile(metric,spk_percentile2)#np.array([(edges[i]+edges[i+1])/2 for i in range(len(edges)-1)])\n",
    "                    cmap = mpl.colors.ListedColormap(colors, N=colors.shape[0])\n",
    "                    norm = mpl.colors.BoundaryNorm(boundaries=np.floor(nranges), ncolors=len(cmap.colors))\n",
    "                    for m in range(len(nranges)-1):\n",
    "                        axs[1,2].axvspan(nranges[m], nranges[m+1],ymin=i*1/2,ymax=(i+1)*1/2,alpha=0.8, color=colors[m],zorder=0)\n",
    "                #     axs[1,2].errorbar(var_ranges[modeln], tuning_curves[celln,modeln], yerr=tuning_stds[celln,modeln], label=titles[modeln], c=clrs[modeln],lw=4,elinewidth=3)\n",
    "                    axs[1,2].plot(edge_mids,stat_range/model_dt,'.-', ms=20, lw=4,c=clrs[modeln])\n",
    "\n",
    "                axs[1,2].set_ylim(bottom=0,top=top_yaxs)\n",
    "                axs[1,2].set_xlim(-30,30)\n",
    "                axs[1,2].set_xlabel('Angle ($ ^{\\degree}$)')\n",
    "                axs[1,2].set_ylabel('Spikes/s')\n",
    "                axs[1,2].set_title('Head Tuning Curves')\n",
    "                lines = axs[1,2].get_lines()\n",
    "                legend1 = axs[1,2].legend([lines[0]],[titles[2]],bbox_to_anchor=(1.01, .2), fontsize=12)\n",
    "                legend2 = axs[1,2].legend([lines[1]],[titles[3]],bbox_to_anchor=(1.01, 1), fontsize=12)\n",
    "                axs[1,2].add_artist(legend1)\n",
    "\n",
    "                # axs[1,2].legend(bbox_to_anchor=(1.01, 1), fontsize=12)\n",
    "\n",
    "                # Set up predicted spike range between 1-99th percentile\n",
    "                stat_bins = 5\n",
    "                pred_range = np.quantile(predcell,[.1,.9])\n",
    "                msp_range = np.quantile(nspcell,[.01,1])\n",
    "                spike_percentiles = np.arange(0,1.25,.25)\n",
    "                spike_percentiles[-1]=.99\n",
    "                spk_percentile2 = np.arange(.125,1.125,.25)\n",
    "                pred_rangelin = np.quantile(predcell,spike_percentiles)\n",
    "                xbin_pts = np.quantile(predcell,spk_percentile2)\n",
    "                stat_bins = len(pred_rangelin) #5\n",
    "                # pred_rangelin = np.linspace(pred_range[0],pred_range[1],stat_bins)\n",
    "                axs[1,3].scatter(mpred[celln]/model_dt,msp[celln]/model_dt,c='k',s=15)\n",
    "                axs[1,3].plot(np.linspace(msp_range[0],msp_range[1]),np.linspace(msp_range[0],msp_range[1]),'k--',zorder=0)\n",
    "                axs[1,3].set_xlabel('Predicted Spike Rate')\n",
    "                axs[1,3].set_ylabel('Actual Spike Rate')\n",
    "                cbar = add_colorbar(img)\n",
    "                # cbar.set_label('count')\n",
    "\n",
    "                hist,xedges,yedges,img =axs[1,4].hist2d(mpred[celln]/model_dt,msp[celln]/model_dt,range=np.vstack((pred_range,msp_range)))#pred_smooth,msp_smooth)\n",
    "                axs[1,4].set_xlabel('Predicted Spike Rate')\n",
    "                axs[1,4].set_ylabel('Actual Spike Rate')\n",
    "                cbar = add_colorbar(img)\n",
    "                cbar.set_label('count')\n",
    "\n",
    "\n",
    "                mse_add = np.zeros((ncells,len(titles),len(quartiles)-1))\n",
    "                mse_mult = np.zeros((ncells,len(titles),len(quartiles)-1))\n",
    "                alpha_add = np.zeros((ncells,len(titles),len(quartiles)-1))\n",
    "                alpha_mult = np.zeros((ncells,len(titles),len(quartiles)-1))\n",
    "\n",
    "                traces = np.zeros((ncells,len(titles),len(quartiles)-1,stat_bins-1)) # (model_type,quartile,FR)\n",
    "                traces_mean = np.zeros((ncells,len(titles),stat_bins-1)) # (model_type,quartile,FR)\n",
    "                edges_all = np.zeros((ncells,len(titles),len(quartiles)-1,stat_bins-1)) # (model_type,quartile,FR)\n",
    "                # df_traces = pd.DataFrame([],columns=['modeln','quartile','FR']) \n",
    "                for modeln in range(len(titles)):\n",
    "                    metric = move_test[:,modeln]\n",
    "                    nranges = np.quantile(metric,quartiles)# np.linspace(np.nanmean(metric)-2*np.nanstd(metric), np.nanmean(metric)+2*np.nanstd(metric),N_bins)\n",
    "                    stat_all, edges, _ = binned_statistic(predcell,nspcell, statistic='mean',bins=pred_rangelin)\n",
    "                    edge_mids = xbin_pts#np.array([(edges[i]+edges[i+1])/2 for i in range(len(edges)-1)])\n",
    "                    traces_mean[celln,modeln]=stat_all\n",
    "                    max_fr = np.max(stat_all)\n",
    "                #     axs[1,modeln].set_xlim(0,pred_range[1]+np.std(pred_range))\n",
    "                #     axs[1,modeln].set_ylim(0,np.max(stat)+np.std(stat))\n",
    "\n",
    "                    for n in range(len(nranges)-1):\n",
    "                        ind = np.where(((metric<=nranges[n+1])&(metric>nranges[n])))[0]\n",
    "                        pred = predcell[ind]\n",
    "                        sp = nspcell[ind]\n",
    "\n",
    "                        stat_range, edges, _ = binned_statistic(pred, sp, statistic='mean',bins=pred_rangelin)\n",
    "                        edge_mids = xbin_pts #np.array([(edges[i]+edges[i+1])/2 for i in range(len(edges)-1)])\n",
    "                        traces[celln,modeln,n]=stat_range\n",
    "                        edges_all[celln,modeln,n]=edge_mids\n",
    "                        res_add = minimize_scalar(f_add,args=(stat_range/max_fr, stat_all/max_fr))\n",
    "                        res_mult = minimize_scalar(f_mult,args=(stat_range/max_fr, stat_all/max_fr))\n",
    "                        mse_add[celln, modeln, n] = res_add.fun\n",
    "                        mse_mult[celln, modeln, n] = res_mult.fun\n",
    "                        alpha_add[celln, modeln, n] = res_add.x\n",
    "                        alpha_mult[celln, modeln, n] = res_mult.x\n",
    "\n",
    "                        axs[2,modeln].plot(edge_mids, stat_range,'.-', c=colors[n],label='{:.02f} : {:.02f}'.format(nranges[n],nranges[n+1]),lw=4,ms=20,alpha=.9)\n",
    "                        axs[2,modeln].set_title('Metric: {}'.format(titles[modeln]), color=clrs[modeln])\n",
    "                        axs[2,modeln].set_xlabel('Predicted Spike Rate')\n",
    "                        axs[2,modeln].set_ylabel('Actual Spike Rate')\n",
    "\n",
    "                    lim_max = np.max(traces[celln,modeln])+np.std(traces_mean[celln,modeln])\n",
    "                    lim_min = np.min(traces[celln,modeln])-np.std(traces_mean[celln,modeln])\n",
    "                    lims = (0, lim_max) if (lim_min)<0 else (lim_min,lim_max) \n",
    "                    axs[2,modeln].plot(np.linspace(lims[0],lims[1]),np.linspace(lims[0],lims[1]),'k--',zorder=0)\n",
    "\n",
    "                    axs[2,modeln].plot(edge_mids, stat_all,'.-', c='k', lw=5, ms=20, label='All_data', alpha=.8)\n",
    "                    axs[2,modeln].legend(bbox_to_anchor=(1.01, 1), fontsize=12)\n",
    "                #     axs[2,modeln].axis('equal')\n",
    "                #     axs[2,modeln].set_xlim(left=0)\n",
    "                    axs[2,modeln].set(xlim=lims, ylim=lims)\n",
    "\n",
    "                #     axs[2,modeln].set_xlim([0,xbin_pts[-1]])\n",
    "                #     axs[2,modeln].set_ylim(bottom=0)\n",
    "\n",
    "                dmodel = mse_add[celln]-mse_mult[celln]\n",
    "                crange = np.max(np.abs(dmodel))\n",
    "                im = axs[2,-1].imshow(dmodel,cmap='seismic',vmin=-crange,vmax=crange)\n",
    "                axs[2,-1].set_yticks(np.arange(0,4))\n",
    "                axs[2,-1].set_yticklabels(titles)\n",
    "                axs[2,-1].set_ylabel('Movement Model')\n",
    "                axs[2,-1].set_xticks(np.arange(0,4))\n",
    "                axs[2,-1].set_xticklabels(['.25','.5','.75','1'])\n",
    "                axs[2,-1].set_xlabel('Quantile Range')\n",
    "                axs[2,-1].set_title('$MSE_{add}$ - $MSE_{mult}$')\n",
    "                cbar = add_colorbar(im)\n",
    "\n",
    "                plt.suptitle('Celln:{}, cc={:.03f}, r2={:.03f}'.format(celln,mcc[celln],mr2[celln]),y=1,fontsize=30)\n",
    "                plt.tight_layout()\n",
    "\n",
    "                pdf.savefig()\n",
    "                plt.close()\n",
    "\n",
    "    # fig.savefig(FigPath/'CellSummary_N{}.png'.format(celln), facecolor='white', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "junior-disease",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "reasonable-credit",
   "metadata": {},
   "source": [
    "### Shuffle Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposite-replacement",
   "metadata": {},
   "outputs": [],
   "source": [
    "GLM_Vis_shuff['mcc']**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "international-background",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dt=.05\n",
    "# for model_dt in [.025,.05,.1]:\n",
    "GLM_Vis_shuff = ioh5.load(save_dir/'GLM_{}_Data_VisOnly_notsmooth_dt{:03d}_T{:02d}_shuffled.h5'.format(model_type,int(model_dt*1000), nt_glm_lag))\n",
    "GLM_Vis = ioh5.load(save_dir/'GLM_{}_Data_VisOnly_notsmooth_dt{:03d}_T{:02d}.h5'.format(model_type,int(model_dt*1000), nt_glm_lag))\n",
    "locals().update(GLM_Vis)\n",
    "\n",
    "count,edges = np.histogram((GLM_Vis['mcc'])**2,bins=np.arange(0,1,.01))\n",
    "count_shuff,edges_shuff = np.histogram((GLM_Vis_shuff['mcc'])**2,bins=np.arange(0,1,.01))\n",
    "edges_mid = np.array([(edges[i]+edges[i+1])/2 for i in range(len(edges)-1)])\n",
    "edges_mid_shuff = np.array([(edges_shuff[i]+edges_shuff[i+1])/2 for i in range(len(edges_shuff)-1)])\n",
    "fig,ax = plt.subplots(1,1,figsize=(7,5))\n",
    "ax.bar(edges_mid, count/len(GLM_Vis['mcc']),color='k',width=.01,alpha=.5, label='Test $R^2$')\n",
    "ax.bar(edges_mid_shuff, count_shuff/len(GLM_Vis['mcc']),color='r',width=.01,alpha=.5, label='Shuffled $R^2$')\n",
    "ax.set_xlabel('$R^2$')\n",
    "\n",
    "# ax.set_ylabel('Proportion')\n",
    "ax.set_xlim(-.01,.6)\n",
    "ax.set_ylim(0,.2)\n",
    "ax.legend(fontsize=12)\n",
    "\n",
    "# count,edges = np.histogram(GLM_Vis['mr2'],bins=np.arange(-.2,1,.05))\n",
    "# count_shuff,edges_shuff = np.histogram(GLM_Vis_shuff['mr2'],bins=np.arange(-.2,1,.05))\n",
    "# edges_mid = np.array([(edges[i]+edges[i+1])/2 for i in range(len(edges)-1)])\n",
    "# ax[1].bar(edges_mid, count/len(GLM_Vis['mr2']),color='k',width=.05,alpha=.5,label='Test $r^2$')\n",
    "# ax[1].bar(edges_mid, count_shuff/len(GLM_Vis['mr2']),color='r',width=.05,alpha=.5,label='Shuffled $r^2$')\n",
    "# ax[1].set_xlabel('Corr. Coef.')\n",
    "# ax[1].set_ylabel('Proportion')\n",
    "# ax[1].set_xlim(-.2,.75)\n",
    "# ax[1].set_ylim(0,.7)\n",
    "# ax[1].legend(fontsize=12)\n",
    "# plt.suptitle('model_dt={:03d}ms'.format(int(model_dt*1000)),fontsize=20)\n",
    "# plt.tight_layout()\n",
    "fig.savefig(FigPath/'R2_comparison_VisOnly_notsmooth_{}_dt{:03d}_T{:02d}.png'.format(model_type,int(model_dt*1000), nt_glm_lag), facecolor='white', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banner-result",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1\n",
    "model_n=-1\n",
    "celln=51\n",
    "# for modeln in range(len(titles)):\n",
    "traces = np.zeros((ncells,len(titles),len(quartiles)-1,stat_bins-1)) # (model_type,quartile,FR)\n",
    "traces_mean = np.zeros((ncells,len(titles),stat_bins-1)) # (model_type,quartile,FR)\n",
    "edges_all = np.zeros((ncells,len(titles),len(quartiles)-1,stat_bins-1)) # (model_type,quartile,FR)\n",
    "metric = move_test[:,modeln]\n",
    "nranges = np.quantile(metric,[0,.25,.5,.75,1])# np.linspace(np.nanmean(metric)-2*np.nanstd(metric), np.nanmean(metric)+2*np.nanstd(metric),N_bins)\n",
    "stat_all, edges, _ = binned_statistic(predcell,nspcell, statistic='mean',bins=pred_rangelin)\n",
    "edge_mids = np.array([(edges[i]+edges[i+1])/2 for i in range(len(edges)-1)])\n",
    "#     axs[1,modeln].set_xlim(0,pred_range[1]+np.std(pred_range))\n",
    "#     axs[1,modeln].set_ylim(0,np.max(stat)+np.std(stat))\n",
    "#     for n in range(len(nranges)-1):\n",
    "ind = np.where(((metric<=nranges[n+1])&(metric>nranges[n])))[0]\n",
    "pred = predcell[ind]\n",
    "sp = nspcell[ind]\n",
    "\n",
    "stat_range, edges, _ = binned_statistic(pred, sp, statistic='mean',bins=pred_rangelin)\n",
    "edge_mids = np.array([(edges[i]+edges[i+1])/2 for i in range(len(edges)-1)])\n",
    "traces_mean[celln,modeln]=stat_all\n",
    "\n",
    "traces[celln,modeln,n]=stat_range\n",
    "edges_all[celln,modeln,n]=edge_mids\n",
    "res_add = minimize_scalar(f_add,args=(stat_range,stat_all))\n",
    "res_mult = minimize_scalar(f_mult,args=(stat_range,stat_all))\n",
    "mse_add[celln, modeln, n] = res_add.fun\n",
    "mse_mult[celln, modeln, n] = res_mult.fun\n",
    "alpha_add[celln, modeln, n] = res_add.x\n",
    "alpha_mult[celln, modeln, n] = res_mult.x\n",
    "        \n",
    "#         axs[1,modeln].plot(edge_mids, stat_range,'.-', c=colors[n],label='{:.02f} : {:.02f}'.format(nranges[n],nranges[n+1]),lw=4,ms=20)\n",
    "#         axs[1,modeln].plot(np.linspace(pred_range[0],pred_range[1]),np.linspace(pred_range[0],pred_range[1]),'k--',zorder=0)\n",
    "#         axs[1,modeln].set_title('Metric: {}'.format(titles[modeln]), color=clrs[modeln])\n",
    "#         axs[1,modeln].set_xlabel('Predicted Spike Rate')\n",
    "#         axs[1,modeln].set_ylabel('Actual Spike Rate')\n",
    "#     axs[1,modeln].plot(edge_mids, stat_all,'.-', c='k', lw=5, ms=20, label='All_data')\n",
    "#     axs[1,modeln].legend(loc='upper left', fontsize=12)\n",
    "#     axs[1,modeln].axis('equal')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bridal-courtesy",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_add = minimize_scalar(f_add,args=(stat_range,stat_all),tol=1e-6)\n",
    "res_mult = minimize_scalar(f_mult,args=(stat_range,stat_all),tol=1e-6)\n",
    "mse_add[celln,modeln, n] = res_add.fun\n",
    "mse_mult[celln,modeln, n] = res_mult.fun\n",
    "alpha_add[celln,modeln, n] = res_add.x\n",
    "alpha_mult[celln,modeln, n] = res_mult.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceramic-burns",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_mult.x,alpha_mult[celln,modeln, n] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shared-tourism",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_add,res_mult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supreme-century",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_add[celln,modeln,n],alpha_mult[celln,modeln,n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "material-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_add[celln,modeln, n], mse_mult[celln,modeln, n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "married-oasis",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.plot(edge_mids,traces_mean[celln,modeln],'.-', c='k', lw=5, ms=20, label='All_data')\n",
    "plt.plot(edge_mids,(traces_mean[celln,modeln]*alpha_mult[celln,modeln,n]).T,'--', label='MultFit',c=colors[n],lw=4,ms=20)\n",
    "plt.plot(edge_mids,(traces_mean[celln,modeln]+alpha_add[celln,modeln,n]).T,'-.', label='AddFit', c=colors[n],lw=4,ms=20)\n",
    "plt.plot(edge_mids, traces[celln,modeln,n],'.-', c=colors[n],label='{:.02f} : {:.02f}'.format(nranges[n],nranges[n+1]),lw=4,ms=20)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subsequent-search",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.arange(-10,10,.001)\n",
    "mse_add = np.zeros(alphas.shape[0])\n",
    "mse_mult = np.zeros(alphas.shape[0])\n",
    "for ind,alpha in enumerate(alphas):\n",
    "    mse_add[ind] = np.mean((stat_range - stat_all+alpha)**2)\n",
    "    mse_mult[ind] = np.mean((stat_range - stat_all*alpha)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banned-state",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(mse_add),np.min(mse_mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binary-motion",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mse_add)\n",
    "plt.plot(mse_mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twenty-arctic",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(stat_all+alphas[np.argmin(mse_add)])\n",
    "plt.plot(stat_all+alphas[np.argmin(mse_mult)])\n",
    "plt.plot(stat_all, 'k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "objective-raise",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(5,5))\n",
    "crange = np.max(np.abs(min_add-min_mult))\n",
    "im = axs.imshow(min_add-min_mult,cmap='seismic',vmin=-crange,vmax=crange,origin='lower')\n",
    "axs.set_yticks(np.arange(0,4))\n",
    "axs.set_yticklabels(titles)\n",
    "axs.set_ylabel('Movement Model')\n",
    "axs.set_xticks(np.arange(0,4))\n",
    "axs.set_xticklabels(['.25','.5','.75','1'])\n",
    "axs.set_xlabel('Quantile Range')\n",
    "axs.set_title('$MSE_{add}$ - $MSE_{mult}$')\n",
    "cbar = add_colorbar(im)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incomplete-stream",
   "metadata": {},
   "source": [
    "### Animations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correct-upgrade",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "measured-trading",
   "metadata": {},
   "outputs": [],
   "source": [
    "GLM_Vis = ioh5.load(save_dir/'GLM_{}_Data_VisOnly_notsmooth_dt{:03d}_T{:02d}.h5'.format(model_type,int(model_dt*1000), nt_glm_lag))\n",
    "locals().update(GLM_Vis)\n",
    "celln=117\n",
    "msp_smooth=(np.convolve(msp[celln], np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "pred_smooth=(np.convolve(mpred[celln], np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "ymax = np.max(np.maximum(msp_smooth, pred_smooth))\n",
    "ymin = np.min(np.minimum(msp_smooth, pred_smooth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ultimate-apache",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf = 4\n",
    "model_vid = np.zeros((test_vid.shape[0],sf*test_vid.shape[1],sf*test_vid.shape[2]))\n",
    "for n in range(test_vid.shape[0]):\n",
    "    model_vid[n] = cv2.resize(test_vid[n],(sf*test_vid.shape[2],sf*test_vid.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apart-breakfast",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0=0\n",
    "dt=3000\n",
    "fig = make_subplots(rows=1, cols=2,)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=np.arange(t0,t0+dt)*model_dt,y=msp_smooth[t0:t0+dt], mode='lines', name='Test FR'),row=1,col=2)\n",
    "fig.add_trace(go.Scatter(x=np.arange(t0,t0+dt)*model_dt,y=pred_smooth[t0:t0+dt], mode='lines', name='Pred FR'),row=1,col=2)\n",
    "fig.add_trace(go.Scatter(x=[t0*model_dt, t0*model_dt], y=[ymin-1, ymax+1], mode='lines', line_color='red', line_width=2, showlegend=False),row=1,col=2)\n",
    "fig.add_trace(go.Heatmap(z=model_vid[t0], colorscale='gray', showscale=False),row=1, col=1)\n",
    "\n",
    "\n",
    "frames = []\n",
    "for t in np.arange(t0,t0+dt,10):\n",
    "    frames.append(dict(name='{}'.format(t),data=[go.Scatter(x=np.arange(t0,t0+dt)*model_dt,y=msp_smooth[t0:t0+dt],mode='lines',name='Test FR'),\n",
    "                                go.Scatter(x=np.arange(t0,t0+dt)*model_dt,y=pred_smooth[t0:t0+dt], mode='lines', name='Pred FR'),\n",
    "                                go.Scatter(x=[t*model_dt, t*model_dt], y=[ymin-1, ymax+1], mode='lines', line_color='red', line_width=2, showlegend=False),\n",
    "                                go.Heatmap(z=model_vid[t], colorscale='gray',showscale=False)\n",
    "                               ], \n",
    "                          traces=[0,1,2,3],\n",
    "                         )\n",
    "                )\n",
    "fig.frames=frames\n",
    "# Slider\n",
    "sliders = [{'yanchor': 'top',\n",
    "            'xanchor': 'left',\n",
    "            'currentvalue': {'font': {'size': 16}, 'prefix': 'Frame: ', 'visible': True, 'xanchor': 'right'},\n",
    "            'transition': {'duration': 0, 'easing': 'linear'},\n",
    "            'pad': {'b': 10, 't': 50},\n",
    "            'len': 0.9, 'x': 0.1, 'y': 0,\n",
    "            'steps': [{'args': [[k], {'frame': {'duration': 0, 'easing': 'linear', 'redraw': True},\n",
    "                                      'transition': {'duration': 0, 'easing': 'linear'}}],\n",
    "                       'label': '{}'.format(k), 'method': 'animate'} for k in np.arange(t0,t0+dt,10)\n",
    "                      ]}]\n",
    "\n",
    "\n",
    "axis_template = dict(autorange = True,\n",
    "             showgrid = False, zeroline = False,\n",
    "             linecolor = 'black', showticklabels = False,\n",
    "             ticks = '' )\n",
    "fig.update_layout(width=1000,\n",
    "                  height=500,\n",
    "                 xaxis=axis_template,\n",
    "                 yaxis=axis_template,\n",
    "                 sliders=sliders\n",
    "                 )\n",
    "fig.update_yaxes(autorange=\"reversed\",row=1,col=1)\n",
    "\n",
    "fig.show() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tribal-embassy",
   "metadata": {},
   "source": [
    "### Head Fixed Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frozen-serial",
   "metadata": {},
   "outputs": [],
   "source": [
    "celln = 25# np.argmax(mcc)\n",
    "bin_length=40\n",
    "\n",
    "fig, axs = plt.subplots(1,nt_glm_lag+1,figsize=(30,5))\n",
    "predcell = mpred[celln]/model_dt\n",
    "nspcell = msp[celln]/model_dt\n",
    "msp_smooth=(np.convolve(msp[celln], np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "pred_smooth=(np.convolve(mpred[celln], np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "axs[0].plot(msp_smooth,'k',label='test FR')\n",
    "axs[0].plot(pred_smooth,'r', label='pred FR')\n",
    "axs[0].set_xlabel('Frame #')\n",
    "axs[0].set_ylabel('Firing Rate (spks/s)')\n",
    "axs[0].legend()\n",
    "axs[0].set_title('Smoothed FRs')\n",
    "\n",
    "crange = np.max(np.abs(msta[celln]))\n",
    "for axn,n in enumerate(range(1,nt_glm_lag+1)):\n",
    "    img = axs[n].imshow(msta[celln,axn],cmap='RdBu_r',vmin=-crange,vmax=crange)\n",
    "    axs[n].axis('off')\n",
    "    axs[n].set_title('Lag: {}ms'.format(int(1000*lag_list[axn]*model_dt)))\n",
    "cbar = add_colorbar(img,) # orientation='horizontal',location='bottom'\n",
    "# cbar.set_ticks([-crange,crange])\n",
    "# cbar.set_ticklabels(['Dark','Light'])\n",
    "plt.suptitle('Celln:{}, cc={:.03f}, r2={:.03}'.format(celln,mcc[celln],mr2[celln]),y=.9,fontsize=20)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naughty-thumb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with PdfPages(FigPath/ 'GLM_{}_dt{:03d}_T{:02d}_cellsummary_sig.pdf'.format(model_type,int(model_dt*1000),nt_glm_lag)) as pdf:\n",
    "    for celln in tqdm(range(msp.shape[0])):\n",
    "        if mcc[celln]>.25:\n",
    "            fig, axs = plt.subplots(1,nt_glm_lag+1,figsize=(30,5))\n",
    "            predcell = mpred[celln]/model_dt\n",
    "            nspcell = msp[celln]/model_dt\n",
    "            msp_smooth=((np.convolve(msp[celln], np.ones(bin_length), 'same')) / (bin_length * model_dt))[bin_length:-bin_length]\n",
    "            pred_smooth=((np.convolve(mpred[celln], np.ones(bin_length), 'same')) / (bin_length * model_dt))[bin_length:-bin_length]\n",
    "            axs[0].plot(msp_smooth,'k',label='test FR')\n",
    "            axs[0].plot(pred_smooth,'r', label='pred FR')\n",
    "            axs[0].set_xlabel('Frame #')\n",
    "            axs[0].set_ylabel('Firing Rate (spks/s)')\n",
    "            axs[0].legend()\n",
    "            axs[0].set_title('Smoothed FRs')\n",
    "\n",
    "            crange = np.max(np.abs(msta[celln]))\n",
    "            for axn,n in enumerate(range(1,nt_glm_lag+1)):\n",
    "                img = axs[n].imshow(msta[celln,axn],cmap='RdBu_r',vmin=-crange,vmax=crange)\n",
    "                axs[n].axis('off')\n",
    "                axs[n].set_title('Lag: {}ms'.format(int(1000*lag_list[axn]*model_dt)))\n",
    "            cbar = add_colorbar(img,) # orientation='horizontal',location='bottom'\n",
    "            # cbar.set_ticks([-crange,crange])\n",
    "            # cbar.set_ticklabels(['Dark','Light'])\n",
    "            plt.suptitle('Celln:{}, cc={:.03f}, r2={:.03}'.format(celln,mcc[celln],mr2[celln]),y=.9,fontsize=20)\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            pdf.savefig()\n",
    "            plt.close()\n",
    "# fig.savefig(FigPath/'TemporalRF_N{}.pdf'.format(celln), facecolor='white', transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "characteristic-tulsa",
   "metadata": {},
   "source": [
    "## Modulate FR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "variable-ethernet",
   "metadata": {},
   "outputs": [],
   "source": [
    "def consecutive(data, stepsize=1):\n",
    "    return np.split(data, np.where(np.diff(data) != stepsize)[0]+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "public-japan",
   "metadata": {},
   "outputs": [],
   "source": [
    "celln = 121\n",
    "predcell = mpred[celln]/model_dt\n",
    "nspcell = msp[celln]/model_dt\n",
    "msp_smooth=(np.convolve(msp[celln], np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "pred_smooth=(np.convolve(mpred[celln], np.ones(bin_length), 'same')) / (bin_length * model_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlikely-palace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modeln = 0\n",
    "modeln = 0#np.argmax(tc_mod[celln])\n",
    "print('Metric:',titles[modeln])\n",
    "metric = move_test[:,modeln]\n",
    "nranges = np.quantile(metric,quartiles)\n",
    "stat_range, edges, _ = binned_statistic(metric ,test_nsp[:,celln], statistic='mean',bins=nranges)\n",
    "edge_mids = np.quantile(metric,spk_percentile2)#np.array([(edges[i]+edges[i+1])/2 for i in range(len(edges)-1)])\n",
    "# for m in range(len(nranges)-1):\n",
    "#     axs[1,1].axvspan(nranges[m], nranges[m+1],ymin=i*1/2,ymax=(i+1)*1/2,alpha=0.8, color=colors[m],zorder=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dirty-hundred",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unravel_index(np.argmax(dmodel),dmodel.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minor-blanket",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outer-booking",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_mult[celln,modeln]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "announced-short",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tc_mod = (np.max(tuning_curves,axis=-1,keepdims=True)-np.min(tuning_curves,axis=-1,keepdims=True))/(np.min(tuning_curves,axis=-1,keepdims=True)+np.max(tuning_curves,axis=-1,keepdims=True))\n",
    "\n",
    "avg_fr = np.mean(tuning_curves,axis=-1).squeeze()\n",
    "\n",
    "thresh_fr = 2\n",
    "tuning_sig = tc_mod.copy()\n",
    "tuning_sig[avg_fr[:,modeln]<thresh_fr] = 0\n",
    "dmodel[avg_fr[:,modeln]<thresh_fr]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approved-gentleman",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_sig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improved-acquisition",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(np.mean(tuning_sig,axis=(-1,-2))),tuning_sig[np.argmax(tuning_sig[:,0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passive-christopher",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = bin_length\n",
    "dt = len(msp_smooth)-2*bin_length\n",
    "celln = 119\n",
    "predcell = mpred[celln]/model_dt\n",
    "nspcell = msp[celln]/model_dt\n",
    "msp_smooth=(np.convolve(msp[celln], np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "pred_smooth=(np.convolve(mpred[celln], np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "fig, ax = plt.subplots(figsize=(15,5))\n",
    "ax.plot(msp_smooth[t:t+dt], color='k',lw=3,label='Test')\n",
    "ax.plot(pred_smooth[t:t+dt], color='r',lw=3,label='Pred')\n",
    "pred_smooth_mult = pred_smooth.copy()\n",
    "pred_mult = mpred[celln].copy()\n",
    "for modeln in range(len(titles)):\n",
    "    metric = move_test[:,modeln]\n",
    "    nranges = np.quantile(metric,quartiles)\n",
    "    stat_range, edges, _ = binned_statistic(metric ,test_nsp[:,celln], statistic='mean',bins=nranges)\n",
    "    metric2 = metric[t:t+dt]\n",
    "    for i in range(len(edges)-1):    \n",
    "        metric_bounds = consecutive(np.where((metric2>edges[i])&(metric2<edges[i+1]))[0])\n",
    "        metric_bounds = [row for row in metric_bounds]\n",
    "        metric_length = [len(row) for row in metric_bounds]\n",
    "    #     print(i,np.sum(metric_length))\n",
    "        # n=1\n",
    "    #     plt.axvspan(metric_bounds[n][0],metric_bounds[n][-1], color=colors[i],zorder=0)\n",
    "    #     print(metric_bounds[n])\n",
    "        for n, row in enumerate(metric_bounds):\n",
    "#             ax.axvspan(metric_bounds[n][0],metric_bounds[n][-1], color=colors[i], zorder=0)\n",
    "            pred_smooth_mult[row] = alpha_mult[celln,modeln,i]*pred_smooth_mult[row]\n",
    "            pred_mult[row] = alpha_mult[celln,modeln,i]*pred_mult[row]\n",
    "# ax.plot(pred_smooth_mult[t:t+dt],'g',lw=3,zorder=1,label='Modulated')\n",
    "ax.plot(((np.convolve(pred_mult, np.ones(bin_length), 'same')) / (bin_length * model_dt))[t:t+dt],'g',lw=3,zorder=1,label='Modulated')\n",
    "\n",
    "cc_mult = np.corrcoef(msp_smooth[bin_length:-bin_length],pred_smooth_mult[bin_length:-bin_length])[0,1]\n",
    "ax.set_title('metric:{}, $R^2_o$:{:.03f}, $R^2_m$:{:.03f}'.format(titles[modeln],mcc[celln]**2,cc_mult**2))\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "drawn-occurrence",
   "metadata": {},
   "source": [
    "## Plots for Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "german-talent",
   "metadata": {},
   "outputs": [],
   "source": [
    "lag=150 # in ms\n",
    "nt_glm_lag = 5\n",
    "# minlag = int(-lag//(1000*model_dt)); maxlag=int((lag//(1000*model_dt))+1)\n",
    "lag_list = np.array([-1,0,1,2,3]) #,np.arange(minlag,maxlag,np.floor((maxlag-minlag)/nt_glm_lag).astype(int))\n",
    "nt_glm_lag = len(lag_list)\n",
    "GLM_Vis = ioh5.load(save_dir/'GLM_{}_Data_VisOnly_notsmooth_dt{:03d}_T{:02d}.h5'.format(model_type,int(model_dt*1000), nt_glm_lag))\n",
    "locals().update(GLM_Vis)\n",
    "data, train_idx, test_idx = load_train_test(file_dict, save_dir, model_dt=model_dt, do_shuffle=do_shuffle, do_norm=False,free_move=free_move, has_imu=free_move, has_mouse=False)\n",
    "locals().update(data)\n",
    "##### Explore Neurons #####\n",
    "# Initialize movement combinations\n",
    "titles = np.array(['th','phi','roll','pitch']) # 'dg_p','dg_n' 'roll','pitch' 'dg_p','dg_n']) #\n",
    "titles_all = []\n",
    "for n in range(1,len(titles)+1):\n",
    "    perms = np.array(list(itertools.combinations(np.arange(len(titles)), n)))\n",
    "    for ind in range(perms.shape[0]):\n",
    "        titles_all.append('_'.join([t for t in titles[perms[ind]]]))\n",
    "\n",
    "# train_dgaze_p = train_dth + np.diff(train_gz,append=0)\n",
    "# train_dgaze_n = train_dth - np.diff(train_gz,append=0)\n",
    "# test_dgaze_p = test_dth + np.diff(test_gz,append=0)\n",
    "# test_dgaze_n = test_dth - np.diff(test_gz,append=0)\n",
    "# move_train = np.hstack((train_th[:,np.newaxis],train_phi[:,np.newaxis],train_roll[:,np.newaxis],train_pitch[:,np.newaxis], train_dth[:,np.newaxis],train_dphi[:,np.newaxis]))\n",
    "move_test = np.hstack((test_th[:,np.newaxis],test_phi[:,np.newaxis],test_roll[:,np.newaxis],test_pitch[:,np.newaxis])) #,test_dgaze_p[:,np.newaxis],test_dgaze_n[:,np.newaxis]))#\n",
    "# move_test = np.hstack((test_th[:,np.newaxis],test_phi[:,np.newaxis],test_dgaze_p[:,np.newaxis],test_dgaze_n[:,np.newaxis]))# test_dth[:,np.newaxis],test_dphi[:,np.newaxis]))\n",
    "move_test = move_test - np.mean(move_test,axis=0)\n",
    "# Create all tuning curves for plotting\n",
    "N_bins=10\n",
    "ncells = model_nsp.shape[-1]\n",
    "ax_ylims = np.zeros((model_nsp.shape[-1],len(titles)))\n",
    "tuning_curves = np.zeros((model_nsp.shape[-1],len(titles),N_bins-1))\n",
    "tuning_stds = np.zeros((model_nsp.shape[-1],len(titles),N_bins-1))\n",
    "var_ranges = np.zeros((len(titles),N_bins-1))\n",
    "for modeln in range(len(titles)):\n",
    "    metric = move_test[:,modeln]\n",
    "    tuning, tuning_std, var_range = tuning_curve(test_nsp, metric, N_bins=N_bins, model_dt=model_dt)\n",
    "    tuning_curves[:,modeln] = tuning\n",
    "    tuning_stds[:,modeln] = tuning_std\n",
    "    ax_ylims[:,modeln] = np.max(tuning,axis=1)\n",
    "    var_ranges[modeln] = var_range\n",
    "\n",
    "quartiles = np.arange(0,1.25,.25)\n",
    "mse_add = np.zeros((ncells,len(titles),len(quartiles)-1))\n",
    "mse_mult = np.zeros((ncells,len(titles),len(quartiles)-1))\n",
    "alpha_add = np.zeros((ncells,len(titles),len(quartiles)-1))\n",
    "alpha_mult = np.zeros((ncells,len(titles),len(quartiles)-1))\n",
    "\n",
    "traces = np.zeros((ncells,len(titles),len(quartiles)-1,stat_bins-1)) # (model_type,quartile,FR)\n",
    "traces_mean = np.zeros((ncells,len(titles),stat_bins-1)) # (model_type,quartile,FR)\n",
    "edges_all = np.zeros((ncells,len(titles),len(quartiles)-1,stat_bins-1)) # (model_type,quartile,FR)\n",
    "for celln in tqdm(range(msp.shape[0])):\n",
    "\n",
    "        predcell = mpred[celln]/model_dt\n",
    "        nspcell = msp[celln]/model_dt\n",
    "        msp_smooth=(np.convolve(msp[celln], np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "        pred_smooth=(np.convolve(mpred[celln], np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "\n",
    "\n",
    "        # axs[1,2].legend(bbox_to_anchor=(1.01, 1), fontsize=12)\n",
    "\n",
    "        # Set up predicted spike range between 1-99th percentile\n",
    "        stat_bins = 5\n",
    "        pred_range = np.quantile(predcell,[.1,.9])\n",
    "        msp_range = np.quantile(nspcell,[.01,1])\n",
    "        spike_percentiles = np.arange(0,1.25,.25)\n",
    "        spike_percentiles[-1]=.99\n",
    "        spk_percentile2 = np.arange(.125,1.125,.25)\n",
    "        pred_rangelin = np.quantile(predcell,spike_percentiles)\n",
    "        xbin_pts = np.quantile(predcell,spk_percentile2)\n",
    "        stat_bins = len(pred_rangelin) #5\n",
    "        # pred_rangelin = np.linspace(pred_range[0],pred_range[1],stat_bins)\n",
    "\n",
    "        for modeln in range(len(titles)):\n",
    "            metric = move_test[:,modeln]\n",
    "            nranges = np.quantile(var_ranges[modeln],quartiles)# np.linspace(np.nanmean(metric)-2*np.nanstd(metric), np.nanmean(metric)+2*np.nanstd(metric),N_bins)\n",
    "            stat_all, edges, _ = binned_statistic(predcell,nspcell, statistic='mean',bins=pred_rangelin)\n",
    "            edge_mids = xbin_pts#np.array([(edges[i]+edges[i+1])/2 for i in range(len(edges)-1)])\n",
    "            traces_mean[celln,modeln]=stat_all\n",
    "            max_fr = np.max(stat_all)\n",
    "\n",
    "            for n in range(len(nranges)-1):\n",
    "                ind = np.where(((metric<=nranges[n+1])&(metric>nranges[n])))[0]\n",
    "                pred = predcell[ind]\n",
    "                sp = nspcell[ind]\n",
    "\n",
    "                stat_range, edges, _ = binned_statistic(pred, sp, statistic='mean',bins=pred_rangelin)\n",
    "                edge_mids = xbin_pts #np.array([(edges[i]+edges[i+1])/2 for i in range(len(edges)-1)])\n",
    "                traces[celln,modeln,n]=stat_range\n",
    "                edges_all[celln,modeln,n]=edge_mids\n",
    "                res_add = minimize_scalar(f_add,args=(stat_range/max_fr, stat_all/max_fr))\n",
    "                res_mult = minimize_scalar(f_mult,args=(stat_range/max_fr, stat_all/max_fr))\n",
    "                mse_add[celln, modeln, n] = res_add.fun\n",
    "                mse_mult[celln, modeln, n] = res_mult.fun\n",
    "                alpha_add[celln, modeln, n] = res_add.x\n",
    "                alpha_mult[celln, modeln, n] = res_mult.x\n",
    "\n",
    "dmodel = mse_add-mse_mult\n",
    "\n",
    "# fig.savefig(FigPath/'CellSummary_N{}.png'.format(celln), facecolor='white', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "provincial-blair",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appreciated-binding",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "herbal-minimum",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot((tuning_curves[51,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attended-windows",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import find_peaks,find_peaks_cwt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sixth-dating",
   "metadata": {},
   "outputs": [],
   "source": [
    "pk_info = find_peaks_cwt(tuning_curves[51,3],widths=[1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identified-metropolitan",
   "metadata": {},
   "outputs": [],
   "source": [
    "pk_info,tuning_curves[51,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepared-wallet",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_index = (tuning_curves[:,:,-1] - tuning_curves[:,:,0])/np.max(np.abs(tuning_curves),axis=-1)\n",
    "mod_index[34]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animated-plumbing",
   "metadata": {},
   "source": [
    "### Freely Moving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "living-pavilion",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dt=.05\n",
    "lag=150 # in ms\n",
    "nt_glm_lag = 5\n",
    "# minlag = int(-lag//(1000*model_dt)); maxlag=int((lag//(1000*model_dt))+1)\n",
    "lag_list = np.array([-1,0,1,2,3]) #,np.arange(minlag,maxlag,np.floor((maxlag-minlag)/nt_glm_lag).astype(int))\n",
    "nt_glm_lag = len(lag_list)\n",
    "print(lag_list,1000*lag_list*model_dt)\n",
    "do_shuffle=False\n",
    "model_type = 'poissonregressor'\n",
    "\n",
    "# for model_dt in [.025,.05,.1]:\n",
    "GLM_Vis = ioh5.load(save_dir/'GLM_{}_Data_VisOnly_notsmooth_dt{:03d}_T{:02d}.h5'.format(model_type,int(model_dt*1000), nt_glm_lag))\n",
    "locals().update(GLM_Vis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "backed-stable",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf = 4\n",
    "msta_up = np.zeros((msta.shape[0],msta.shape[1],sf*msta.shape[-2],sf*msta.shape[-1]))\n",
    "for n in range(msta.shape[0]):\n",
    "    for t in range(msta.shape[1]):\n",
    "        msta_up[n,t] = cv2.resize(msta[n,t],(sf*msta.shape[-1],sf*msta.shape[-2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiovascular-reform",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "posted-headline",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brave-implementation",
   "metadata": {},
   "outputs": [],
   "source": [
    "celln= 126 #np.argmax(mcc)\n",
    "bin_length=40\n",
    "colors = plt.cm.cool(np.linspace(0,1,4))\n",
    "clrs = ['blue','orange','green','red']\n",
    "# quartiles = np.arange(.1,1,.2)#[0,.25,.5,.75,1]\n",
    "quartiles = np.arange(0,1.25,.25)\n",
    "lag=150 # in ms\n",
    "nt_glm_lag = 5\n",
    "# # minlag = int(-lag//(1000*model_dt)); maxlag=int((lag//(1000*model_dt))+1)\n",
    "lag_list = np.array([-1,0,1,2,3]) #,np.arange(minlag,maxlag,np.floor((maxlag-minlag)/nt_glm_lag).astype(int))\n",
    "nt_glm_lag = len(lag_list)\n",
    "\n",
    "fig, axs = plt.subplots(1,1, figsize=((15,5))) \n",
    "\n",
    "predcell = mpred[celln]/model_dt\n",
    "nspcell = msp[celln]/model_dt\n",
    "msp_smooth=(np.convolve(msp[celln], np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "pred_smooth=(np.convolve(mpred[celln], np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "axs.plot(np.arange(len(msp_smooth))*model_dt,msp_smooth,'k',lw=3, label='Test FR')\n",
    "axs.plot(np.arange(len(pred_smooth))*model_dt,pred_smooth,'r',lw=3, label='Pred. FR')\n",
    "axs.set_xlabel('Time (s)')\n",
    "axs.set_ylabel('Firing Rate (spks/s)')\n",
    "axs.set_xticks(np.arange(0,len(pred_smooth)*model_dt,30))\n",
    "axs.legend()\n",
    "# axs.set_title('Smoothed FRs')\n",
    "axs.set_title('$R^2$={:.02f}'.format(GLM_Vis['mcc'][celln]**2))\n",
    "plt.tight_layout()\n",
    "# fig.savefig(FigPath/'Example_TestPred_{}_Cell{}.png'.format(stim_type,celln), facecolor='white', transparent=True)\n",
    "\n",
    "fig, axs = plt.subplots(1,nt_glm_lag, figsize=((20,5))) \n",
    "crange = np.max(np.abs(msta[celln]))\n",
    "for n in range(nt_glm_lag):\n",
    "    img = axs[n].imshow(msta_up[celln,n],cmap='RdBu_r',vmin=-crange,vmax=crange)\n",
    "    axs[n].axis('off')\n",
    "    axs[n].set_title('Lag:{:03d} ms'.format(int(1000*lag_list[n]*model_dt)))\n",
    "    axs[n].axis('off')\n",
    "cbar = add_colorbar(img)\n",
    "cbar.set_ticks([-crange,crange])\n",
    "cbar.set_ticklabels(['Dark','Light'])\n",
    "plt.tight_layout()\n",
    "# fig.savefig(FigPath/'Example_STA_temporal_{}_Cell{}.png'.format(stim_type,celln), facecolor='white', transparent=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informal-strain",
   "metadata": {},
   "outputs": [],
   "source": [
    "celln = 21\n",
    "modeln=0\n",
    "predcell = mpred[celln]/model_dt\n",
    "nspcell = msp[celln]/model_dt\n",
    "fig, axs = plt.subplots(1,2,figsize=(12,5))\n",
    "# Eye Tuning Curve\n",
    "ax = axs[0]\n",
    "top_yaxs = np.max(ax_ylims[celln])+2*np.max(tuning_stds[celln])\n",
    "metric = move_test[:,modeln]\n",
    "nranges = np.quantile(metric,quartiles)\n",
    "stat_range, edges, _ = binned_statistic(metric,test_nsp[:,celln],statistic='mean',bins=nranges)\n",
    "edge_mids = np.quantile(metric,spk_percentile2)#np.array([(edges[i]+edges[i+1])/2 for i in range(len(edges)-1)])\n",
    "cmap = mpl.colors.ListedColormap(colors, N=colors.shape[0])\n",
    "norm = mpl.colors.BoundaryNorm(boundaries=np.floor(nranges), ncolors=len(cmap.colors))\n",
    "for m in range(len(nranges)-1):\n",
    "    ax.axvspan(nranges[m], nranges[m+1],alpha=0.8, color=colors[m],zorder=0)\n",
    "#     axs[1,1].errorbar(var_ranges[modeln],tuning_curves[celln,modeln], yerr=tuning_stds[celln,modeln],label=titles[modeln],c=clrs[modeln],lw=4,elinewidth=3)\n",
    "ax.plot(edge_mids,stat_range/model_dt,'.-', ms=20, lw=4,c=clrs[modeln])\n",
    "ax.set_ylim(bottom=0,top=np.max(ax_ylims,axis=1)[celln]+2*np.max(tuning_stds,axis=(1,2))[celln])\n",
    "ax.set_xlim(-30,30)\n",
    "ax.set_xlabel('Angle ($ ^{\\degree}$)')\n",
    "ax.set_ylabel('Spikes/s')\n",
    "ax.set_title('Eye Tuning Curves')\n",
    "lines = ax.get_lines()\n",
    "legend1 = ax.legend([lines[0]],['th'],bbox_to_anchor=(1.01, .2), fontsize=12)\n",
    "# legend2 = axs.legend([lines[1]],['phi'],bbox_to_anchor=(1.01, 1), fontsize=12)\n",
    "ax.add_artist(legend1)\n",
    "\n",
    "spike_percentiles = np.arange(0,1.25,.25)\n",
    "spike_percentiles[-1]=.99\n",
    "spk_percentile2 = np.arange(.125,1.125,.25)\n",
    "pred_rangelin = np.quantile(predcell,spike_percentiles)\n",
    "xbin_pts = np.quantile(predcell,spk_percentile2)\n",
    "stat_bins = len(pred_rangelin) #5\n",
    "\n",
    "traces = np.zeros((ncells,len(titles),len(quartiles)-1,stat_bins-1)) # (model_type,quartile,FR)\n",
    "traces_mean = np.zeros((ncells,len(titles),stat_bins-1)) # (model_type,quartile,FR)\n",
    "edges_all = np.zeros((ncells,len(titles),len(quartiles)-1,stat_bins-1)) # (model_type,quartile,FR)\n",
    "\n",
    "mse_add = np.zeros((ncells,len(titles),len(quartiles)-1))\n",
    "mse_mult = np.zeros((ncells,len(titles),len(quartiles)-1))\n",
    "alpha_add = np.zeros((ncells,len(titles),len(quartiles)-1))\n",
    "alpha_mult = np.zeros((ncells,len(titles),len(quartiles)-1))\n",
    "\n",
    "\n",
    "metric = move_test[:,modeln]\n",
    "nranges = np.quantile(var_ranges[modeln],quartiles)# np.linspace(np.nanmean(metric)-2*np.nanstd(metric), np.nanmean(metric)+2*np.nanstd(metric),N_bins)\n",
    "stat_all, edges, _ = binned_statistic(predcell,nspcell, statistic='mean',bins=pred_rangelin)\n",
    "edge_mids = xbin_pts#np.array([(edges[i]+edges[i+1])/2 for i in range(len(edges)-1)])\n",
    "traces_mean[celln,modeln]=stat_all\n",
    "ax = axs[1]\n",
    "max_fr = np.max(stat_all)\n",
    "for n in range(len(nranges)-1):\n",
    "    ind = np.where(((metric<=nranges[n+1])&(metric>nranges[n])))[0]\n",
    "    pred = predcell[ind]\n",
    "    sp = nspcell[ind]\n",
    "    stat_range, edges, _ = binned_statistic(pred, sp, statistic='mean',bins=pred_rangelin)\n",
    "    edge_mids = xbin_pts #np.array([(edges[i]+edges[i+1])/2 for i in range(len(edges)-1)])\n",
    "    traces[celln,modeln,n]=stat_range\n",
    "    edges_all[celln,modeln,n]=edge_mids\n",
    "    res_add = minimize_scalar(f_add,args=(stat_range/max_fr, stat_all/max_fr))\n",
    "    res_mult = minimize_scalar(f_mult,args=(stat_range/max_fr, stat_all/max_fr))\n",
    "    mse_add[celln, modeln, n] = res_add.fun\n",
    "    mse_mult[celln, modeln, n] = res_mult.fun\n",
    "    alpha_add[celln, modeln, n] = res_add.x\n",
    "    alpha_mult[celln, modeln, n] = res_mult.x\n",
    "\n",
    "    ax.plot(edge_mids, stat_range,'.-', c=colors[n],label='{:d} : {:d}'.format(int(nranges[n]),int(nranges[n+1])),lw=4,ms=20,alpha=.9)\n",
    "    ax.set_title('Metric: {}'.format(titles[modeln]), color=clrs[modeln])\n",
    "    ax.set_xlabel('Predicted Spike Rate')\n",
    "    ax.set_ylabel('Actual Spike Rate')\n",
    "\n",
    "lim_max = np.max(traces[celln,modeln])+np.std(traces_mean[celln,modeln])\n",
    "lim_min = np.min(traces[celln,modeln])-np.std(traces_mean[celln,modeln])\n",
    "lims = (0, lim_max) if (lim_min)<0 else (lim_min,lim_max) \n",
    "ax.plot(np.linspace(lims[0],lims[1]),np.linspace(lims[0],lims[1]),'k--',zorder=0)\n",
    "ax.plot(edge_mids, stat_all,'.-', c='k', lw=5, ms=20, label='All_data', alpha=.8)\n",
    "ax.legend(bbox_to_anchor=(1.01, .8), fontsize=12)\n",
    "ax.set(xlim=lims, ylim=lims)\n",
    "ax.set_xticks(np.arange(0,lims[-1],10))\n",
    "plt.tight_layout()\n",
    "# fig.savefig(FigPath/'Example_mult{}.png'.format(stim_type), facecolor='white', transparent=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selected-drain",
   "metadata": {},
   "outputs": [],
   "source": [
    "celln = 121\n",
    "modeln=0\n",
    "quartiles = np.arange(0,1.25,.25)\n",
    "angn=4\n",
    "\n",
    "predcell = mpred[celln]/model_dt\n",
    "nspcell = msp[celln]/model_dt\n",
    "fig, axs = plt.subplots(1,2,figsize=(12,5))\n",
    "\n",
    "\n",
    "# Eye Tuning Curve\n",
    "top_yaxs = np.max(ax_ylims[celln])+2*np.max(tuning_stds[celln])\n",
    "metric = move_test[:,modeln]\n",
    "nranges = np.quantile(var_ranges[modeln],quartiles)\n",
    "cmap = mpl.colors.ListedColormap(colors, N=colors.shape[0])\n",
    "norm = mpl.colors.BoundaryNorm(boundaries=np.floor(nranges), ncolors=cmap.N)\n",
    "ax = axs[0]\n",
    "img = ax.imshow(np.round(nranges,decimals=1)[None,:angn],aspect='auto', origin='lower',cmap=cmap,norm=norm,extent=(nranges[0],nranges[angn],0,top_yaxs),alpha=.8)\n",
    "ax.errorbar(var_ranges[modeln],tuning_curves[celln,modeln], yerr=tuning_stds[celln,modeln],label=titles[modeln],c=clrs[modeln],lw=4,elinewidth=3)\n",
    "ax.set_ylim(bottom=0,top=np.max(ax_ylims,axis=1)[celln]+2*np.max(tuning_stds,axis=(1,2))[celln])\n",
    "ax.set_xlim(-50,50)\n",
    "ax.set_xlabel('Angle ($ ^{\\degree}$)')\n",
    "ax.set_ylabel('Spikes/s')\n",
    "ax.set_title('Eye Tuning Curves')\n",
    "ax.legend(bbox_to_anchor=(1.01, .2), fontsize=12)\n",
    "\n",
    "# lines = ax.get_lines()\n",
    "# legend1 = ax.legend([lines[0]],titles[0],bbox_to_anchor=(1.01, .2), fontsize=12)\n",
    "# legend2 = axs.legend([lines[1]],['phi'],bbox_to_anchor=(1.01, 1), fontsize=12)\n",
    "# ax.add_artist(legend1)\n",
    "\n",
    "spike_percentiles = np.arange(0,1.25,.25)\n",
    "spike_percentiles[-1]=.99\n",
    "spk_percentile2 = np.arange(.125,1.125,.25)\n",
    "pred_rangelin = np.quantile(predcell,spike_percentiles)\n",
    "xbin_pts = np.quantile(predcell,spk_percentile2)\n",
    "stat_bins = len(pred_rangelin) #5\n",
    "\n",
    "traces = np.zeros((ncells,len(titles),len(quartiles)-1,stat_bins-1)) # (model_type,quartile,FR)\n",
    "traces_mean = np.zeros((ncells,len(titles),stat_bins-1)) # (model_type,quartile,FR)\n",
    "edges_all = np.zeros((ncells,len(titles),len(quartiles)-1,stat_bins-1)) # (model_type,quartile,FR)\n",
    "\n",
    "mse_add = np.zeros((ncells,len(titles),len(quartiles)-1))\n",
    "mse_mult = np.zeros((ncells,len(titles),len(quartiles)-1))\n",
    "alpha_add = np.zeros((ncells,len(titles),len(quartiles)-1))\n",
    "alpha_mult = np.zeros((ncells,len(titles),len(quartiles)-1))\n",
    "\n",
    "\n",
    "metric = move_test[:,modeln]\n",
    "nranges = np.quantile(var_ranges[modeln],quartiles)# np.linspace(np.nanmean(metric)-2*np.nanstd(metric), np.nanmean(metric)+2*np.nanstd(metric),N_bins)\n",
    "stat_all, edges, _ = binned_statistic(predcell,nspcell, statistic='mean',bins=pred_rangelin)\n",
    "edge_mids = xbin_pts#np.array([(edges[i]+edges[i+1])/2 for i in range(len(edges)-1)])\n",
    "traces_mean[celln,modeln]=stat_all\n",
    "ax = axs[1]\n",
    "max_fr = np.max(stat_all)\n",
    "for n in range(angn):\n",
    "    ind = np.where(((metric<=nranges[n+1])&(metric>nranges[n])))[0]\n",
    "    pred = predcell[ind]\n",
    "    sp = nspcell[ind]\n",
    "    stat_range, edges, _ = binned_statistic(pred, sp, statistic='mean',bins=pred_rangelin)\n",
    "    edge_mids = xbin_pts #np.array([(edges[i]+edges[i+1])/2 for i in range(len(edges)-1)])\n",
    "    traces[celln,modeln,n]=stat_range\n",
    "    edges_all[celln,modeln,n]=edge_mids\n",
    "    res_add = minimize_scalar(f_add,args=(stat_range/max_fr, stat_all/max_fr))\n",
    "    res_mult = minimize_scalar(f_mult,args=(stat_range/max_fr, stat_all/max_fr))\n",
    "    mse_add[celln, modeln, n] = res_add.fun\n",
    "    mse_mult[celln, modeln, n] = res_mult.fun\n",
    "    alpha_add[celln, modeln, n] = res_add.x\n",
    "    alpha_mult[celln, modeln, n] = res_mult.x\n",
    "    ax.plot(edge_mids, stat_range,'.-', c=colors[n],label='{:d}$\\degree$ : {:d}$\\degree$'.format(int(nranges[n]),int(nranges[n+1])),lw=4,ms=20,alpha=.9)\n",
    "ax.set_title('Metric: {}'.format(titles[modeln]), color=clrs[modeln])\n",
    "ax.set_xlabel('Predicted Spike Rate')\n",
    "ax.set_ylabel('Actual Spike Rate')\n",
    "\n",
    "lim_max = np.max(traces[celln,modeln])+np.std(traces_mean[celln,modeln])\n",
    "lim_min = np.min(traces[celln,modeln])-np.std(traces_mean[celln,modeln])\n",
    "lims = (0, lim_max) if (lim_min)<0 else (lim_min,lim_max) \n",
    "ax.plot(np.linspace(lims[0],lims[1]),np.linspace(lims[0],lims[1]),'k--',zorder=0)\n",
    "ax.plot(edge_mids, stat_all,'.-', c='k', lw=5, ms=20, label='All_data', alpha=.8)\n",
    "ax.legend(bbox_to_anchor=(1.01, .8), fontsize=12)\n",
    "ax.set(xlim=lims, ylim=lims)\n",
    "ax.set_xticks(np.arange(0,lims[-1],10))\n",
    "plt.tight_layout()\n",
    "# fig.savefig(FigPath/'Example_mult_{}_midang4.png'.format(stim_type), facecolor='white', transparent=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elder-trainer",
   "metadata": {},
   "outputs": [],
   "source": [
    "angn=1\n",
    "spike_percentiles = np.arange(0,1.25,.25)\n",
    "spike_percentiles[-1]=.99\n",
    "spk_percentile2 = np.arange(.125,1.125,.25)\n",
    "pred_rangelin = np.quantile(predcell,spike_percentiles)\n",
    "xbin_pts = np.quantile(predcell,spk_percentile2)\n",
    "stat_bins = len(pred_rangelin) #5\n",
    "\n",
    "traces = np.zeros((len(titles),len(quartiles)-1,stat_bins-1)) # (model_type,quartile,FR)\n",
    "traces_mean = np.zeros((len(titles),stat_bins-1)) # (model_type,quartile,FR)\n",
    "edges_all = np.zeros((len(titles),len(quartiles)-1,stat_bins-1)) # (model_type,quartile,FR)\n",
    "\n",
    "mse_add = np.zeros((len(titles),len(quartiles)-1))\n",
    "mse_mult = np.zeros((len(titles),len(quartiles)-1))\n",
    "alpha_add = np.zeros((len(titles),len(quartiles)-1))\n",
    "alpha_mult = np.zeros((len(titles),len(quartiles)-1))\n",
    "\n",
    "\n",
    "metric = move_test[:,modeln]\n",
    "nranges = np.quantile(var_ranges[modeln],quartiles)# np.linspace(np.nanmean(metric)-2*np.nanstd(metric), np.nanmean(metric)+2*np.nanstd(metric),N_bins)\n",
    "stat_all, edges, _ = binned_statistic(predcell,nspcell, statistic='mean',bins=pred_rangelin)\n",
    "edge_mids = xbin_pts#np.array([(edges[i]+edges[i+1])/2 for i in range(len(edges)-1)])\n",
    "traces_mean[modeln]=stat_all\n",
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "max_fr = np.max(stat_all)\n",
    "for n in range(angn):\n",
    "    ind = np.where(((metric<=nranges[n+1])&(metric>nranges[n])))[0]\n",
    "    pred = predcell[ind]\n",
    "    sp = nspcell[ind]\n",
    "    stat_range, edges, _ = binned_statistic(pred, sp, statistic='mean',bins=pred_rangelin)\n",
    "    edge_mids = xbin_pts #np.array([(edges[i]+edges[i+1])/2 for i in range(len(edges)-1)])\n",
    "    traces[modeln,n]=stat_range\n",
    "    edges_all[modeln,n]=edge_mids\n",
    "    res_add = minimize_scalar(f_add,args=(stat_range/max_fr, stat_all/max_fr))\n",
    "    res_mult = minimize_scalar(f_mult,args=(stat_range/max_fr, stat_all/max_fr))\n",
    "    mse_add[modeln, n] = res_add.fun\n",
    "    mse_mult[modeln, n] = res_mult.fun\n",
    "    alpha_add[modeln, n] = res_add.x\n",
    "    alpha_mult[modeln, n] = res_mult.x\n",
    "    ax.plot(edge_mids, stat_range,'.-', c=colors[n],label='{:d}$\\degree$ : {:d}$\\degree$'.format(int(nranges[n]),int(nranges[n+1])),lw=4,ms=20,alpha=.9)\n",
    "ax.set_title('Metric: {}'.format(titles[modeln]), color=clrs[modeln])\n",
    "ax.set_xlabel('Predicted Spike Rate')\n",
    "ax.set_ylabel('Actual Spike Rate')\n",
    "\n",
    "lim_max = np.max(traces[modeln])+np.std(traces_mean[modeln])\n",
    "lim_min = np.min(traces[modeln])-np.std(traces_mean[modeln])\n",
    "lims = (0, lim_max) if (lim_min)<0 else (lim_min,lim_max) \n",
    "ax.plot(np.linspace(lims[0],lims[1]),np.linspace(lims[0],lims[1]),'k--',zorder=0)\n",
    "ax.plot(edge_mids, stat_all,'.-', c='k', lw=5, ms=20, label='All_data', alpha=.8)\n",
    "ax.legend(bbox_to_anchor=(1.01, .8), fontsize=12)\n",
    "ax.set(xlim=lims, ylim=lims)\n",
    "ax.set_xticks(np.arange(0,lims[-1],10))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portable-effect",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_fit = stat_all/max_fr+alpha_add[modeln,0]\n",
    "mul_fit = stat_all/max_fr*alpha_mult[modeln,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "every-departure",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_add[0,0]-mse_mult[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continental-traveler",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_add = minimize_scalar(f_add,args=(stat_range/max_fr, stat_all/max_fr),method='bounded',bounds=[-10,10])\n",
    "res_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ignored-genius",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean((stat_range - (stat_all+alpha_add[modeln,0]))**2)\n",
    "# np.mean((stat_range-(stat_all*alpha_add[modeln,0]))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intended-duplicate",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_add[modeln,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspended-potter",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_add(6,stat_range,stat_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eleven-belly",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(edge_mids,stat_all/max_fr,'k')\n",
    "plt.plot(edge_mids,stat_range/max_fr,'b')\n",
    "plt.plot(edge_mids,add_fit,'r')\n",
    "plt.plot(edge_mids,mul_fit,'g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temporal-coupon",
   "metadata": {},
   "outputs": [],
   "source": [
    "celln = 121\n",
    "\n",
    "predcell = mpred[celln]/model_dt\n",
    "nspcell = msp[celln]/model_dt\n",
    "msp_smooth=(np.convolve(msp[celln], np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "pred_smooth=(np.convolve(mpred[celln], np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "fig, axs = plt.subplots(2,4, figsize=(24,10))\n",
    "# Eye Tuning Curve\n",
    "top_yaxs = np.max(ax_ylims[celln])+2*np.max(tuning_stds[celln])\n",
    "for i,modeln in enumerate(range(len(titles))):\n",
    "    metric = move_test[:,modeln]\n",
    "    nranges = np.quantile(metric,quartiles)\n",
    "    stat_range, edges, _ = binned_statistic(metric,test_nsp[:,celln],statistic='mean',bins=nranges)\n",
    "    edge_mids = np.quantile(metric,spk_percentile2)#np.array([(edges[i]+edges[i+1])/2 for i in range(len(edges)-1)])\n",
    "    cmap = mpl.colors.ListedColormap(colors, N=colors.shape[0])\n",
    "    norm = mpl.colors.BoundaryNorm(boundaries=np.floor(nranges), ncolors=len(cmap.colors))\n",
    "    for m in range(len(nranges)-1):\n",
    "        axs[0,modeln].axvspan(nranges[m], nranges[m+1],alpha=0.8, color=colors[m],zorder=0)\n",
    "    #     axs[1,1].errorbar(var_ranges[modeln],tuning_curves[celln,modeln], yerr=tuning_stds[celln,modeln],label=titles[modeln],c=clrs[modeln],lw=4,elinewidth=3)\n",
    "    axs[0,modeln].plot(edge_mids,stat_range/model_dt,'.-', ms=20, lw=4,c=clrs[modeln])\n",
    "\n",
    "#     axs[0,modeln].imshow(np.round(nranges,decimals=1)[None,:angn],aspect='auto', origin='lower',cmap=cmap,norm=norm,extent=(nranges[0],nranges[angn],0,top_yaxs),alpha=.8)\n",
    "#     axs[0,modeln].errorbar(var_ranges[modeln],tuning_curves[celln,modeln], yerr=tuning_stds[celln,modeln],label=titles[modeln],c=clrs[modeln],lw=4,elinewidth=3)\n",
    "    axs[0,modeln].set_ylim(bottom=0,top=np.max(ax_ylims,axis=1)[celln]+2*np.max(tuning_stds,axis=(1,2))[celln])\n",
    "    axs[0,modeln].set_xlim(-30,30)\n",
    "    axs[0,modeln].set_xlabel('Angle ($ ^{\\degree}$)')\n",
    "    axs[0,modeln].set_ylabel('Spikes/s')\n",
    "    axs[0,modeln].set_title('{} Tuning Curve'.format(titles[modeln]), color=clrs[modeln])\n",
    "# lines = axs[1,1].get_lines()\n",
    "# legend1 = axs[1,1].legend([lines[0]],['th'],bbox_to_anchor=(1.01, .2), fontsize=12)\n",
    "# legend2 = axs[1,1].legend([lines[1]],['phi'],bbox_to_anchor=(1.01, 1), fontsize=12)\n",
    "# axs[1,1].add_artist(legend1)\n",
    "\n",
    "# axs[1,2].add_artist(legend2)\n",
    "# axs[1,2].legend(bbox_to_anchor=(1.01, 1), fontsize=12)\n",
    "\n",
    "# Set up predicted spike range between 1-99th percentile\n",
    "stat_bins = 5\n",
    "pred_range = np.quantile(predcell,[.1,.9])\n",
    "msp_range = np.quantile(nspcell,[.01,1])\n",
    "spike_percentiles = np.arange(0,1.25,.25)\n",
    "spike_percentiles[-1]=.99\n",
    "spk_percentile2 = np.arange(.125,1.125,.25)\n",
    "pred_rangelin = np.quantile(predcell,spike_percentiles)\n",
    "xbin_pts = np.quantile(predcell,spk_percentile2)\n",
    "stat_bins = len(pred_rangelin) #5\n",
    "\n",
    "mse_add = np.zeros((ncells,len(titles),len(quartiles)-1))\n",
    "mse_mult = np.zeros((ncells,len(titles),len(quartiles)-1))\n",
    "alpha_add = np.zeros((ncells,len(titles),len(quartiles)-1))\n",
    "alpha_mult = np.zeros((ncells,len(titles),len(quartiles)-1))\n",
    "\n",
    "traces = np.zeros((ncells,len(titles),len(quartiles)-1,stat_bins-1)) # (model_type,quartile,FR)\n",
    "traces_mean = np.zeros((ncells,len(titles),stat_bins-1)) # (model_type,quartile,FR)\n",
    "edges_all = np.zeros((ncells,len(titles),len(quartiles)-1,stat_bins-1)) # (model_type,quartile,FR)\n",
    "# df_traces = pd.DataFrame([],columns=['modeln','quartile','FR']) \n",
    "for modeln in range(len(titles)):\n",
    "    metric = move_test[:,modeln]\n",
    "    nranges = np.quantile(metric,quartiles)# np.linspace(np.nanmean(metric)-2*np.nanstd(metric), np.nanmean(metric)+2*np.nanstd(metric),N_bins)\n",
    "    stat_all, edges, _ = binned_statistic(predcell,nspcell, statistic='mean',bins=pred_rangelin)\n",
    "    edge_mids = xbin_pts#np.array([(edges[i]+edges[i+1])/2 for i in range(len(edges)-1)])\n",
    "    traces_mean[celln,modeln]=stat_all\n",
    "    max_fr = np.max(stat_all)\n",
    "#     axs[1,modeln].set_xlim(0,pred_range[1]+np.std(pred_range))\n",
    "#     axs[1,modeln].set_ylim(0,np.max(stat)+np.std(stat))\n",
    "\n",
    "    for n in range(len(nranges)-1):\n",
    "        ind = np.where(((metric<=nranges[n+1])&(metric>nranges[n])))[0]\n",
    "        pred = predcell[ind]\n",
    "        sp = nspcell[ind]\n",
    "\n",
    "        stat_range, edges, _ = binned_statistic(pred, sp, statistic='mean',bins=pred_rangelin)\n",
    "        edge_mids = xbin_pts #np.array([(edges[i]+edges[i+1])/2 for i in range(len(edges)-1)])\n",
    "        traces[celln,modeln,n]=stat_range\n",
    "        edges_all[celln,modeln,n]=edge_mids\n",
    "        res_add = minimize_scalar(f_add,args=(stat_range/max_fr, stat_all/max_fr))\n",
    "        res_mult = minimize_scalar(f_mult,args=(stat_range/max_fr, stat_all/max_fr))\n",
    "        mse_add[celln, modeln, n] = res_add.fun\n",
    "        mse_mult[celln, modeln, n] = res_mult.fun\n",
    "        alpha_add[celln, modeln, n] = res_add.x\n",
    "        alpha_mult[celln, modeln, n] = res_mult.x\n",
    "\n",
    "        axs[1,modeln].plot(edge_mids, stat_range,'.-', c=colors[n],label='{:d}$\\degree$ : {:d}$\\degree$'.format(int(nranges[n]),int(nranges[n+1])),lw=4,ms=20,alpha=.9)\n",
    "        axs[1,modeln].set_title('Metric: {}'.format(titles[modeln]), color=clrs[modeln])\n",
    "        axs[1,modeln].set_xlabel('Predicted Spike Rate')\n",
    "        axs[1,modeln].set_ylabel('Actual Spike Rate')\n",
    "    \n",
    "    lim_max = np.max(traces[celln,modeln])+np.std(traces_mean[celln,modeln])\n",
    "    lim_min = np.min(traces[celln,modeln])-np.std(traces_mean[celln,modeln])\n",
    "    lims = (0, lim_max) if (lim_min)<0 else (lim_min,lim_max) \n",
    "    axs[1,modeln].plot(np.linspace(lims[0],lims[1]),np.linspace(lims[0],lims[1]),'k--',zorder=0)\n",
    "\n",
    "    axs[1,modeln].plot(edge_mids, stat_all,'.-', c='k', lw=5, ms=20, label='All_data', alpha=.8)\n",
    "    axs[1,modeln].legend(loc='upper left', fontsize=12)\n",
    "    axs[1,modeln].set(xlim=lims, ylim=lims)\n",
    "    axs[1,modeln].set_xticks(np.arange(0,lims[-1],10))\n",
    "    axs[1,modeln].set_yticks(np.arange(0,lims[-1],10))\n",
    "plt.tight_layout()\n",
    "fig.savefig(FigPath/'Example_Tuning_{}_celln{}.pdf'.format(stim_type,celln), facecolor='white', transparent=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bronze-roommate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schematic of Addative vs. Multiplicative\n",
    "\n",
    "celln = 121\n",
    "modeln=0\n",
    "quartiles = np.arange(0,1.25,.25)\n",
    "\n",
    "predcell = mpred[celln]/model_dt\n",
    "nspcell = msp[celln]/model_dt\n",
    "angn=0\n",
    "\n",
    "metric = move_test[:,modeln]\n",
    "nranges = np.quantile(var_ranges[modeln],quartiles)# np.linspace(np.nanmean(metric)-2*np.nanstd(metric), np.nanmean(metric)+2*np.nanstd(metric),N_bins)\n",
    "stat_all, edges, _ = binned_statistic(predcell,nspcell, statistic='mean',bins=pred_rangelin)\n",
    "edge_mids = xbin_pts#np.array([(edges[i]+edges[i+1])/2 for i in range(len(edges)-1)])\n",
    "traces_mean[celln,modeln]=stat_all\n",
    "max_fr = np.max(stat_all)\n",
    "\n",
    "lim_max = 40\n",
    "lim_min = 0\n",
    "lims = (0, lim_max) if (lim_min)<0 else (lim_min,lim_max) \n",
    "spike_percentiles = np.arange(0,1.25,.25)\n",
    "spike_percentiles[-1]=.99\n",
    "spk_percentile2 = np.arange(.125,1.125,.25)\n",
    "pred_rangelin = np.quantile(predcell,spike_percentiles)\n",
    "xbin_pts = np.quantile(predcell,spk_percentile2)\n",
    "stat_bins = len(pred_rangelin) #5\n",
    "mult_range = np.linspace(1.2,.7,4)\n",
    "for angn in range(5):\n",
    "    fig, axs = plt.subplots(1,1,figsize=(8,5))\n",
    "    metric = move_test[:,modeln]\n",
    "    \n",
    "    nranges = np.quantile(var_ranges[modeln],quartiles)# np.linspace(np.nanmean(metric)-2*np.nanstd(metric), np.nanmean(metric)+2*np.nanstd(metric),N_bins)\n",
    "#     stat_all, edges, _ = binned_statistic(predcell,nspcell, statistic='mean',bins=pred_rangelin)\n",
    "    stat_all = np.linspace(10,30,4)\n",
    "    edge_mids = stat_all#np.array([(edges[i]+edges[i+1])/2 for i in range(len(edges)-1)])\n",
    "    traces_mean[celln,modeln]=stat_all\n",
    "    ax = axs\n",
    "    max_fr = np.max(stat_all)\n",
    "    for n in range(angn):\n",
    "        stat_mult = np.linspace(10,30*mult_range[n],4)\n",
    "#         stat_mult[1:] = stat_mult[1:]*mult_range[n]\n",
    "        ax.plot(edge_mids, stat_mult,'.-', c=colors[n],label='{:d}$\\degree$ : {:d}$\\degree$'.format(int(nranges[n]),int(nranges[n+1])),lw=4,ms=20,alpha=1)\n",
    "    ax.set_title('Metric: {} Multiplicative'.format(titles[modeln]), color=clrs[modeln])\n",
    "    ax.set_xlabel('Predicted Spike Rate')\n",
    "    ax.set_ylabel('Actual Spike Rate')\n",
    "\n",
    "    ax.plot(np.linspace(lims[0],lims[1]),np.linspace(lims[0],lims[1]),'k--',zorder=0)\n",
    "    \n",
    "    ax.plot(edge_mids, stat_all,'.-', c='k', lw=5, ms=20, label='All_data', alpha=.8)\n",
    "    ax.legend(bbox_to_anchor=(1.02, .8), fontsize=12)\n",
    "    ax.set(xlim=lims, ylim=lims)\n",
    "    ax.set_xticks(np.arange(0,lims[-1],10))\n",
    "    plt.tight_layout()\n",
    "\n",
    "#     fig.savefig(FigPath/'Schematic_mult_{}_N{}.png'.format(stim_type,angn), facecolor='white', transparent=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fiscal-crawford",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "existing-dominican",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_range = np.linspace(8,-8,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple-evolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schematic of Addative vs. Multiplicative\n",
    "\n",
    "celln = 121\n",
    "modeln=0\n",
    "quartiles = np.arange(0,1.25,.25)\n",
    "\n",
    "predcell = mpred[celln]/model_dt\n",
    "nspcell = msp[celln]/model_dt\n",
    "angn=4\n",
    "\n",
    "lim_max = 40\n",
    "lim_min = 0\n",
    "lims = (0, lim_max) if (lim_min)<0 else (lim_min,lim_max) \n",
    "for angn in range(5):\n",
    "    fig, axs = plt.subplots(1,1,figsize=(8,5))\n",
    "    metric = move_test[:,modeln]\n",
    "    nranges = np.quantile(var_ranges[modeln],quartiles)# np.linspace(np.nanmean(metric)-2*np.nanstd(metric), np.nanmean(metric)+2*np.nanstd(metric),N_bins)\n",
    "#     stat_all, edges, _ = binned_statistic(predcell,nspcell, statistic='mean',bins=pred_rangelin)\n",
    "    stat_all = np.linspace(10,30,4)\n",
    "    edge_mids = stat_all#np.array([(edges[i]+edges[i+1])/2 for i in range(len(edges)-1)])\n",
    "    traces_mean[celln,modeln]=stat_all\n",
    "    ax = axs\n",
    "    max_fr = np.max(stat_all)\n",
    "\n",
    "    for n in range(angn):\n",
    "        ind = np.where(((metric<=nranges[n+1])&(metric>nranges[n])))[0]\n",
    "        pred = predcell[ind]\n",
    "        sp = nspcell[ind]\n",
    "        ax.plot(edge_mids, stat_all+add_range[n],'.-', c=colors[n],label='{:d}$\\degree$ : {:d}$\\degree$'.format(int(nranges[n]),int(nranges[n+1])),lw=4,ms=20,alpha=1)\n",
    "    ax.set_title('Metric: {} Additive'.format(titles[modeln]), color=clrs[modeln])\n",
    "    ax.set_xlabel('Predicted Spike Rate')\n",
    "    ax.set_ylabel('Actual Spike Rate')\n",
    "\n",
    "    ax.plot(np.linspace(lims[0],lims[1]),np.linspace(lims[0],lims[1]),'k--',zorder=0)\n",
    "    ax.plot(edge_mids, stat_all,'.-', c='k', lw=5, ms=20, label='All_data', alpha=.8)\n",
    "    ax.legend(bbox_to_anchor=(1.02, .8), fontsize=12)\n",
    "    ax.set(xlim=lims, ylim=lims)\n",
    "    ax.set_xticks(np.arange(0,lims[-1],10))\n",
    "    plt.tight_layout()\n",
    "\n",
    "    fig.savefig(FigPath/'Schematic_add_{}_N{}.png'.format(stim_type,angn), facecolor='white', transparent=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dense-natural",
   "metadata": {},
   "outputs": [],
   "source": [
    "for angn in range(5):\n",
    "    fig, axs = plt.subplots(1,1,figsize=(8,5))\n",
    "    # Eye Tuning Curve\n",
    "    top_yaxs = np.max(ax_ylims[celln])+2*np.max(tuning_stds[celln])\n",
    "    metric = move_test[:,modeln]\n",
    "    var_ranges_schematic = np.linspace(var_ranges[modeln,0],var_ranges[modeln,-1],len(var_ranges[modeln]))\n",
    "    nranges = np.quantile(var_ranges_schematic,quartiles)\n",
    "    cmap = mpl.colors.ListedColormap(colors, N=colors.shape[0])\n",
    "    norm = mpl.colors.BoundaryNorm(boundaries=np.floor(nranges), ncolors=cmap.N)\n",
    "    ax = axs\n",
    "    tuning_schematic = np.linspace(tuning_curves[celln,modeln][0],tuning_curves[celln,modeln][-1],len(tuning_curves[celln,modeln]))\n",
    "    img = ax.imshow(np.round(nranges,decimals=1)[None,:angn],aspect='auto', origin='lower',cmap=cmap,norm=norm,extent=(nranges[0],nranges[angn],0,top_yaxs),alpha=.8)\n",
    "    ax.errorbar(var_ranges_schematic,tuning_schematic, yerr=tuning_stds[celln,modeln],label=titles[modeln],c=clrs[modeln],lw=4,elinewidth=3)\n",
    "    ax.set_ylim(bottom=0,top=np.max(ax_ylims,axis=1)[celln]+2*np.max(tuning_stds,axis=(1,2))[celln])\n",
    "    ax.set_xlim(-50,50)\n",
    "    ax.set_xlabel('Angle ($ ^{\\degree}$)')\n",
    "    ax.set_ylabel('Spikes/s')\n",
    "    ax.set_title('Eye Tuning Curves')\n",
    "    ax.legend(bbox_to_anchor=(1.01, .2), fontsize=12)\n",
    "\n",
    "    lines = ax.get_lines()\n",
    "    legend1 = ax.legend([lines[0]],[titles[0]],bbox_to_anchor=(1.01, .2), fontsize=12)\n",
    "#     legend2 = axs.legend([lines[1]],[titles[1]],bbox_to_anchor=(1.01, 1), fontsize=12)\n",
    "#     ax.add_artist(legend1)\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(FigPath/'Schematic_Tuning_{}_N{}.png'.format(stim_type,angn), facecolor='white', transparent=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moderate-lafayette",
   "metadata": {},
   "outputs": [],
   "source": [
    "celln = 121\n",
    "fig, axs = plt.subplots(figsize=(8,8))\n",
    "dmodel = mse_add[celln]-mse_mult[celln]\n",
    "crange = np.max(np.abs(dmodel))\n",
    "im = axs.imshow(dmodel,cmap='seismic',vmin=-crange,vmax=crange)\n",
    "axs.set_yticks(np.arange(0,4))\n",
    "axs.set_yticklabels(titles)\n",
    "axs.set_ylabel('Movement Model')\n",
    "axs.set_xticks(np.arange(0,4))\n",
    "axs.set_xticklabels(['.25','.5','.75','1'])\n",
    "axs.set_xlabel('Quantile Range')\n",
    "axs.set_title('$MSE_{add}$ - $MSE_{mult}$')\n",
    "cbar = add_colorbar(im)\n",
    "cbar.set_label('$\\Delta$ MSE')\n",
    "# cbar.set_ticks([-crange,crange])\n",
    "# cbar.set_ticklabels(['Add.','Mult.'])\n",
    "# plt.suptitle('Celln:{}, cc={:.03f}, r2={:.03f}'.format(celln,mcc[celln],mr2[celln]),y=1,fontsize=30)\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "fig.savefig(FigPath/'deltaMSE{}_T{:02d}.pdf'.format(celln,nt_glm_lag), facecolor='white', transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banner-liverpool",
   "metadata": {},
   "source": [
    "#### Add/Mult across cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "different-basic",
   "metadata": {},
   "outputs": [],
   "source": [
    "quartiles = np.arange(0,1.25,.25)\n",
    "stat_bins = 5\n",
    "\n",
    "bin_length=40\n",
    "ncells=model_nsp.shape[-1]\n",
    "colors = plt.cm.cool(np.linspace(0,1,4))\n",
    "clrs = ['blue','orange','green','red']\n",
    "lag=150 # in ms\n",
    "nt_glm_lag = 5\n",
    "# minlag = int(-lag//(1000*model_dt)); maxlag=int((lag//(1000*model_dt))+1)\n",
    "lag_list = np.array([-1,0,1,2,3]) #,np.arange(minlag,maxlag,np.floor((maxlag-minlag)/nt_glm_lag).astype(int))\n",
    "nt_glm_lag = len(lag_list)\n",
    "GLM_Vis = ioh5.load(save_dir/'GLM_{}_Data_VisOnly_notsmooth_dt{:03d}_T{:02d}.h5'.format(model_type,int(model_dt*1000), nt_glm_lag))\n",
    "locals().update(GLM_Vis)\n",
    "data, train_idx, test_idx = load_train_test(file_dict, save_dir, model_dt=model_dt, do_shuffle=False, do_norm=False,free_move=free_move, has_imu=free_move, has_mouse=False)\n",
    "locals().update(data)\n",
    "##### Explore Neurons #####\n",
    "# Initialize movement combinations\n",
    "titles = np.array(['Theta','Phi','Roll','Pitch']) # 'dg_p','dg_n' 'roll','pitch'\n",
    "titles_all = []\n",
    "for n in range(1,len(titles)+1):\n",
    "    perms = np.array(list(itertools.combinations(np.arange(len(titles)), n)))\n",
    "    for ind in range(perms.shape[0]):\n",
    "        titles_all.append('_'.join([t for t in titles[perms[ind]]]))\n",
    "\n",
    "# train_dgaze_p = train_dth + np.diff(train_gz,append=0)\n",
    "# train_dgaze_n = train_dth - np.diff(train_gz,append=0)\n",
    "# test_dgaze_p = test_dth + np.diff(test_gz,append=0)\n",
    "# test_dgaze_n = test_dth - np.diff(test_gz,append=0)\n",
    "# move_train = np.hstack((train_th[:,np.newaxis],train_phi[:,np.newaxis],train_roll[:,np.newaxis],train_pitch[:,np.newaxis], train_dth[:,np.newaxis],train_dphi[:,np.newaxis]))\n",
    "move_test = np.hstack((test_th[:,np.newaxis],test_phi[:,np.newaxis],test_roll[:,np.newaxis],test_pitch[:,np.newaxis])) #,test_dgaze_p[:,np.newaxis],test_dgaze_n[:,np.newaxis]))#\n",
    "# move_test = np.hstack((test_th[:,np.newaxis],test_phi[:,np.newaxis],test_dgaze_p[:,np.newaxis],test_dgaze_n[:,np.newaxis]))# test_dth[:,np.newaxis],test_dphi[:,np.newaxis]))\n",
    "move_test = move_test - np.mean(move_test,axis=0)\n",
    "# Create all tuning curves for plotting\n",
    "N_bins=10\n",
    "ncells = model_nsp.shape[-1]\n",
    "ax_ylims = np.zeros((model_nsp.shape[-1],len(titles)))\n",
    "tuning_curves = np.zeros((model_nsp.shape[-1],len(titles),N_bins-1))\n",
    "tuning_stds = np.zeros((model_nsp.shape[-1],len(titles),N_bins-1))\n",
    "var_ranges = np.zeros((len(titles),N_bins-1))\n",
    "for modeln in range(len(titles)):\n",
    "    metric = move_test[:,modeln]\n",
    "    tuning, tuning_std, var_range = tuning_curve(test_nsp, metric, N_bins=N_bins, model_dt=model_dt)\n",
    "    tuning_curves[:,modeln] = tuning\n",
    "    tuning_stds[:,modeln] = tuning_std\n",
    "    ax_ylims[:,modeln] = np.max(tuning,axis=1)\n",
    "    var_ranges[modeln] = var_range\n",
    "\n",
    "\n",
    "mse_add = np.zeros((ncells,len(titles),len(quartiles)-1))\n",
    "mse_mult = np.zeros((ncells,len(titles),len(quartiles)-1))\n",
    "alpha_add = np.zeros((ncells,len(titles),len(quartiles)-1))\n",
    "alpha_mult = np.zeros((ncells,len(titles),len(quartiles)-1))\n",
    "\n",
    "traces = np.zeros((ncells,len(titles),len(quartiles)-1,stat_bins-1)) # (model_type,quartile,FR)\n",
    "traces_mean = np.zeros((ncells,len(titles),stat_bins-1)) # (model_type,quartile,FR)\n",
    "edges_all = np.zeros((ncells,len(titles),len(quartiles)-1,stat_bins-1)) # (model_type,quartile,FR)\n",
    "# df_traces = pd.DataFrame([],columns=['modeln','quartile','FR']) \n",
    "for celln in range(ncells):\n",
    "#     if mcc[celln]>.3:\n",
    "    predcell = mpred[celln]/model_dt\n",
    "    nspcell = msp[celln]/model_dt\n",
    "    stat_bins = 5\n",
    "    pred_range = np.quantile(predcell,[.1,.9])\n",
    "    msp_range = np.quantile(nspcell,[.01,1])\n",
    "    spike_percentiles = np.arange(0,1.25,.25)\n",
    "    spike_percentiles[-1]=.99\n",
    "    spk_percentile2 = np.arange(.125,1.125,.25)\n",
    "    pred_rangelin = np.quantile(predcell,spike_percentiles)\n",
    "    xbin_pts = np.quantile(predcell,spk_percentile2)\n",
    "    stat_bins = len(pred_rangelin) #5\n",
    "    for modeln in range(len(titles)):\n",
    "        metric = move_test[:,modeln]\n",
    "        nranges = np.quantile(metric,quartiles)# np.linspace(np.nanmean(metric)-2*np.nanstd(metric), np.nanmean(metric)+2*np.nanstd(metric),N_bins)\n",
    "        stat_all, edges, _ = binned_statistic(predcell,nspcell, statistic='mean',bins=pred_rangelin)\n",
    "        edge_mids = xbin_pts#np.array([(edges[i]+edges[i+1])/2 for i in range(len(edges)-1)])\n",
    "        traces_mean[celln,modeln]=stat_all\n",
    "        max_fr = np.max(stat_all)\n",
    "    #     axs[1,modeln].set_xlim(0,pred_range[1]+np.std(pred_range))\n",
    "    #     axs[1,modeln].set_ylim(0,np.max(stat)+np.std(stat))\n",
    "\n",
    "        for n in range(len(nranges)-1):\n",
    "            ind = np.where(((metric<=nranges[n+1])&(metric>nranges[n])))[0]\n",
    "            pred = predcell[ind]\n",
    "            sp = nspcell[ind]\n",
    "\n",
    "            stat_range, edges, _ = binned_statistic(pred, sp, statistic='mean',bins=pred_rangelin)\n",
    "            edge_mids = xbin_pts #np.array([(edges[i]+edges[i+1])/2 for i in range(len(edges)-1)])\n",
    "            traces[celln,modeln,n]=stat_range\n",
    "            edges_all[celln,modeln,n]=edge_mids\n",
    "            res_add = minimize_scalar(f_add,args=(stat_range/max_fr, stat_all/max_fr)) # \n",
    "            res_mult = minimize_scalar(f_mult,args=(stat_range/max_fr, stat_all/max_fr)) # \n",
    "            mse_add[celln, modeln, n] = res_add.fun\n",
    "            mse_mult[celln, modeln, n] = res_mult.fun\n",
    "            alpha_add[celln, modeln, n] = res_add.x\n",
    "            alpha_mult[celln, modeln, n] = res_mult.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incomplete-freight",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tuning_curves = np.zeros((model_nsp.shape[-1],len(titles),4))\n",
    "for celln in range(test_nsp.shape[-1]):\n",
    "    for i, modeln in enumerate(range(len(titles))):\n",
    "        metric = move_test[:,modeln]\n",
    "        #     metric = metric[(metric>var_range[modeln,0])&(metric<var_range[modeln,-1\n",
    "        nranges = np.round(np.quantile(metric,quartiles),decimals=1)\n",
    "        stat_range, edges, _ = binned_statistic(metric,test_nsp[:,celln],statistic='mean',bins=nranges)\n",
    "        edge_mids = np.quantile(metric,spk_percentile2)\n",
    "        tuning_curves[celln,modeln] = stat_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bronze-holocaust",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tc_mod = (tuning_curves-np.mean(tuning_curves,axis=(2))[:,:,np.newaxis])/(np.mean(tuning_curves,axis=(2))[:,:,np.newaxis])\n",
    "tc_mod = (np.max(tuning_curves,axis=-1,keepdims=True)-np.min(tuning_curves,axis=-1,keepdims=True))/(np.min(tuning_curves,axis=-1,keepdims=True)+np.max(tuning_curves,axis=-1,keepdims=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welsh-complaint",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(tc_mod[:,0].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "second-finder",
   "metadata": {},
   "outputs": [],
   "source": [
    "tc_mod.shape, dmodel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smooth-standard",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_fr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identified-cathedral",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latin-browse",
   "metadata": {},
   "outputs": [],
   "source": [
    "dmodel = mse_add-mse_mult\n",
    "dmodel = dmodel[:,:,(0,-1)]\n",
    "\n",
    "for modeln in range(len(titles)):\n",
    "    dmodel[(avg_fr[:,modeln]<thresh_fr) & (np.abs(tc_mod[:,modeln].squeeze())>.25)]=0\n",
    "dmult_num = np.sum(dmodel_sig>0)/np.size(dmodel_sig[dmodel_sig!=0])\n",
    "dadd_num = np.sum(dmodel_sig<0)/np.size(dmodel_sig[dmodel_sig!=0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informational-ceremony",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documented-talent",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_data = {'dmode':dmodel,'tc_mod':tc_mod}\n",
    "\n",
    "np.savez(save_dir/'dmodels.npz', mod_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "above-charlotte",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unravel_index(np.argmax(dmodel),dmodel.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coupled-holly",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dmodel = mse_add-mse_mult\n",
    "# dmodel = dmodel[:,:,(0,-1)].flatten()\n",
    "# dmodel = dmodel[np.abs(dmodel)>.002]\n",
    "# dmult_num = np.sum(dmodel>0)/np.size(dmodel)\n",
    "# dadd_num = np.sum(dmodel<0)/np.size(dmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peripheral-commander",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "ax.hist(dmodel_sig[dmodel_sig<0],bins=np.arange(-.1,0,.002),color='b')\n",
    "ax.axvline(x=np.median(dmodel_sig[dmodel_sig<0]),color='b',linestyle='--')\n",
    "\n",
    "ax.hist(dmodel_sig[dmodel_sig>0],bins=np.arange(.002,.1,.002),color='r')\n",
    "ax.axvline(x=np.median(dmodel_sig[dmodel_sig>0]),color='r',linestyle='--')\n",
    "ax.legend(['Add. Median','Mult.Median'])\n",
    "ax.set_xlabel(r'$\\Delta$MSE')\n",
    "# fig.savefig(FigPath/'AddMultHist.png', facecolor='white', transparent=True)\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "ax.bar(0,dadd_num*100, color='b')\n",
    "ax.bar(1,dmult_num*100, color='r')\n",
    "# ax.scatter(np.zeros(len(dadd)),dadd,sz=3,)\n",
    "ax.set_ylim(0,100)\n",
    "ax.set_xticks([0,1])\n",
    "ax.set_xticklabels(['Add.','Mult.'])\n",
    "ax.set_ylabel('Proportion (%)')\n",
    "plt.tight_layout()\n",
    "# fig.savefig(FigPath/'AddMult_Propotion.png', facecolor='white', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amateur-drilling",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "ax.hist(dmodel[dmodel<0],bins=np.arange(-.1,-.001,.002),color='b')\n",
    "ax.axvline(x=np.median(dmodel[dmodel<0]),color='b',linestyle='--')\n",
    "ax.hist(dmodel[dmodel>0],bins=np.arange(.001,.1,.002),color='r')\n",
    "ax.axvline(x=np.median(dmodel[dmodel>0]),color='r',linestyle='--')\n",
    "ax.legend(['Add. Median','Mult.Median'])\n",
    "ax.set_xlabel(r'$\\Delta$MSE')\n",
    "# fig.savefig(FigPath/'AddMultHist.png', facecolor='white', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dutch-irish",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "ax.bar(0,dadd_num*100, color='b')\n",
    "ax.bar(1,dmult_num*100, color='r')\n",
    "# ax.scatter(np.zeros(len(dadd)),dadd,sz=3,)\n",
    "ax.set_ylim(0,100)\n",
    "ax.set_xticks([0,1])\n",
    "ax.set_xticklabels(['Add.','Mult.'])\n",
    "ax.set_ylabel('Proportion (%)')\n",
    "plt.tight_layout()\n",
    "# fig.savefig(FigPath/'AddMult_Propotion.png', facecolor='white', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "light-constitutional",
   "metadata": {},
   "outputs": [],
   "source": [
    "crange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electoral-italy",
   "metadata": {},
   "outputs": [],
   "source": [
    "celln = 25# np.argmax(mcc)\n",
    "bin_length=40\n",
    "n=1; axn=1\n",
    "fig, axs = plt.subplots(1,1,figsize=(15,5))\n",
    "crange = np.max(np.abs(msta_up[celln,axn]))\n",
    "img = axs.imshow(msta_up[celln,axn],cmap='gray',vmin=-crange+.01,vmax=crange-.01)\n",
    "axs.axis('off')\n",
    "# axs.set_title('Lag: {}ms'.format(int(1000*lag_list[axn]*model_dt)))\n",
    "axs.set_title('Free Moving STA')\n",
    "\n",
    "cbar = add_colorbar(img,) # orientation='horizontal',location='bottom'\n",
    "cbar.set_ticks([-crange+.01,crange-.01])\n",
    "cbar.set_ticklabels(['Dark','Light'])\n",
    "plt.tight_layout()\n",
    "fig.savefig(FigPath/'Example_STA_{}.png'.format(stim_type), facecolor='white', transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "digital-shell",
   "metadata": {},
   "source": [
    "### Exmaple Tunign Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "green-harris",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dt = .05\n",
    "do_shuffle=True\n",
    "do_norm = False\n",
    "data,train_idx,test_idx = load_train_test(file_dict, save_dir, model_dt=model_dt, do_shuffle=do_shuffle, do_norm=do_norm,free_move=free_move, has_imu=free_move, has_mouse=False)\n",
    "locals().update(data)\n",
    "\n",
    "tuning_curves = np.zeros((model_nsp.shape[-1],len(titles),len(quartiles)-1))\n",
    "tuning_stds = np.zeros((model_nsp.shape[-1],len(titles),len(quartiles)-1))\n",
    "for celln in range(test_nsp.shape[-1]):\n",
    "    for i, modeln in enumerate(range(len(titles))):\n",
    "        metric = move_test[:,modeln]\n",
    "        nranges = np.round(np.quantile(metric,quartiles),decimals=1)\n",
    "        stat_range, edges, _ = binned_statistic(metric,test_nsp[:,celln],statistic='mean',bins=nranges)\n",
    "        bin_std,_,_ =binned_statistic(metric,test_nsp[:,celln],statistic='std',bins=nranges)\n",
    "        edge_mids = np.quantile(metric,spk_percentile2)\n",
    "        tuning_curves[celln,modeln] = stat_range\n",
    "        tuning_stds[celln,modeln] = bin_std\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worth-instruction",
   "metadata": {},
   "outputs": [],
   "source": [
    "celln = 0\n",
    "\n",
    "predcell = mpred[celln]/model_dt\n",
    "nspcell = msp[celln]/model_dt\n",
    "msp_smooth=(np.convolve(msp[celln], np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "pred_smooth=(np.convolve(mpred[celln], np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "fig, axs = plt.subplots(1,4, figsize=(24,5))\n",
    "# Eye Tuning Curve\n",
    "top_yaxs = np.max(ax_ylims[celln])+2*np.max(tuning_stds[celln])\n",
    "for i,modeln in enumerate(range(len(titles))):\n",
    "    metric = move_test[:,modeln]\n",
    "    nranges = np.quantile(metric,quartiles)\n",
    "    stat_range, edges, _ = binned_statistic(metric,test_nsp[:,celln],statistic='mean',bins=nranges)\n",
    "    edge_mids = np.quantile(metric,spk_percentile2)#np.array([(edges[i]+edges[i+1])/2 for i in range(len(edges)-1)])\n",
    "#     for m in range(len(nranges)-1):\n",
    "#         axs[0,modeln].axvspan(nranges[m], nranges[m+1],alpha=0.8, color=colors[m],zorder=0)\n",
    "    axs[modeln].errorbar(edge_mids,tuning_curves[celln,modeln]/model_dt, yerr=tuning_stds[celln,modeln],label=titles[modeln],c=clrs[modeln],lw=4,elinewidth=3)\n",
    "#     axs[0,modeln].plot(edge_mids,stat_range/model_dt,'.-', ms=20, lw=4,c=clrs[modeln])\n",
    "\n",
    "#     axs[0,modeln].imshow(np.round(nranges,decimals=1)[None,:angn],aspect='auto', origin='lower',cmap=cmap,norm=norm,extent=(nranges[0],nranges[angn],0,top_yaxs),alpha=.8)\n",
    "#     axs[0,modeln].errorbar(var_ranges[modeln],tuning_curves[celln,modeln], yerr=tuning_stds[celln,modeln],label=titles[modeln],c=clrs[modeln],lw=4,elinewidth=3)\n",
    "    axs[modeln].set_ylim(bottom=0,top=np.max(ax_ylims,axis=1)[celln]+2*np.max(tuning_stds,axis=(1,2))[celln])\n",
    "    axs[modeln].set_xlim(-30,30)\n",
    "    axs[modeln].set_xlabel('Angle ($ ^{\\degree}$)')\n",
    "    axs[modeln].set_ylabel('Spikes/s')\n",
    "    axs[modeln].set_title('{} Tuning Curve'.format(titles[modeln]), color=clrs[modeln])\n",
    "    \n",
    "plt.tight_layout()\n",
    "if do_shuffle:\n",
    "    fig.savefig(FigPath/'TuningCurves_{}_Cell{}_shuff.png'.format(stim_type,celln), facecolor='white', transparent=True)\n",
    "else:\n",
    "    fig.savefig(FigPath/'TuningCurves_{}_Cell{}.png'.format(stim_type,celln), facecolor='white', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charged-indiana",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tc_mod = (tuning_curves-np.mean(tuning_curves,axis=(2))[:,:,np.newaxis])/(np.mean(tuning_curves,axis=(2))[:,:,np.newaxis])\n",
    "\n",
    "tc_mod = (np.max(tuning_curves,axis=-1,keepdims=True)-np.min(tuning_curves,axis=-1,keepdims=True))/(np.min(tuning_curves,axis=-1,keepdims=True)+np.max(tuning_curves,axis=-1,keepdims=True))\n",
    "\n",
    "avg_fr = np.mean(tuning_curves,axis=-1).squeeze()\n",
    "\n",
    "thresh_fr = 1\n",
    "tuning_sig = tc_mod.copy()\n",
    "tunig_sig = tuning_sig[avg_fr[:,modeln]<thresh_fr].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helpful-master",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,4,figsize=(25,5))\n",
    "for modeln in np.arange(len(titles)):\n",
    "    count,edges = np.histogram(tuning_sig[:,modeln], bins=np.arange(0,1.1,.05))\n",
    "    edges_mid = np.array([(edges[i]+edges[i+1])/2 for i in range(len(edges)-1)])\n",
    "    ax[modeln].bar(edges_mid, count/len(tuning_sig[:,modeln]),color=clrs[modeln],width=.05)\n",
    "    ax[modeln].set_title('{}'.format(titles[modeln]), color=clrs[modeln])\n",
    "    ax[modeln].set_xlabel('Modulation Index')\n",
    "    ax[modeln].set_ylabel('Proportion')\n",
    "    ax[modeln].set_xlim(0,1)\n",
    "    ax[modeln].set_ylim(0,.4)\n",
    "plt.tight_layout()\n",
    "if do_shuffle:\n",
    "    fig.savefig(FigPath/'TuningHists{}_shuff.png'.format(stim_type), facecolor='white', transparent=True)\n",
    "else:\n",
    "    fig.savefig(FigPath/'TuningHists{}.png'.format(stim_type), facecolor='white', transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "veterinary-union",
   "metadata": {},
   "source": [
    "### Head Fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "velvet-productivity",
   "metadata": {},
   "outputs": [],
   "source": [
    "GLM_Vis = ioh5.load(save_dir/'GLM_{}_Data_VisOnly_notsmooth_dt{:03d}_T{:02d}.h5'.format(model_type,int(model_dt*1000), nt_glm_lag))\n",
    "locals().update(GLM_Vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monetary-beads",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf = 4\n",
    "msta_up = np.zeros((msta.shape[0],msta.shape[1],sf*msta.shape[-2],sf*msta.shape[-1]))\n",
    "for n in range(msta.shape[0]):\n",
    "    for t in range(msta.shape[1]):\n",
    "        msta_up[n,t] = cv2.resize(msta[n,t],(sf*msta.shape[-1],sf*msta.shape[-2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transsexual-conditions",
   "metadata": {},
   "outputs": [],
   "source": [
    "celln = 25# np.argmax(mcc)\n",
    "bin_length=40\n",
    "fig, axs = plt.subplots(1,figsize=(15,5))\n",
    "predcell = mpred[celln]/model_dt\n",
    "nspcell = msp[celln]/model_dt\n",
    "msp_smooth=(np.convolve(msp[celln], np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "pred_smooth=(np.convolve(mpred[celln], np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "axs.plot(np.arange(len(msp_smooth))*model_dt,msp_smooth,'k',label='test FR')\n",
    "axs.plot(np.arange(len(pred_smooth))*model_dt,pred_smooth,'r', label='pred FR')\n",
    "axs.set_xlabel('Time (s)')\n",
    "axs.set_ylabel('Firing Rate (spks/s)')\n",
    "axs.legend()\n",
    "axs.set_title('Smoothed FRs')\n",
    "plt.tight_layout()\n",
    "fig.savefig(FigPath/'Example_TestPred_{}_cell{}.png'.format(stim_type,celln), facecolor='white', transparent=True)\n",
    "n=1; axn=4\n",
    "fig, axs = plt.subplots(1,nt_glm_lag, figsize=((20,5))) \n",
    "crange = np.max(np.abs(msta[celln]))\n",
    "for n in range(nt_glm_lag):\n",
    "    img = axs[n].imshow(msta_up[celln,n],cmap='RdBu_r',vmin=-crange,vmax=crange)\n",
    "    axs[n].axis('off')\n",
    "    axs[n].set_title('Lag:{:03d} ms'.format(int(1000*lag_list[n]*model_dt)))\n",
    "    axs[n].axis('off')\n",
    "cbar = add_colorbar(img)\n",
    "cbar.set_ticks([-crange,crange])\n",
    "cbar.set_ticklabels(['Dark','Light'])\n",
    "plt.tight_layout()\n",
    "fig.savefig(FigPath/'Example_STA_{}_cell{}.png'.format(stim_type,celln), facecolor='white', transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agreed-theme",
   "metadata": {},
   "source": [
    "## PyGLMNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conventional-houston",
   "metadata": {},
   "outputs": [],
   "source": [
    "rolled_vid = np.hstack([np.roll(model_vid_sm, nframes, axis=0) for nframes in lag_list]) # nt_glm_lag\n",
    "x_train = rolled_vid[train_idx].reshape(len(train_idx),-1)\n",
    "x_test = rolled_vid[test_idx].reshape(len(test_idx),-1)\n",
    "# x_train = train_vid.reshape(train_vid.shape[0],-1)\n",
    "# x_train = np.hstack([np.roll(x_train, nframes, axis=0) for nframes in lag_list]) # nt_glm_lag\n",
    "# x_test = test_vid.reshape(test_vid.shape[0],-1) \n",
    "# x_test = np.hstack([np.roll(x_test, nframes, axis=0) for nframes in lag_list])#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hydraulic-alfred",
   "metadata": {},
   "outputs": [],
   "source": [
    "celln = 51\n",
    "start = time.time()\n",
    "glm = GLMCV(distr='poisson', score_metric='pseudo_R2', verbose=True, cv=2, alpha=.2,reg_lambda=.001)\n",
    "glm.fit(x_train,train_nsp[:,celln])\n",
    "print('GLM Time: ', time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acknowledged-things",
   "metadata": {},
   "outputs": [],
   "source": [
    "glm.reg_lambda_opt_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infrared-empty",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = glm.beta_.reshape(nt_glm_lag,train_vid.shape[1],train_vid.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "present-violin",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = glm.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regional-crawford",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_nsp[:,celln])\n",
    "plt.plot(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "measured-birthday",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "plt.imshow(np.hstack(rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressing-profile",
   "metadata": {},
   "source": [
    "cleaning up fits, and how to quantify the fits, how to compare them to white noise RF's. take average then look at average shift. \n",
    "\n",
    "Getting add vs mult terms only for significant modulated neurons.\n",
    "\n",
    "Compare decoding weights of visual vs motor, overlap with dot proudct\n",
    "\n",
    "\n",
    "\n",
    "How to get an average across units for multiplictive?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "institutional-lafayette",
   "metadata": {},
   "source": [
    "# Vis+Movement SK learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "looking-output",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intelligent-group",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def do_glm_fit_vismov_skl(train_nsp, test_nsp, x_train, x_test, celln, model_type, lag_list, bin_length=40, model_dt=.1):\n",
    "    \n",
    "    ##### Format data #####\n",
    "    nt_glm_lag = len(lag_list)\n",
    "    \n",
    "    # Shift spikes by -lag for GLM fits\n",
    "    sps_train = train_nsp[:,celln] # np.roll(train_nsp[:,celln],-lag)\n",
    "    sps_test = test_nsp[:,celln] # np.roll(test_nsp[:,celln],-lag)\n",
    "\n",
    "\n",
    "    if model_type == 'elasticnetcv':\n",
    "        model = lm.ElasticNetCV(l1_ratio=[.05, .01, .5, .7]) # lm.RidgeCV(alphas=np.arange(100,10000,1000))) #  #MultiOutputRegressor(lm.Ridge(),n_jobs=-1)) \n",
    "        model.fit(x_train,sps_train)\n",
    "        sta_all = np.reshape(model.coef_,(nt_glm_lag,)+nks)\n",
    "        sp_pred = model.predict(x_test)\n",
    "    elif model_type == 'ridgecv':\n",
    "        lambdas = 1024 * (2**np.arange(0,16))\n",
    "        model = lm.RidgeCV(alphas=lambdas)\n",
    "        model.fit(x_train,sps_train)\n",
    "        sta_all = np.reshape(model.coef_,(nt_glm_lag,)+nks)\n",
    "        sp_pred = model.predict(x_test)\n",
    "    else:\n",
    "    #     lambdas = 2048 * (2**np.arange(0,16))\n",
    "        lambdas = 2**np.arange(0,16)\n",
    "        nlam = len(lambdas)\n",
    "        # Initialze mse traces for regularization cross validation\n",
    "        msetrain = np.zeros((nlam,1))\n",
    "        msetest = np.zeros((nlam,1))\n",
    "        pred_all =np.zeros((x_test.shape[0],nlam)) \n",
    "        w_ridge = np.zeros((x_train.shape[-1],nlam))\n",
    "        w_intercept = np.zeros((nlam,1))\n",
    "        # loop over regularization strength\n",
    "        for l in range(len(lambdas)):\n",
    "            model = lm.PoissonRegressor(alpha=lambdas[l],max_iter=300)\n",
    "            # calculate MAP estimate               \n",
    "            model.fit(x_train,sps_train)\n",
    "            w_ridge[:,l] = model.coef_\n",
    "            w_intercept[l] = model.intercept_\n",
    "            pred_all[:,l] = model.predict(x_test)\n",
    "            # calculate test and training rms error\n",
    "            msetrain[l] = mean_poisson_deviance(sps_train,model.predict(x_train)) #np.mean((sps_train - model.predict(x_train))**2)\n",
    "            msetest[l] = mean_poisson_deviance(sps_test,pred_all[:,l]) # np.mean((sps_test - model.predict(x_test))**2)\n",
    "        # select best cross-validated lambda for RF\n",
    "        best_lambda = np.argmin(msetest)\n",
    "        w = w_ridge[:,best_lambda]\n",
    "        intercept= w_intercept[best_lambda]\n",
    "        ridge_rf = w_ridge[:,best_lambda]\n",
    "        sta_all = np.reshape(w,(nt_glm_lag,)+nks)\n",
    "        sp_pred = pred_all[:,best_lambda]\n",
    "    #     model = make_pipeline(StandardScaler(), lm.PoissonRegressor(alpha=lambdas[best_lambda]))\n",
    "    #     model.fit(x_train,sps_train)\n",
    "    # predicted firing rate\n",
    "    # bin the firing rate to get smooth rate vs time\n",
    "    sp_smooth = (np.convolve(sps_test, np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "    pred_smooth = (np.convolve(sp_pred, np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "    # a few diagnostics\n",
    "    err = np.mean((sp_smooth-pred_smooth)**2)\n",
    "    cc = np.corrcoef(sp_smooth[bin_length:-bin_length], pred_smooth[bin_length:-bin_length])\n",
    "    cc_all = cc[0,1]\n",
    "    r2_all = r2_score(sp_smooth,pred_smooth)\n",
    "    return cc_all, sta_all, sps_test, sp_pred, r2_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supported-cheat",
   "metadata": {},
   "outputs": [],
   "source": [
    "lag=150 # in ms\n",
    "nt_glm_lag = 5\n",
    "# minlag = int(-lag//(1000*model_dt)); maxlag=int((lag//(1000*model_dt))+1)\n",
    "lag_list = np.array([-1,0,1,2,3]) #,np.arange(minlag,maxlag,np.floor((maxlag-minlag)/nt_glm_lag).astype(int))\n",
    "nt_glm_lag = len(lag_list)\n",
    "print(lag_list,1000*lag_list*model_dt)\n",
    "do_shuffle=False\n",
    "model_type = 'poissonregressor'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eligible-device",
   "metadata": {},
   "outputs": [],
   "source": [
    "for do_shuffle in [False,True]:\n",
    "    # Load Data\n",
    "    data, train_idx, test_idx = load_train_test(file_dict, save_dir, model_dt=model_dt, do_shuffle=do_shuffle, do_norm=True,free_move=free_move, has_imu=free_move, has_mouse=False)\n",
    "    locals().update(data)\n",
    "\n",
    "    ##### Start GLM Parallel Processing #####\n",
    "    start = time.time()\n",
    "    nks = np.shape(train_vid)[1:]; nk = nks[0]*nks[1]*nt_glm_lag\n",
    "    # Reshape data (video) into (T*n)xN array\n",
    "#     x_train = train_vid.reshape(train_vid.shape[0],-1)\n",
    "#     x_train = np.hstack([np.roll(x_train, nframes, axis=0) for nframes in lag_list]) # nt_glm_lag\n",
    "#     x_test = test_vid.reshape(test_vid.shape[0],-1) \n",
    "#     x_test = np.hstack([np.roll(x_test, nframes, axis=0) for nframes in lag_list])#\n",
    "    rolled_vid = np.hstack([np.roll(model_vid_sm, nframes, axis=0) for nframes in lag_list]) # nt_glm_lag\n",
    "    x_train = rolled_vid[train_idx].reshape(len(train_idx),-1)\n",
    "    x_test = rolled_vid[test_idx].reshape(len(test_idx),-1)\n",
    "    \n",
    "    # Put data into shared memory for parallization \n",
    "    train_nsp_r = ray.put(train_nsp)\n",
    "    test_nsp_r = ray.put(test_nsp)\n",
    "    train_data_r = ray.put(x_train)\n",
    "    test_data_r = ray.put(x_test)\n",
    "    result_ids = []\n",
    "    # Loop over parameters appending process ids\n",
    "    for celln in range(train_nsp.shape[1]):\n",
    "        result_ids.append(do_glm_fit_vis_skl.remote(train_nsp_r, test_nsp_r, train_data_r, test_data_r, celln, model_type, lag_list, model_dt=model_dt))\n",
    "\n",
    "    print('N_proc:', len(result_ids))\n",
    "    results_p = ray.get(result_ids)\n",
    "    print('GLM: ', time.time()-start)\n",
    "\n",
    "    ##### Gather Data and Find Max CC Model #####\n",
    "    mcc = np.stack([results_p[i][0] for i in range(len(results_p))])\n",
    "    msta = np.stack([results_p[i][1] for i in range(len(results_p))])\n",
    "    msp = np.stack([results_p[i][2] for i in range(len(results_p))])\n",
    "    mpred = np.stack([results_p[i][3] for i in range(len(results_p))])\n",
    "    mr2 = np.stack([results_p[i][4] for i in range(len(results_p))])\n",
    "\n",
    "    nt_glm_lag = len(lag_list)\n",
    "    GLM_Data = {'mcc': mcc,\n",
    "                'msta': msta,\n",
    "                'msp': msp,\n",
    "                'mpred': mpred,\n",
    "                'mr2':mr2,}\n",
    "    if do_shuffle:\n",
    "        ioh5.save(save_dir/'GLM_{}_Data_VisOnly_notsmooth_dt{:03d}_T{:02d}_shuffled.h5'.format(model_type,int(model_dt*1000), nt_glm_lag), GLM_Data)\n",
    "    else:\n",
    "        ioh5.save(save_dir/'GLM_{}_Data_VisOnly_notsmooth_dt{:03d}_T{:02d}.h5'.format(model_type,int(model_dt*1000), nt_glm_lag), GLM_Data)\n",
    "        \n",
    "    del train_nsp_r, test_nsp_r, train_data_r, test_data_r, result_ids, results_p, mcc, msta, msp, mpred, mr2,\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interior-chase",
   "metadata": {},
   "source": [
    "# GLM Movement Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sized-variation",
   "metadata": {},
   "outputs": [],
   "source": [
    "celln=51\n",
    "model_type = 'poissonregressor'\n",
    "nt_glm_lag=5\n",
    "# Load Data\n",
    "do_shuffle=False\n",
    "data, train_idx, test_idx = load_train_test(file_dict, save_dir, model_dt=model_dt, do_shuffle=False, do_norm=False,free_move=free_move, has_imu=free_move, has_mouse=False)\n",
    "locals().update(data)\n",
    "\n",
    "# Initialize movement combinations\n",
    "titles = np.array(['Theta','Phi','Roll','Pitch']) # 'dg_p','dg_n' 'roll','pitch'\n",
    "titles_all = []\n",
    "for n in range(1,len(titles)+1):\n",
    "    perms = np.array(list(itertools.combinations(np.arange(len(titles)), n)))\n",
    "    for ind in range(perms.shape[0]):\n",
    "        titles_all.append('_'.join([t for t in titles[perms[ind]]]))\n",
    "\n",
    "train_dgaze_p = train_dth + np.diff(train_gz,append=0)\n",
    "train_dgaze_n = train_dth - np.diff(train_gz,append=0)\n",
    "test_dgaze_p = test_dth + np.diff(test_gz,append=0)\n",
    "test_dgaze_n = test_dth - np.diff(test_gz,append=0)\n",
    "\n",
    "move_train = np.hstack((train_th[:,np.newaxis],train_phi[:,np.newaxis],train_roll[:,np.newaxis],train_pitch[:,np.newaxis])) #, train_dth[:,np.newaxis],train_dphi[:,np.newaxis]))\n",
    "move_test = np.hstack((test_th[:,np.newaxis],test_phi[:,np.newaxis],test_roll[:,np.newaxis],test_pitch[:,np.newaxis])) #,test_dgaze_p[:,np.newaxis],test_dgaze_n[:,np.newaxis]))#\n",
    "# move_test = np.hstack((test_th[:,np.newaxis],test_phi[:,np.newaxis],test_dgaze_p[:,np.newaxis],test_dgaze_n[:,np.newaxis]))# test_dth[:,np.newaxis],test_dphi[:,np.newaxis]))\n",
    "# move_train = np.hstack((train_th[:,np.newaxis],train_phi[:,np.newaxis],train_dgaze_p[:,np.newaxis],train_dgaze_n[:,np.newaxis]))# train_roll[:,np.newaxis],train_pitch[:,np.newaxis]))\n",
    "\n",
    "perms = np.array(list(itertools.combinations(np.arange(len(titles)), 3)))[2]\n",
    "print(perms)\n",
    "##### Format data #####\n",
    "nt_glm_lag = len(lag_list)\n",
    "# save shape of train_data for initialization\n",
    "nks = np.shape(train_vid)[1:]; nk = nks[0]*nks[1]\n",
    "#     nks = np.shape(train_data)[1:]; nk = nks[0]*nks[1]\n",
    "w_move = np.zeros(move_train.shape[1])\n",
    "# Shift spikes by -lag for GLM fits\n",
    "sps_train = train_nsp[:,celln] # np.roll(train_nsp[:,celln],-lag)\n",
    "sps_test = test_nsp[:,celln] # np.roll(test_nsp[:,celln],-lag)\n",
    "\n",
    "# Reshape data (video) into (T*n)xN array\n",
    "x_train = move_train[:,perms]\n",
    "x_test = move_test[:,perms]\n",
    "\n",
    "if model_type == 'elasticnetcv':\n",
    "    model = lm.ElasticNetCV(l1_ratio=[.05, .01, .5, .7]) # lm.RidgeCV(alphas=np.arange(100,10000,1000))) #  #MultiOutputRegressor(lm.Ridge(),n_jobs=-1)) \n",
    "    model.fit(x_train,sps_train)\n",
    "    w_move[perms] = model.coef_\n",
    "    sp_pred = model.predict(x_test)\n",
    "elif model_type == 'ridgecv':\n",
    "    lambdas = 1024 * (2**np.arange(0,16))\n",
    "    model = lm.RidgeCV(alphas=lambdas)\n",
    "    model.fit(x_train,sps_train)\n",
    "    w_move[perms] = model.coef_\n",
    "    sp_pred = model.predict(x_test)\n",
    "else:\n",
    "    model = lm.PoissonRegressor(alpha=0,max_iter=300)\n",
    "    \n",
    "    # calculate MAP estimate               \n",
    "    model.fit(x_train,sps_train)\n",
    "    intercept = model.intercept_\n",
    "    w_move[perms] = model.coef_\n",
    "    sp_pred = model.predict(x_test)\n",
    "#     lambdas = 2048 * (2**np.arange(0,16))\n",
    "    lambdas = 2**np.arange(0,16)\n",
    "#     nlam = len(lambdas)\n",
    "#     # Initialze mse traces for regularization cross validation\n",
    "#     msetrain = np.zeros((nlam,1))\n",
    "#     msetest = np.zeros((nlam,1))\n",
    "#     pred_all =np.zeros((x_test.shape[0],nlam)) \n",
    "#     w_ridge = np.zeros((x_train.shape[-1],nlam))\n",
    "#     w_intercept = np.zeros((nlam,1))\n",
    "#     # loop over regularization strength\n",
    "#     for l in range(len(lambdas)):\n",
    "#         model = lm.PoissonRegressor(alpha=lambdas[l],max_iter=300)\n",
    "#         # calculate MAP estimate               \n",
    "#         model.fit(x_train,sps_train)\n",
    "#         w_ridge[:,l] = model.coef_\n",
    "#         w_intercept[l] = model.intercept_\n",
    "#         pred_all[:,l] = model.predict(x_test)\n",
    "#         # calculate test and training rms error\n",
    "#         msetrain[l] = mean_poisson_deviance(sps_train,model.predict(x_train)) #np.mean((sps_train - model.predict(x_train))**2)\n",
    "#         msetest[l] = mean_poisson_deviance(sps_test,pred_all[:,l]) # np.mean((sps_test - model.predict(x_test))**2)\n",
    "#     # select best cross-validated lambda for RF\n",
    "#     best_lambda = np.argmin(msetest)\n",
    "#     w = w_ridge[:,best_lambda]\n",
    "#     intercept= w_intercept[best_lambda]\n",
    "#     ridge_rf = w_ridge[:,best_lambda]\n",
    "#     w_move[perms] = w\n",
    "#     sp_pred = pred_all[:,best_lambda]\n",
    "#     model = make_pipeline(StandardScaler(), lm.PoissonRegressor(alpha=lambdas[best_lambda]))\n",
    "#     model.fit(x_train,sps_train)\n",
    "# predicted firing rate\n",
    "# bin the firing rate to get smooth rate vs time\n",
    "sp_smooth = (np.convolve(sps_test, np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "pred_smooth = (np.convolve(sp_pred, np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "# a few diagnostics\n",
    "err = np.mean((sp_smooth-pred_smooth)**2)\n",
    "cc = np.corrcoef(sp_smooth, pred_smooth)\n",
    "cc_all = cc[0,1]\n",
    "r2_all = r2_score(sp_smooth,pred_smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bottom-carter",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "magnetic-youth",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = move_test[:,perms]\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apparent-polish",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sp_smooth)\n",
    "plt.plot(pred_smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "egyptian-secretariat",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def do_glm_fit_mot_skl(train_nsp, test_nsp, move_train, move_test, celln, perms, model_type, bin_length=40, model_dt=.1):\n",
    "    \n",
    "    ##### Format data #####\n",
    "    w_move = np.zeros(move_train.shape[1])\n",
    "    # Shift spikes by -lag for GLM fits\n",
    "    sps_train = train_nsp[:,celln] # np.roll(train_nsp[:,celln],-lag)\n",
    "    sps_test = test_nsp[:,celln] # np.roll(test_nsp[:,celln],-lag)\n",
    "    \n",
    "    # Reshape data (video) into (T*n)xN array\n",
    "    x_train = move_train[:,perms]\n",
    "    x_test = move_test[:,perms]\n",
    "    \n",
    "    if model_type == 'elasticnetcv':\n",
    "        model = lm.ElasticNetCV(l1_ratio=[.05, .01, .5, .7]) # lm.RidgeCV(alphas=np.arange(100,10000,1000))) #  #MultiOutputRegressor(lm.Ridge(),n_jobs=-1)) \n",
    "        model.fit(x_train,sps_train)\n",
    "        w_move[perms] = model.coef_\n",
    "        sp_pred = model.predict(x_test)\n",
    "    elif model_type == 'ridgecv':\n",
    "        lambdas = 1024 * (2**np.arange(0,16))\n",
    "        model = lm.RidgeCV(alphas=lambdas)\n",
    "        model.fit(x_train,sps_train)\n",
    "        w_move[perms] = model.coef_\n",
    "        sp_pred = model.predict(x_test)\n",
    "    else:\n",
    "        model = lm.PoissonRegressor(alpha=0,max_iter=300)\n",
    "        # calculate MAP estimate               \n",
    "        model.fit(x_train,sps_train)\n",
    "        intercept=  model.intercept_\n",
    "        w_move[perms] = model.coef_\n",
    "        sp_pred = model.predict(x_test)\n",
    "\n",
    "    # bin the firing rate to get smooth rate vs time\n",
    "    sp_smooth = (np.convolve(sps_test, np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "    pred_smooth = (np.convolve(sp_pred, np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "    # a few diagnostics\n",
    "    err = np.mean((sp_smooth-pred_smooth)**2)\n",
    "    cc = np.corrcoef(sp_smooth, pred_smooth)\n",
    "    cc_all = cc[0,1]\n",
    "    r2_all = r2_score(sp_smooth,pred_smooth)\n",
    "    return cc_all, w_move, sps_test, sp_pred, r2_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removable-baseline",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "model_type = 'poissonregressor'\n",
    "nt_glm_lag=5\n",
    "# Load Data\n",
    "do_shuffle=False\n",
    "data, train_idx, test_idx = load_train_test(file_dict, save_dir, model_dt=model_dt, do_shuffle=False, do_norm=False,free_move=free_move, has_imu=free_move, has_mouse=False)\n",
    "locals().update(data)\n",
    "\n",
    "# Initialize movement combinations\n",
    "titles = np.array(['th','phi','roll','pitch']) # 'dg_p','dg_n' 'roll','pitch'\n",
    "titles_all = []\n",
    "for n in range(1,len(titles)+1):\n",
    "    perms = np.array(list(itertools.combinations(np.arange(len(titles)), n)))\n",
    "    for ind in range(perms.shape[0]):\n",
    "        titles_all.append('_'.join([t for t in titles[perms[ind]]]))\n",
    "\n",
    "train_dgaze_p = train_dth + np.diff(train_gz,append=0)\n",
    "train_dgaze_n = train_dth - np.diff(train_gz,append=0)\n",
    "test_dgaze_p = test_dth + np.diff(test_gz,append=0)\n",
    "test_dgaze_n = test_dth - np.diff(test_gz,append=0)\n",
    "\n",
    "move_train = np.hstack((train_th[:,np.newaxis],train_phi[:,np.newaxis],train_roll[:,np.newaxis],train_pitch[:,np.newaxis])) #, train_dth[:,np.newaxis],train_dphi[:,np.newaxis]))\n",
    "move_test = np.hstack((test_th[:,np.newaxis],test_phi[:,np.newaxis],test_roll[:,np.newaxis],test_pitch[:,np.newaxis])) #,test_dgaze_p[:,np.newaxis],test_dgaze_n[:,np.newaxis]))#\n",
    "# move_test = np.hstack((test_th[:,np.newaxis],test_phi[:,np.newaxis],test_dgaze_p[:,np.newaxis],test_dgaze_n[:,np.newaxis]))# test_dth[:,np.newaxis],test_dphi[:,np.newaxis]))\n",
    "# move_train = np.hstack((train_th[:,np.newaxis],train_phi[:,np.newaxis],train_dgaze_p[:,np.newaxis],train_dgaze_n[:,np.newaxis]))# train_roll[:,np.newaxis],train_pitch[:,np.newaxis]))\n",
    "\n",
    "\n",
    "# Put data into shared memory for parallization \n",
    "train_nsp_r = ray.put(train_nsp)\n",
    "test_nsp_r = ray.put(test_nsp)\n",
    "# train_data_r = ray.put(train_vid)\n",
    "# test_data_r = ray.put(test_vid)\n",
    "move_train_r = ray.put(move_train)\n",
    "move_test_r = ray.put(move_test)\n",
    "result_ids = []\n",
    "# Loop over parameters appending process ids\n",
    "for celln in range(train_nsp.shape[1]):\n",
    "    for n in range(1,len(titles)+1):\n",
    "        perms = np.array(list(itertools.combinations(np.arange(len(titles)), n)))\n",
    "        for ind in range(perms.shape[0]):\n",
    "            result_ids.append(do_glm_fit_mot_skl.remote(train_nsp_r, test_nsp_r, move_train_r, move_test_r, celln, perms[ind], model_type, nt_glm_lag, model_dt=model_dt))\n",
    "\n",
    "print('N_proc:', len(result_ids))\n",
    "results_p = ray.get(result_ids)\n",
    "print('GLM Time: ', time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressed-independence",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Gather Data and Find Max CC Model #####\n",
    "cc_all = np.stack([results_p[i][0] for i in range(len(results_p))])\n",
    "w_move_all = np.stack([results_p[i][1] for i in range(len(results_p))])\n",
    "sp_raw = np.stack([results_p[i][2] for i in range(len(results_p))])\n",
    "pred_raw = np.stack([results_p[i][3] for i in range(len(results_p))])\n",
    "r2_all =  np.stack([results_p[i][4] for i in range(len(results_p))])\n",
    "\n",
    "cc_all = cc_all.reshape((model_nsp.shape[1],len(titles_all),) + cc_all.shape[1:])\n",
    "w_move_all = w_move_all.reshape((model_nsp.shape[1],len(titles_all),) + w_move_all.shape[1:])\n",
    "sp_raw = sp_raw.reshape((model_nsp.shape[1],len(titles_all),) + sp_raw.shape[1:])\n",
    "pred_raw = pred_raw.reshape((model_nsp.shape[1],len(titles_all),) + pred_raw.shape[1:])\n",
    "r2_all = r2_all.reshape((model_nsp.shape[1],len(titles_all),) + r2_all.shape[1:])\n",
    "\n",
    "m_cells, m_models,  = np.where(cc_all==np.max(cc_all,axis=(-1), keepdims=True))\n",
    "m_cells, m_cinds = np.unique(m_cells,return_index=True)\n",
    "m_models = m_models[m_cinds]\n",
    "mcc = cc_all[m_cells,m_models]\n",
    "msp = sp_raw[m_cells,m_models]\n",
    "mpred = pred_raw[m_cells,m_models]\n",
    "mw_move = w_move_all[m_cells,m_models]\n",
    "mr2 = r2_all[m_cells,m_models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocational-allergy",
   "metadata": {},
   "outputs": [],
   "source": [
    "GLM_Data = {'cc_all': cc_all,\n",
    "            'sp_raw': sp_raw,\n",
    "            'pred_raw': pred_raw,\n",
    "            'w_move_all': w_move_all,\n",
    "            'r2_all': r2_all,}\n",
    "\n",
    "if do_shuffle:\n",
    "    ioh5.save(save_dir/'GLM_{}_Data_MotOnly_notsmooth_dt{:03d}_T{:02d}_shuffled.h5'.format(model_type,int(model_dt*1000), nt_glm_lag), GLM_Data)\n",
    "else:\n",
    "    ioh5.save(save_dir/'GLM_{}_Data_MotOnly_notsmooth_dt{:03d}_T{:02d}.h5'.format(model_type,int(model_dt*1000), nt_glm_lag), GLM_Data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gothic-petersburg",
   "metadata": {},
   "source": [
    "## Motor only Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hybrid-relaxation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_add(alpha,stat_range,stat_all):\n",
    "    return np.mean((stat_range - stat_all+alpha)**2)\n",
    "\n",
    "def f_mult(alpha,stat_range,stat_all):\n",
    "    return np.mean((stat_range - stat_all*alpha)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fossil-pendant",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_length=40\n",
    "data, train_idx, test_idx = load_train_test(file_dict, save_dir, model_dt=model_dt, do_shuffle=do_shuffle, do_norm=False)\n",
    "locals().update(data)\n",
    "##### Explore Neurons #####\n",
    "colors = plt.cm.winter(np.linspace(0,1,4))\n",
    "clrs = ['blue','orange','green','red']\n",
    "# Initialize movement combinations\n",
    "titles = np.array(['th','phi','roll','pitch']) # 'dg_p','dg_n' 'roll','pitch'\n",
    "titles_all = []\n",
    "for n in range(1,len(titles)+1):\n",
    "    perms = np.array(list(itertools.combinations(np.arange(len(titles)), n)))\n",
    "    for ind in range(perms.shape[0]):\n",
    "        titles_all.append('_'.join([t for t in titles[perms[ind]]]))\n",
    "\n",
    "# train_dgaze_p = train_dth + np.diff(train_gz,append=0)\n",
    "# train_dgaze_n = train_dth - np.diff(train_gz,append=0)\n",
    "# test_dgaze_p = test_dth + np.diff(test_gz,append=0)\n",
    "# test_dgaze_n = test_dth - np.diff(test_gz,append=0)\n",
    "\n",
    "move_train = np.hstack((train_th[:,np.newaxis],train_phi[:,np.newaxis],train_roll[:,np.newaxis],train_pitch[:,np.newaxis])) #, train_dth[:,np.newaxis],train_dphi[:,np.newaxis]))\n",
    "move_test = np.hstack((test_th[:,np.newaxis],test_phi[:,np.newaxis],test_roll[:,np.newaxis],test_pitch[:,np.newaxis])) #,test_dgaze_p[:,np.newaxis],test_dgaze_n[:,np.newaxis]))#\n",
    "# move_test = np.hstack((test_th[:,np.newaxis],test_phi[:,np.newaxis],test_dgaze_p[:,np.newaxis],test_dgaze_n[:,np.newaxis]))# test_dth[:,np.newaxis],test_dphi[:,np.newaxis]))\n",
    "# move_train = np.hstack((train_th[:,np.newaxis],train_phi[:,np.newaxis],train_dgaze_p[:,np.newaxis],train_dgaze_n[:,np.newaxis]))# train_roll[:,np.newaxis],train_pitch[:,np.newaxis]))\n",
    "move_test = move_test - np.mean(move_test,axis=0)\n",
    "# Create all tuning curves for plotting\n",
    "N_bins=10\n",
    "ncells = model_nsp.shape[-1]\n",
    "ax_ylims = np.zeros((model_nsp.shape[-1],len(titles)))\n",
    "tuning_curves = np.zeros((model_nsp.shape[-1],len(titles),N_bins-1))\n",
    "tuning_stds = np.zeros((model_nsp.shape[-1],len(titles),N_bins-1))\n",
    "var_ranges = np.zeros((len(titles),N_bins-1))\n",
    "for modeln in range(len(titles)):\n",
    "    metric = move_test[:,modeln]\n",
    "    tuning, tuning_std, var_range = tuning_curve(test_nsp, metric, N_bins=N_bins, model_dt=.1)\n",
    "    tuning_curves[:,modeln] = tuning\n",
    "    tuning_stds[:,modeln] = tuning_std\n",
    "    ax_ylims[:,modeln] = np.max(tuning,axis=1)\n",
    "    var_ranges[modeln] = var_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colonial-nickel",
   "metadata": {},
   "outputs": [],
   "source": [
    "celln=np.argmax(mr2)\n",
    "bin_length=40\n",
    "ncells = model_nsp.shape[-1]\n",
    "colors = plt.cm.winter(np.linspace(0,1,4))\n",
    "clrs = ['blue','orange','green','red']\n",
    "fig, axs = plt.subplots(2,5, figsize=((25,10))) #np.floor(7.5*len(model_nsp)).astype(int)\n",
    "predcell = mpred[celln]/model_dt\n",
    "nspcell = msp[celln]/model_dt\n",
    "msp_smooth=(np.convolve(msp[celln], np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "pred_smooth=(np.convolve(mpred[celln], np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "axs[0,0].plot(msp_smooth,'k',label='test FR')\n",
    "axs[0,0].plot(pred_smooth,'r', label='pred FR')\n",
    "axs[0,0].set_xlabel('Frame #')\n",
    "axs[0,0].set_ylabel('Firing Rate (spks/s)')\n",
    "axs[0,0].legend()\n",
    "axs[0,0].set_title('Smoothed FRs')\n",
    "\n",
    "    \n",
    "    \n",
    "# Eye Tuning Curve\n",
    "for modeln in range(len(titles)-2):\n",
    "    axs[0,1].errorbar(var_ranges[modeln],tuning_curves[celln,modeln], yerr=tuning_stds[celln,modeln],label=titles[modeln],c=clrs[modeln],lw=4,elinewidth=3)\n",
    "axs[0,1].set_ylim(bottom=0,top=np.max(ax_ylims,axis=1)[celln]+2*np.max(tuning_stds,axis=(1,2))[celln])\n",
    "axs[0,1].set_xlim(-50,50)\n",
    "axs[0,1].set_xlabel('Angle ($ ^{\\degree}$)')\n",
    "axs[0,1].set_ylabel('Spikes/s')\n",
    "axs[0,1].set_title('Eye Tuning Curves')\n",
    "axs[0,1].legend(bbox_to_anchor=(1.01, 1), fontsize=12)\n",
    "\n",
    "# Head Tuning Curves\n",
    "for modeln in range(2,len(titles)):\n",
    "    axs[0,2].errorbar(var_ranges[modeln],tuning_curves[celln,modeln], yerr=tuning_stds[celln,modeln],label=titles[modeln],c=clrs[modeln],lw=4,elinewidth=3)\n",
    "axs[0,2].set_ylim(bottom=0,top=np.max(ax_ylims,axis=1)[celln]+2*np.max(tuning_stds,axis=(1,2))[celln])\n",
    "axs[0,2].set_xlim(-50,50)\n",
    "axs[0,2].set_xlabel('Angle ($ ^{\\degree}$)')\n",
    "axs[0,2].set_ylabel('Spikes/s')\n",
    "axs[0,2].set_title('Head Tuning Curves')\n",
    "axs[0,2].legend(bbox_to_anchor=(1.01, 1), fontsize=12)\n",
    "\n",
    "# Set up predicted spike range between 1-99th percentile\n",
    "stat_bins = 5\n",
    "pred_range = np.quantile(predcell,[.1,.99])\n",
    "msp_range = np.quantile(nspcell,[.1,.99])\n",
    "pred_rangelin = np.quantile(predcell,[.01,.2,.4,.6,.8,.99])\n",
    "stat_bins = len(pred_rangelin)#5\n",
    "quartiles = [0,.25,.5,.75,1]\n",
    "axs[0,3].scatter(mpred[celln]/model_dt,msp[celln]/model_dt,c='k',s=15)\n",
    "axs[0,3].set_xlabel('Predicted Spike Rate')\n",
    "axs[0,3].set_ylabel('Actual Spike Rate')\n",
    "    \n",
    "for modeln in range(len(titles)):\n",
    "    axs[0,4].bar(modeln, mw_move[celln,modeln], color=clrs[modeln])\n",
    "    axs[0,4].set_xticks(np.arange(0,len(titles)))\n",
    "    axs[0,4].set_xticklabels(titles)\n",
    "    axs[0,4].set_ylabel('GLM Weight')\n",
    "\n",
    "\n",
    "mse_add = np.zeros((ncells,len(titles),len(quartiles)-1))\n",
    "mse_mult = np.zeros((ncells,len(titles),len(quartiles)-1))\n",
    "alpha_add = np.zeros((ncells,len(titles),len(quartiles)-1))\n",
    "alpha_mult = np.zeros((ncells,len(titles),len(quartiles)-1))\n",
    "\n",
    "traces = np.zeros((ncells,len(titles),len(quartiles)-1,stat_bins-1)) # (model_type,quartile,FR)\n",
    "traces_mean = np.zeros((ncells,len(titles),stat_bins-1)) # (model_type,quartile,FR)\n",
    "edges_all = np.zeros((ncells,len(titles),len(quartiles)-1,stat_bins-1)) # (model_type,quartile,FR)\n",
    "# df_traces = pd.DataFrame([],columns=['modeln','quartile','FR']) \n",
    "for modeln in range(len(titles)):\n",
    "    metric = move_test[:,modeln]\n",
    "    nranges = np.quantile(metric,quartiles)# np.linspace(np.nanmean(metric)-2*np.nanstd(metric), np.nanmean(metric)+2*np.nanstd(metric),N_bins)\n",
    "    stat_all, edges, _ = binned_statistic(predcell,nspcell, statistic='mean',bins=pred_rangelin)\n",
    "    edge_mids = np.array([(edges[i]+edges[i+1])/2 for i in range(len(edges)-1)])\n",
    "    traces_mean[celln,modeln]=stat_all\n",
    "    max_fr = np.max(stat_all)\n",
    "#     axs[1,modeln].set_xlim(0,pred_range[1]+np.std(pred_range))\n",
    "#     axs[1,modeln].set_ylim(0,np.max(stat)+np.std(stat))\n",
    "\n",
    "    for n in range(len(nranges)-1):\n",
    "        ind = np.where(((metric<=nranges[n+1])&(metric>nranges[n])))[0]\n",
    "        pred = predcell[ind]\n",
    "        sp = nspcell[ind]\n",
    "\n",
    "        stat_range, edges, _ = binned_statistic(pred, sp, statistic='mean',bins=pred_rangelin)\n",
    "        edge_mids = np.array([(edges[i]+edges[i+1])/2 for i in range(len(edges)-1)])\n",
    "        traces[celln,modeln,n]=stat_range\n",
    "        edges_all[celln,modeln,n]=edge_mids\n",
    "        res_add = minimize_scalar(f_add,args=(stat_range/max_fr, stat_all/max_fr))\n",
    "        res_mult = minimize_scalar(f_mult,args=(stat_range/max_fr, stat_all/max_fr))\n",
    "        mse_add[celln, modeln, n] = res_add.fun\n",
    "        mse_mult[celln, modeln, n] = res_mult.fun\n",
    "        alpha_add[celln, modeln, n] = res_add.x\n",
    "        alpha_mult[celln, modeln, n] = res_mult.x\n",
    "\n",
    "        axs[1,modeln].plot(edge_mids, stat_range,'.-', c=colors[n],label='{:.02f} : {:.02f}'.format(nranges[n],nranges[n+1]),lw=4,ms=20)\n",
    "        axs[1,modeln].plot(np.linspace(pred_range[0],pred_range[1]),np.linspace(pred_range[0],pred_range[1]),'k--',zorder=0)\n",
    "        axs[1,modeln].set_title('Metric: {}'.format(titles[modeln]), color=clrs[modeln])\n",
    "        axs[1,modeln].set_xlabel('Predicted Spike Rate')\n",
    "        axs[1,modeln].set_ylabel('Actual Spike Rate')\n",
    "#     axs[1,modeln].plot(edge_mids, stat_all,'.-', c='k', lw=5, ms=20, label='All_data')\n",
    "    axs[1,modeln].legend(loc='upper left', fontsize=12)\n",
    "    axs[1,modeln].axis('equal')\n",
    "\n",
    "dmodel = mse_add[celln]-mse_mult[celln]\n",
    "crange = np.max(np.abs(dmodel))\n",
    "im = axs[1,-1].imshow(dmodel,cmap='seismic',vmin=-crange,vmax=crange)\n",
    "axs[1,-1].set_yticks(np.arange(0,4))\n",
    "axs[1,-1].set_yticklabels(titles)\n",
    "axs[1,-1].set_ylabel('Movement Model')\n",
    "axs[1,-1].set_xticks(np.arange(0,4))\n",
    "axs[1,-1].set_xticklabels(['.25','.5','.75','1'])\n",
    "axs[1,-1].set_xlabel('Quantile Range')\n",
    "axs[1,-1].set_title('$MSE_{add}$ - $MSE_{mult}$')\n",
    "cbar = add_colorbar(im)\n",
    "\n",
    "plt.suptitle('Celln:{}, cc={:.03f}, r2={:.03f}'.format(celln,mcc[celln],mr2[celln]),y=1,fontsize=30)\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "# fig.savefig(FigPath/'CellSummary_N{}_T{:02d}.png'.format(celln,nt_glm_lag), facecolor='white', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reflected-fifteen",
   "metadata": {},
   "outputs": [],
   "source": [
    "with PdfPages(FigPath/ 'GLM_MotOnly_{}_dt{:03d}_T{:02d}_cellsummary_sig.pdf'.format(model_type,int(model_dt*1000),nt_glm_lag)) as pdf:\n",
    "    for celln in tqdm(range(msp.shape[0])):\n",
    "        if mcc[celln]>.3:\n",
    "            fig, axs = plt.subplots(2,5, figsize=((25,10))) #np.floor(7.5*len(model_nsp)).astype(int)\n",
    "            predcell = mpred[celln]/model_dt\n",
    "            nspcell = msp[celln]/model_dt\n",
    "            msp_smooth=(np.convolve(msp[celln], np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "            pred_smooth=(np.convolve(mpred[celln], np.ones(bin_length), 'same')) / (bin_length * model_dt)\n",
    "            axs[0,0].plot(msp_smooth,'k',label='test FR')\n",
    "            axs[0,0].plot(pred_smooth,'r', label='pred FR')\n",
    "            axs[0,0].set_xlabel('Frame #')\n",
    "            axs[0,0].set_ylabel('Firing Rate (spks/s)')\n",
    "            axs[0,0].legend()\n",
    "            axs[0,0].set_title('Smoothed FRs')\n",
    "\n",
    "\n",
    "\n",
    "            # Eye Tuning Curve\n",
    "            for modeln in range(len(titles)-2):\n",
    "                axs[0,1].errorbar(var_ranges[modeln],tuning_curves[celln,modeln], yerr=tuning_stds[celln,modeln],label=titles[modeln],c=clrs[modeln],lw=4,elinewidth=3)\n",
    "            axs[0,1].set_ylim(bottom=0,top=np.max(ax_ylims,axis=1)[celln]+2*np.max(tuning_stds,axis=(1,2))[celln])\n",
    "            axs[0,1].set_xlim(-50,50)\n",
    "            axs[0,1].set_xlabel('Angle ($ ^{\\degree}$)')\n",
    "            axs[0,1].set_ylabel('Spikes/s')\n",
    "            axs[0,1].set_title('Eye Tuning Curves')\n",
    "            axs[0,1].legend(bbox_to_anchor=(1.01, 1), fontsize=12)\n",
    "\n",
    "            # Head Tuning Curves\n",
    "            for modeln in range(2,len(titles)):\n",
    "                axs[0,2].errorbar(var_ranges[modeln],tuning_curves[celln,modeln], yerr=tuning_stds[celln,modeln],label=titles[modeln],c=clrs[modeln],lw=4,elinewidth=3)\n",
    "            axs[0,2].set_ylim(bottom=0,top=np.max(ax_ylims,axis=1)[celln]+2*np.max(tuning_stds,axis=(1,2))[celln])\n",
    "            axs[0,2].set_xlim(-50,50)\n",
    "            axs[0,2].set_xlabel('Angle ($ ^{\\degree}$)')\n",
    "            axs[0,2].set_ylabel('Spikes/s')\n",
    "            axs[0,2].set_title('Head Tuning Curves')\n",
    "            axs[0,2].legend(bbox_to_anchor=(1.01, 1), fontsize=12)\n",
    "\n",
    "            # Set up predicted spike range between 1-99th percentile\n",
    "            stat_bins = 5\n",
    "            pred_range = np.quantile(predcell,[.01,.99])\n",
    "            msp_range = np.quantile(nspcell,[.01,.99])\n",
    "            pred_rangelin = np.quantile(predcell,[.01,.2,.4,.6,.8,.99])\n",
    "            stat_bins = len(pred_rangelin)#5\n",
    "            quartiles = [0,.25,.5,.75,1]\n",
    "            axs[0,3].scatter(mpred[celln]/model_dt,msp[celln]/model_dt,c='k',s=15)\n",
    "            axs[0,3].set_xlabel('Predicted Spike Rate')\n",
    "            axs[0,3].set_ylabel('Actual Spike Rate')\n",
    "\n",
    "            yrange = np.max(np.abs(mw_move))\n",
    "            for modeln in range(len(titles)):\n",
    "                axs[0,4].bar(modeln, mw_move[celln,modeln], color=clrs[modeln])\n",
    "                axs[0,4].set_ylim(-yrange,yrange)\n",
    "                axs[0,4].set_xticks(np.arange(0,len(titles)))\n",
    "                axs[0,4].set_xticklabels(titles)\n",
    "                axs[0,4].set_ylabel('GLM Weight')\n",
    "\n",
    "\n",
    "            mse_add = np.zeros((ncells,len(titles),len(quartiles)-1))\n",
    "            mse_mult = np.zeros((ncells,len(titles),len(quartiles)-1))\n",
    "            alpha_add = np.zeros((ncells,len(titles),len(quartiles)-1))\n",
    "            alpha_mult = np.zeros((ncells,len(titles),len(quartiles)-1))\n",
    "\n",
    "            traces = np.zeros((ncells,len(titles),len(quartiles)-1,stat_bins-1)) # (model_type,quartile,FR)\n",
    "            traces_mean = np.zeros((ncells,len(titles),stat_bins-1)) # (model_type,quartile,FR)\n",
    "            edges_all = np.zeros((ncells,len(titles),len(quartiles)-1,stat_bins-1)) # (model_type,quartile,FR)\n",
    "            # df_traces = pd.DataFrame([],columns=['modeln','quartile','FR']) \n",
    "            for modeln in range(len(titles)):\n",
    "                metric = move_test[:,modeln]\n",
    "                nranges = np.quantile(metric,quartiles)# np.linspace(np.nanmean(metric)-2*np.nanstd(metric), np.nanmean(metric)+2*np.nanstd(metric),N_bins)\n",
    "                stat_all, edges, _ = binned_statistic(predcell,nspcell, statistic='mean',bins=pred_rangelin)\n",
    "                edge_mids = np.array([(edges[i]+edges[i+1])/2 for i in range(len(edges)-1)])\n",
    "                traces_mean[celln,modeln]=stat_all\n",
    "                max_fr = np.max(stat_all)\n",
    "            #     axs[1,modeln].set_xlim(0,pred_range[1]+np.std(pred_range))\n",
    "            #     axs[1,modeln].set_ylim(0,np.max(stat)+np.std(stat))\n",
    "\n",
    "                for n in range(len(nranges)-1):\n",
    "                    ind = np.where(((metric<=nranges[n+1])&(metric>nranges[n])))[0]\n",
    "                    pred = predcell[ind]\n",
    "                    sp = nspcell[ind]\n",
    "\n",
    "                    stat_range, edges, _ = binned_statistic(pred, sp, statistic='mean',bins=pred_rangelin)\n",
    "                    edge_mids = np.array([(edges[i]+edges[i+1])/2 for i in range(len(edges)-1)])\n",
    "                    traces[celln,modeln,n]=stat_range\n",
    "                    edges_all[celln,modeln,n]=edge_mids\n",
    "                    res_add = minimize_scalar(f_add,args=(stat_range/max_fr, stat_all/max_fr))\n",
    "                    res_mult = minimize_scalar(f_mult,args=(stat_range/max_fr, stat_all/max_fr))\n",
    "                    mse_add[celln, modeln, n] = res_add.fun\n",
    "                    mse_mult[celln, modeln, n] = res_mult.fun\n",
    "                    alpha_add[celln, modeln, n] = res_add.x\n",
    "                    alpha_mult[celln, modeln, n] = res_mult.x\n",
    "\n",
    "                    axs[1,modeln].plot(edge_mids, stat_range,'.-', c=colors[n],label='{:.02f} : {:.02f}'.format(nranges[n],nranges[n+1]),lw=4,ms=20)\n",
    "                    axs[1,modeln].plot(np.linspace(pred_range[0],pred_range[1]),np.linspace(pred_range[0],pred_range[1]),'k--',zorder=0)\n",
    "                    axs[1,modeln].set_title('Metric: {}'.format(titles[modeln]), color=clrs[modeln])\n",
    "                    axs[1,modeln].set_xlabel('Predicted Spike Rate')\n",
    "                    axs[1,modeln].set_ylabel('Actual Spike Rate')\n",
    "            #     axs[1,modeln].plot(edge_mids, stat_all,'.-', c='k', lw=5, ms=20, label='All_data')\n",
    "                axs[1,modeln].legend(loc='upper left', fontsize=12)\n",
    "                axs[1,modeln].axis('equal')\n",
    "\n",
    "            dmodel = mse_add[celln]-mse_mult[celln]\n",
    "            crange = np.max(np.abs(dmodel))\n",
    "            im = axs[1,-1].imshow(dmodel,cmap='seismic',vmin=-crange,vmax=crange)\n",
    "            axs[1,-1].set_yticks(np.arange(0,4))\n",
    "            axs[1,-1].set_yticklabels(titles)\n",
    "            axs[1,-1].set_ylabel('Movement Model')\n",
    "            axs[1,-1].set_xticks(np.arange(0,4))\n",
    "            axs[1,-1].set_xticklabels(['.25','.5','.75','1'])\n",
    "            axs[1,-1].set_xlabel('Quantile Range')\n",
    "            axs[1,-1].set_title('$MSE_{add}$ - $MSE_{mult}$')\n",
    "            cbar = add_colorbar(im)\n",
    "\n",
    "            plt.suptitle('Celln:{}, cc={:.03f}, r2={:.03f}'.format(celln,mcc[celln],mr2[celln]),y=1,fontsize=30)\n",
    "            plt.tight_layout()\n",
    "            pdf.savefig()\n",
    "            plt.close()\n",
    "        \n",
    "# fig.savefig(FigPath/'CellSummary_N{}.png'.format(celln), facecolor='white', transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dimensional-joint",
   "metadata": {},
   "source": [
    "## Shuffle Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deluxe-basketball",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "GLM_mot_shuff = ioh5.load(save_dir/'GLM_{}_Data_MotOnly_notsmooth_dt{:03d}_T{:02d}_shuffled.h5'.format(model_type,int(model_dt*1000), nt_glm_lag))\n",
    "GLM_mot = ioh5.load(save_dir/'GLM_{}_Data_MotOnly_notsmooth_dt{:03d}_T{:02d}.h5'.format(model_type,int(model_dt*1000), nt_glm_lag))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "combined-dominant",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "m_cells, m_models,  = np.where(GLM_mot_shuff['cc_all']==np.max(GLM_mot_shuff['cc_all'],axis=(-1), keepdims=True))\n",
    "m_cells, m_cinds = np.unique(m_cells,return_index=True)\n",
    "m_models = m_models[m_cinds]\n",
    "mcc_shuff = GLM_mot_shuff['cc_all'][m_cells,m_models]\n",
    "msp_shuff = GLM_mot_shuff['sp_raw'][m_cells,m_models]\n",
    "mpred_shuff = GLM_mot_shuff['pred_raw'][m_cells,m_models]\n",
    "mw_move_shuff = GLM_mot_shuff['w_move_all'][m_cells,m_models]\n",
    "mr2_shuff = GLM_mot_shuff['r2_all'][m_cells,m_models]\n",
    "\n",
    "m_cells, m_models,  = np.where(GLM_mot['cc_all']==np.max(GLM_mot['cc_all'],axis=(-1), keepdims=True))\n",
    "m_cells, m_cinds = np.unique(m_cells,return_index=True)\n",
    "m_models = m_models[m_cinds]\n",
    "mcc = GLM_mot['cc_all'][m_cells,m_models]\n",
    "msp = GLM_mot['sp_raw'][m_cells,m_models]\n",
    "mpred = GLM_mot['pred_raw'][m_cells,m_models]\n",
    "mw_move = GLM_mot['w_move_all'][m_cells,m_models]\n",
    "mr2 = GLM_mot['r2_all'][m_cells,m_models]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pacific-shape",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2,figsize=(12,5))\n",
    "axs[0].hist(mcc,bins=20,color='k',alpha=.5,label='Test CC')\n",
    "axs[0].hist(mcc_shuff,bins=20,color='r', alpha=.5,label='Shuffled CC')\n",
    "axs[0].set_xlabel('Corr. Coeff.')\n",
    "axs[0].legend(bbox_to_anchor=(1.01, 1), fontsize=12)\n",
    "\n",
    "axs[1].hist(mr2,bins=20,color='k',alpha=.5,label='Test $r^2$')\n",
    "axs[1].hist(mr2_shuff,bins=20,color='r', alpha=.5,label='Shuffled $r^2$')\n",
    "axs[1].set_xlabel('$R^2$')\n",
    "axs[1].legend(bbox_to_anchor=(1.01, 1), fontsize=12)\n",
    "plt.savefig(FigPath/'CC_comparison_mot_{}_dt{}.png'.format(model_type,int(1000*model_dt)), facecolor='white', transparent=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
